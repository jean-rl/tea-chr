year,title,link,abstract
2022,proofs and certificates for max-sat,https://www.jair.org/index.php/jair/article/view/13811,"current max-sat solvers are able to efficiently compute the optimal value of an input instance but they do not provide any certificate of its validity. in this paper, we present a tool, called ms-builder, which generates certificates for the max-sat problem in the particular form of a sequence of equivalence-preserving transformations. to generate a certificate, ms-builder iteratively calls a sat oracle to get a sat resolution refutation which is handled and adapted into a sound refutation for max-sat. in particular, we prove that the size of the computed max-sat refutation is linear with respect to the size of the initial refutation if it is semi-read-once, tree-like regular, tree-like or semi-tree-like. additionally, we propose an extendable tool, called ms-checker, able to verify the validity of any max-sat certificate using max-sat inference rules. both tools are evaluated on the unweighted and weighted benchmark instances of the 2020 max-sat evaluation."
2022,chance-constrained static schedules for temporally probabilistic plans,https://www.jair.org/index.php/jair/article/view/13636,"time management under uncertainty is essential to large scale projects. from space exploration to industrial production, there is a need to schedule and perform activities. given complex specifications on timing. in order to generate schedules that are robust to uncertainty in the duration of activities, prior work has focused on a problem framing that uses an interval-bounded uncertainty representation. however, such approaches are unable to take advantage of known probability distributions over duration.
in this paper we concentrate on a probabilistic formulation of temporal problems with uncertain duration, called the probabilistic simple temporal problem. as distributions often have an unbounded range of outcomes, we consider chance-constrained solutions, with guarantees on the probability of meeting temporal constraints. by considering distributions over uncertain duration, we are able to use risk as a resource, reason over the relative likelihood of outcomes, and derive higher utility solutions. we first demonstrate our approach by encoding the problem as a convex program. we then develop a more efficient hybrid algorithm whose parent solver generates risk allocations and whose child solver generates schedules for a particular risk allocation. the child is made efficient by leveraging existing interval-bounded scheduling algorithms, while the parent is made efficient by extracting conflicts over risk allocations. we perform numerical experiments to show the advantages of reasoning over probabilistic uncertainty, by comparing the utility of schedules generated with risk allocation against those generated from reasoning over bounded uncertainty. we also empirically show that solution time is greatly reduced by incorporating conflict-directed risk allocation."
2022,towards evidence retrieval cost reduction in abstract argumentation frameworks with fallible evidence,https://www.jair.org/index.php/jair/article/view/13639,"arguments in argumentation systems cannot always be considered as standalone entities, requiring the consideration of the pieces of evidence they rely on. this evidence might have to be retrieved from external sources such as databases or the web, and each attempt to retrieve a piece of evidence comes with an associated cost. moreover, a piece of evidence may be available in a given scenario but not in others, and this is not known beforehand. as a result, the collection of active arguments (whose entire set of evidence is available) that can be used by the argumentation machinery of the system may vary from one scenario to another. in this work, we consider an abstract argumentation framework with fallible evidence that accounts for these issues, and propose a heuristic measure used as part of the acceptability calculus (specifically, for building pruned dialectical trees) with the aim of minimizing the evidence retrieval cost of the arguments involved in the reasoning process. we provide an algorithmic solution that is empirically tested against two baselines and formally show the correctness of our approach."
2022,first-order rewritability and complexity of two-dimensional temporal ontology-mediated queries,https://www.jair.org/index.php/jair/article/view/13511,"aiming at ontology-based data access to temporal data, we design two-dimensional temporal ontology and query languages by combining logics from the (extended) dl-lite family with linear temporal logic ltl over discrete time (z,<). our main concern is first-order rewritability of ontology-mediated queries (omqs) that consist of a 2d ontology and a positive temporal instance query. our target languages for fo-rewritings are two-sorted fo(<)--first-order logic with sorts for time instants ordered by the built-in precedence relation < and for the domain of individuals--its extension fo(<,) with the standard congruence predicates t 0 (mod n), for any fixed n > 1, and fo(rpr) that admits relational primitive recursion. in terms of circuit complexity, fo(<,)- and fo(rpr)-rewritability guarantee answering omqs in uniform ac0 and nc1, respectively.
we proceed in three steps. first, we define a hierarchy of 2d dl-lite/ltl ontology languages and investigate the fo-rewritability of omqs with atomic queries by constructing projections onto 1d ltl omqs and employing recent results on the fo-rewritability of propositional ltl omqs. as the projections involve deciding consistency of ontologies and data, we also consider the consistency problem for our languages. while the undecidability of consistency for 2d ontology languages with expressive boolean role inclusions might be expected, we also show that, rather surprisingly, the restriction to krom and horn role inclusions leads to decidability (and expspace-completeness), even if one admits full booleans on concepts. as a final step, we lift some of the rewritability results for atomic omqs to omqs with expressive positive temporal instance queries. the lifting results are based on an in-depth study of the canonical models and only concern horn ontologies."
2022,strategy graphs for influence diagrams,https://www.jair.org/index.php/jair/article/view/13865,"an influence diagram is a graphical model of a bayesian decision problem that is solved by finding a strategy that maximizes expected utility. when an influence diagram is solved by variable elimination or a related dynamic programming algorithm, it is traditional to represent a strategy as a sequence of policies, one for each decision variable, where a policy maps the relevant history for a decision to an action. we propose an alternative representation of a strategy as a graph, called a strategy graph, and show how to modify a variable elimination algorithm so that it constructs a strategy graph. we consider both a classic variable elimination algorithm for influence diagrams and a recent extension of this algorithm that has more relaxed constraints on elimination order that allow improved performance. we consider the advantages of representing a strategy as a graph and, in particular, how to simplify a strategy graph so that it is easier to interpret and analyze."
2022,learning to design fair and private voting rules,https://www.jair.org/index.php/jair/article/view/13734,"voting is used widely to identify a collective decision for a group of agents, based on their preferences. in this paper, we focus on evaluating and designing voting rules that support both the privacy of the voting agents and a notion of fairness over such agents. to do this, we introduce a novel notion of group fairness and adopt the existing notion of local differential privacy. we then evaluate the level of group fairness in several existing voting rules, as well as the trade-offs between fairness and privacy, showing that it is not possible to always obtain maximal economic efficiency with high fairness or high privacy levels. then, we present both a machine learning and a constrained optimization approach to design new voting rules that are fair while maintaining a high level of economic efficiency. finally, we empirically examine the effect of adding noise to create local differentially private voting rules and discuss the three-way trade-off between economic efficiency, fairness, and privacy.
this paper appears in the special track on ai & society."
2022,asymmetric action abstractions for planning in real-time strategy games,https://www.jair.org/index.php/jair/article/view/13769,"action abstractions restrict the number of legal actions available for real-time planning in zero-sum extensive-form games, thus allowing algorithms to focus their search on a set of promising actions. even though unabstracted game trees can lead to optimal policies, due to real-time constraints and the tree size, they are not a practical choice. in this context, we introduce an action abstraction scheme which we call asymmetric action abstraction. asymmetric abstractions allow search algorithms to ""pay more attention"" to some aspects of the game by unevenly dividing the algorithm's search effort amongst different aspects of the game. we also introduce four algorithms that search in asymmetrically abstracted game trees to evaluate the effectiveness of our abstraction schemes. two of our algorithms are adaptations of algorithms developed for searching in action-abstracted spaces, portfolio greedy search and stratified strategy selection, and the other two are adaptations of an algorithm developed for searching in unabstracted spaces, naivemcts. an extensive set of experiments in a real-time strategy game shows that search algorithms using asymmetric abstractions are able to outperform all other search algorithms tested."
2022,reinforcement learning from optimization proxy for ride-hailing vehicle relocation,https://www.jair.org/index.php/jair/article/view/13794,"idle vehicle relocation is crucial for addressing demand-supply imbalance that frequently arises in the ride-hailing system. current mainstream methodologies - optimization and reinforcement learning - suffer from obvious computational drawbacks. optimization models need to be solved in real-time and often trade off model fidelity (hence quality of solutions) for computational efficiency. reinforcement learning is expensive to train and often struggles to achieve coordination among a large fleet. this paper designs a hybrid approach that leverages the strengths of the two while overcoming their drawbacks. specifically, it trains an optimization proxy, i.e., a machine-learning model that approximates an optimization model, and then refines the proxy with reinforcement learning. this reinforcement learning from optimization proxy (rlop) approach is computationally efficient to train and deploy, and achieves better results than rl or optimization alone. numerical experiments on the new york city dataset show that the rlop approach reduces both the relocation costs and computation time significantly compared to the optimization model, while pure reinforcement learning fails to converge due to computational complexity."
2022,initialization of feature selection search for classification,https://www.jair.org/index.php/jair/article/view/14015,"selecting the best features in a dataset improves accuracy and efficiency of classifiers in a learning process. datasets generally have more features than necessary, some of them being irrelevant or redundant to others. for this reason, numerous feature selection methods have been developed, in which different evaluation functions and measures are applied. this paper proposes the systematic application of individual feature evaluation methods to initialize search-based feature subset selection methods. an exhaustive review of the starting methods used by genetic algorithms from 2014 to 2020 has been carried out. subsequently, an in-depth empirical study has been carried out evaluating the proposal for different search-based feature selection methods (sequential forward and backward selection, las vegas filter and wrapper, simulated annealing and genetic algorithms). since the computation time is reduced and the classification accuracy with the selected features is improved, the initialization of feature selection proposed in this work is proved to be worth considering while designing any feature selection algorithms."
2022,fair in the eyes of others,https://www.jair.org/index.php/jair/article/view/13778,"envy-freeness is a widely studied notion in resource allocation, capturing some aspects of fairness. the notion of envy being inherently subjective though, it might be the case that an agent envies another agent, but that from the other agents' point of view, she has no reason to do so. the difficulty here is to define the notion of objectivity, since no ground-truth can properly serve as a basis of this definition. a natural approach is to consider the judgement of the other agents as a proxy for objectivity. building on previous work by parijs (who introduced ""unanimous envy"") we propose the notion of approval envy: an agent ai experiences approval envy towards aj if she is envious of aj, and sufficiently many agents agree that this should be the case, from their own perspectives. another thoroughly studied notion in resource allocation is proportionality. the same variant can be studied, opening natural questions regarding the links between these two notions. we exhibit several properties of these notions. computing the minimal threshold guaranteeing approval envy and approval non-proportionality clearly inherits well-known intractable results from envy-freeness and proportionality, but (i) we identify some tractable cases such as house allocation; and (ii) we provide a general method based on a mixed integer programming encoding of the problem, which proves to be efficient in practice. this allows us in particular to show experimentally that existence of such allocations, with a rather small threshold, is very often observed."
2022,creative problem solving in artificially intelligent agents: a survey and framework,https://www.jair.org/index.php/jair/article/view/13864,"creative problem solving (cps) is a sub-area within artificial intelligence (ai) that focuses on methods for solving off-nominal, or anomalous problems in autonomous systems. despite many advancements in planning and learning, resolving novel problems or adapting existing knowledge to a new context, especially in cases where the environment may change in unpredictable ways post deployment, remains a limiting factor in the safe and useful integration of intelligent systems. the emergence of increasingly autonomous systems dictates the necessity for ai agents to deal with environmental uncertainty through creativity. to stimulate further research in cps, we present a definition and a framework of cps, which we adopt to categorize existing ai methods in this field. our framework consists of four main components of a cps problem, namely, 1) problem formulation, 2) knowledge representation, 3) method of knowledge manipulation, and 4) method of evaluation. we conclude our survey with open research questions, and suggested directions for the future."
2022,interpretable local concept-based explanation with human feedback to predict all-cause mortality,https://www.jair.org/index.php/jair/article/view/14019,"machine learning models are incorporated in different fields and disciplines in which some of them require a high level of accountability and transparency, for example, the healthcare sector. with the general data protection regulation (gdpr), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. a widely used category of explanation techniques attempts to explain models' predictions by quantifying the importance score of each input feature. however, summarizing such scores to provide human-interpretable explanations is challenging. another category of explanation techniques focuses on learning a domain representation in terms of high-level human-understandable concepts and then utilizing them to explain predictions. these explanations are hampered by how concepts are constructed, which is not intrinsically interpretable. to this end, we propose concept-based local explanations with feedback (clef), a novel local model agnostic explanation framework for learning a set of high-level transparent concept definitions in high-dimensional tabular data that uses clinician-labeled concepts rather than raw features. clef maps the raw input features to high-level intuitive concepts and then decompose the evidence of prediction of the instance being explained into concepts. in addition, the proposed framework generates counterfactual explanations, suggesting the minimum changes in the instance's concept based explanation that will lead to a different prediction. we demonstrate with simulated user feedback on predicting the risk of mortality. such direct feedback is more effective than other techniques, that rely on hand-labelled or automatically extracted concepts, in learning concepts that align with ground truth concept definitions."
2022,computational short cuts in infinite domain constraint satisfaction,https://www.jair.org/index.php/jair/article/view/13787,"a backdoor in a finite-domain csp instance is a set of variables where each possible instantiation moves the instance into a polynomial-time solvable class. backdoors have found many applications in artificial intelligence and elsewhere, and the algorithmic problem of finding such backdoors has consequently been intensively studied. sioutis and janhunen have proposed a generalised backdoor concept suitable for infinite-domain csp instances over binary constraints. we generalise their concept into a large class of csps that allow for higher-arity constraints. we show that this kind of infinite-domain backdoors have many of the positive computational properties that finite-domain backdoors have: the associated computational problems are fixed parameter tractable whenever the underlying constraint language is finite. on the other hand, we show that infinite languages make the problems considerably harder: the general backdoor detection problem is w[2]-hard and fixed-parameter tractability is ruled out under standard complexity-theoretic assumptions. we demonstrate that backdoors may have suboptimal behaviour on binary constraints--this is detrimental from an ai perspective where binary constraints are predominant in, for instance, spatiotemporal applications. in response to this, we introduce sidedoors as an alternative to backdoors. the fundamental computational problems for sidedoors remain fixed-parameter tractable for finite constraint language (possibly also containing non-binary relations). moreover, the sidedoor approach has appealing computational properties that sometimes leads to faster algorithms than the backdoor approach."
2022,solving the watchman route problem with heuristic search,https://www.jair.org/index.php/jair/article/view/13685,"this paper solves the watchman route problem (wrp) on a general discrete graph with heuristic search. given a graph, a line-of-sight (los) function, and a start vertex, the task is to (offline) find a (shortest) path through the graph such that all vertices in the graph will be visually seen by at least one vertex on the path. wrp is reminiscent but different from graph covering and mapping problems, which are done online on an unknown graph. we formalize wrp as a heuristic search problem and solve it optimally with an a*-based algorithm. we develop a series of admissible heuristics with increasing difficulty and accuracy. our heuristics abstract the underlying graph into a disjoint line-of-sight graph (gdls) which is based on disjoint clusters of vertices such that vertices within the same cluster have los to the same specific vertex. we use solutions for the minimum spanning tree (mst) and the traveling salesman problem (tsp) of gdls as admissible heuristics for wrp. we theoretically and empirically investigate these heuristics. then, we show how the optimal methods can be modified (by intelligently pruning away large sub-trees) to obtain various suboptimal solvers with and without bound guarantees. these suboptimal solvers are much faster and expand fewer nodes than the optimal solver with only minor reduction in the quality of the solution."
2022,deepsym: deep symbol generation and rule learning for planning from unsupervised robot interaction,https://www.jair.org/index.php/jair/article/view/13754,"symbolic planning and reasoning are powerful tools for robots tackling complex tasks. however, the need to manually design the symbols restrict their applicability, especially for robots that are expected to act in open-ended environments. therefore symbol formation and rule extraction should be considered part of robot learning, which, when done properly, will offer scalability, flexibility, and robustness. towards this goal, we propose a novel general method that finds action-grounded, discrete object and effect categories and builds probabilistic rules over them for non-trivial action planning. our robot interacts with objects using an initial action repertoire that is assumed to be acquired earlier and observes the effects it can create in the environment. to form action-grounded object, effect, and relational categories, we employ a binary bottleneck layer in a predictive, deep encoderdecoder network that takes the image of the scene and the action applied as input, and generates the resulting effects in the scene in pixel coordinates. after learning, the binary latent vector represents action-driven object categories based on the interaction experience of the robot. to distill the knowledge represented by the neural network into rules useful for symbolic reasoning, a decision tree is trained to reproduce its decoder function. probabilistic rules are extracted from the decision paths of the tree and are represented in the probabilistic planning domain definition language (ppddl), allowing off-the-shelf planners to operate on the knowledge extracted from the sensorimotor experience of the robot. the deployment of the proposed approach for a simulated robotic manipulator enabled the discovery of discrete representations of object properties such as 'rollable' and 'insertable'. in turn, the use of these representations as symbols allowed the generation of effective plans for achieving goals, such as building towers of the desired height, demonstrating the effectiveness of the approach for multi-step object manipulation. finally, we demonstrate that the system is not only restricted to the robotics domain by assessing its applicability to the mnist 8-puzzle domain in which learned symbols allow for the generation of plans that move the empty tile into any given position."
2022,aan+: generalized average attention network for accelerating neural transformer,https://www.jair.org/index.php/jair/article/view/13896,"transformer benefits from the high parallelization of attention networks in fast training, but it still suffers from slow decoding partially due to the linear dependency o(m) of the decoder self-attention on previous target words at inference. in this paper, we propose a generalized average attention network (aan+) aiming at speeding up decoding by reducing the dependency from o(m) to o(1). we find that the learned self-attention weights in the decoder follow some patterns which can be approximated via a dynamic structure. based on this insight, we develop aan+, extending our previously proposed average attention (zhang et al., 2018a, aan) to support more general position- and content-based attention patterns. aan+ only requires to maintain a small constant number of hidden states during decoding, ensuring its o(1) dependency. we apply aan+ as a drop-in replacement of the decoder selfattention and conduct experiments on machine translation (with diverse language pairs), table-to-text generation and document summarization. with masking tricks and dynamic programming, aan+ enables transformer to decode sentences around 20% faster without largely compromising in the training speed and the generation performance. our results further reveal the importance of the localness (neighboring words) in aan+ and its capability in modeling long-range dependency."
2022,communication-aware local search for distributed constraint optimization,https://www.jair.org/index.php/jair/article/view/13826,"most studies investigating models and algorithms for distributed constraint optimization problems (dcops) assume that messages arrive instantaneously and are never lost. specifically, distributed local search dcop algorithms, have been designed as synchronous algorithms (i.e., they perform in synchronous iterations in which each agent exchanges messages with all its neighbors), despite running in asynchronous environments. this is true also for an anytime mechanism that reports the best solution explored during the run of synchronous distributed local search algorithms. thus, when the assumption of perfect communication is relaxed, the properties that were established for the state-of-the-art local search algorithms and the anytime mechanism may not necessarily apply.
in this work, we address this limitation by: (1) proposing a communication-aware dcop model (ca-dcop) that can represent scenarios with different communication disturbances; (2) investigating the performance of existing local search dcop algorithms, specifically distributed stochastic algorithm (dsa) and maximum gain messages (mgm), in the presence of message latency and message loss; (3) proposing a latency-aware monotonic distributed local search dcop algorithm; and (4) proposing an asynchronous anytime framework for reporting the best solution explored by non-monotonic asynchronous local search dcop algorithms. our empirical results demonstrate that imperfect communication has a positive effect on distributed local search algorithms due to increased exploration. furthermore, the asynchronous anytime framework we proposed allows one to benefit from algorithms with inherent explorative heuristics."
2022,low-rank representation of reinforcement learning policies,https://www.jair.org/index.php/jair/article/view/13854,"we propose a general framework for policy representation for reinforcement learning tasks. this framework involves finding a low-dimensional embedding of the policy on a reproducing kernel hilbert space (rkhs). the usage of rkhs based methods allows us to derive strong theoretical guarantees on the expected return of the reconstructed policy. such guarantees are typically lacking in black-box models, but are very desirable in tasks requiring stability and convergence guarantees. we conduct several experiments on classic rl domains. the results confirm that the policies can be robustly represented in a low-dimensional space while the embedded policy incurs almost no decrease in returns."
2022,mean-semivariance policy optimization via risk-averse reinforcement learning,https://www.jair.org/index.php/jair/article/view/13833,"keeping risk under control is often more crucial than maximizing expected reward in real-world decision-making situations, such as finance, robotics, autonomous driving, etc. the most natural choice of risk measures is variance, while it penalizes the upside volatility as much as the downside part. instead, the (downside) semivariance, which captures the negative deviation of a random variable under its mean, is more suitable for risk-averse proposes. this paper aims at optimizing the mean-semivariance (msv) criterion in reinforcement learning w.r.t. steady rewards. since semivariance is time-inconsistent and does not satisfy the standard bellman equation, the traditional dynamic programming methods are inapplicable to msv problems directly. to tackle this challenge, we resort to the perturbation analysis (pa) theory and establish the performance difference formula for msv. we reveal that the msv problem can be solved by iteratively solving a sequence of rl problems with a policy-dependent reward function. further, we propose two on-policy algorithms based on the policy gradient theory and the trust region method. finally, we conduct diverse experiments from simple bandit problems to continuous control tasks in mujoco, which demonstrate the effectiveness of our proposed methods."
2022,planted dense subgraphs in dense random graphs can be recovered using graph-based machine learning,https://www.jair.org/index.php/jair/article/view/13976,"multiple methods of finding the vertices belonging to a planted dense subgraph in a random dense g(n, p) graph have been proposed, with an emphasis on planted cliques. such methods can identify the planted subgraph in polynomial time, but are all limited to several subgraph structures. here, we present pygon, a graph neural network-based algorithm, which is insensitive to the structure of the planted subgraph. this is the first algorithm that uses learning tools for recovering dense subgraphs. we show that pygon can recover cliques of sizes th ( n), where n is the size of the background graph, comparable with the state of the art. we also show that the same algorithm can recover multiple other planted subgraphs of size th ( n), in both directed and undirected graphs. we suggest a conjecture that no polynomial time pac-learning algorithm can detect planted dense subgraphs with size smaller than o ( n), even if in principle one could find dense subgraphs of logarithmic size."
2022,planning with perspectives -- decomposing epistemic planning using functional strips,https://www.jair.org/index.php/jair/article/view/13446,"in this paper, we present a novel approach to epistemic planning called planning with perspectives (pwp) that is both more expressive and computationally more efficient than existing state-of-the-art epistemic planning tools. epistemic planning -- planning with knowledge and belief -- is essential in many multi-agent and human-agent interaction domains. most state-of-the-art epistemic planners solve epistemic planning problems by either compiling to propositional classical planning (for example, generating all possible knowledge atoms or compiling epistemic formulae to normal forms); or explicitly encoding kripke-based semantics. however, these methods become computationally infeasible as problem sizes grow. in this paper, we decompose epistemic planning by delegating reasoning about epistemic formulae to an external solver. we do this by modelling the problem using functional strips, which is more expressive than standard strips and supports the use of external, black-box functions within action models. building on recent work that demonstrates the relationship between what an agent 'sees' and what it knows, we define the perspective of each agent using an external function, and build a solver for epistemic logic around this. modellers can customise the perspective function of agents, allowing new epistemic logics to be defined without changing the planner. we ran evaluations on well-known epistemic planning benchmarks to compare an existing state-of-the-art planner, and on new scenarios that demonstrate the expressiveness of the pwp approach. the results show that our pwp planner scales significantly better than the state-of-the-art planner that we compared against, and can express problems more succinctly."
2022,a survey of methods for automated algorithm configuration,https://www.jair.org/index.php/jair/article/view/13676,"algorithm configuration (ac) is concerned with the automated search of the most suitable parameter configuration of a parametrized algorithm. there is currently a wide variety of ac problem variants and methods proposed in the literature. existing reviews do not take into account all derivatives of the ac problem, nor do they offer a complete classification scheme. to this end, we introduce taxonomies to describe the ac problem and features of configuration methods, respectively. we review existing ac literature within the lens of our taxonomies, outline relevant design choices of configuration approaches, contrast methods and problem variants against each other, and describe the state of ac in industry. finally, our review provides researchers and practitioners with a look at future research directions in the field of ac."
2022,domain adaptation and multi-domain adaptation for neural machine translation: a survey,https://www.jair.org/index.php/jair/article/view/13566,"the development of deep learning techniques has allowed neural machine translation (nmt) models to become extremely powerful, given sufficient training data and training time. however, systems struggle when translating text from a new domain with a distinct style or vocabulary. fine-tuning on in-domain data allows good domain adaptation, but requires sufficient relevant bilingual data. even if this is available, simple fine-tuning can cause overfitting to new data and catastrophic forgetting of previously learned behaviour.
we survey approaches to domain adaptation for nmt, particularly where a system may need to translate across multiple domains. we divide techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure. we finally highlight the benefits of domain adaptation and multidomain adaptation techniques to other lines of nmt research."
2022,multi-agent path finding: a new boolean encoding,https://www.jair.org/index.php/jair/article/view/13818,"multi-agent pathfinding (mapf) is an np-hard problem. as such, dense maps may be very hard to solve optimally. in such scenarios, compilation-based approaches, via boolean satisfiability (sat) and answer set programming (asp), have been shown to outperform heuristic-search-based approaches, such as conflict-based search (cbs). in this paper, we propose a new boolean encoding for mapf, and show how to implement it in asp and maxsat. a feature that distinguishes our encoding from existing ones is that swap and follow conflicts are encoded using binary clauses, which can be exploited by current conflict-driven clause learning (cdcl) solvers. in addition, the number of clauses used to encode swap and follow conflicts do not depend on the number of agents, allowing us to scale better. for maxsat, we study different ways in which we may combine the msu3 and lsu algorithms for maximum performance. in our experimental evaluation, we used square grids, ranging from 20 x 20 to 50 x 50 cells, and warehouse maps, with a varying number of agents and obstacles. we compared against representative solvers of the state-of-the-art, including the search-based algorithm cbs, the asp-based solver asp-mapf, and the branch-and-cut-and-price hybrid solver, bcp. we observe that the asp implementation of our encoding, asp-mapf2 outperforms other solvers in most of our experiments. the maxsat implementation of our encoding, mtms shows best performance in relatively small warehouse maps when the number of agents is large, which are the instances with closer resemblance to hard puzzle-like problems."
2022,on tackling explanation redundancy in decision trees,https://www.jair.org/index.php/jair/article/view/13575,"decision trees (dts) epitomize the ideal of interpretability of machine learning (ml) models. the interpretability of decision trees motivates explainability approaches by so-called intrinsic interpretability, and it is at the core of recent proposals for applying interpretable ml models in high-risk applications. the belief in dt interpretability is justified by the fact that explanations for dt predictions are generally expected to be succinct. indeed, in the case of dts, explanations correspond to dt paths. since decision trees are ideally shallow, and so paths contain far fewer features than the total number of features, explanations in dts are expected to be succinct, and hence interpretable. this paper offers both theoretical and experimental arguments demonstrating that, as long as interpretability of decision trees equates with succinctness of explanations, then decision trees ought not be deemed interpretable. the paper introduces logically rigorous path explanations and path explanation redundancy, and proves that there exist functions for which decision trees must exhibit paths with explanation redundancy that is arbitrarily larger than the actual path explanation. the paper also proves that only a very restricted class of functions can be represented with dts that exhibit no explanation redundancy. in addition, the paper includes experimental results substantiating that path explanation redundancy is observed ubiquitously in decision trees, including those obtained using different tree learning algorithms, but also in a wide range of publicly available decision trees. the paper also proposes polynomial-time algorithms for eliminating path explanation redundancy, which in practice require negligible time to compute. thus, these algorithms serve to indirectly attain irreducible, and so succinct, explanations for decision trees. furthermore, the paper includes novel results related with duality and enumeration of explanations, based on using sat solvers as witness-producing np-oracles."
2022,on efficient reinforcement learning for full-length game of starcraft ii,https://www.jair.org/index.php/jair/article/view/13743,"starcraft ii (sc2) poses a grand challenge for reinforcement learning (rl), of which the main difficulties include huge state space, varying action space, and a long time horizon. in this work, we investigate a set of rl techniques for the full-length game of starcraft ii. we investigate a hierarchical rl approach, where the hierarchy involves two. one is the extracted macro-actions from experts' demonstration trajectories to reduce the action space in an order of magnitude. the other is a hierarchical architecture of neural networks, which is modular and facilitates scale. we investigate a curriculum transfer training procedure that trains the agent from the simplest level to the hardest level. we train the agent on a single machine with 4 gpus and 48 cpu threads. on a 64x64 map and using restrictive units, we achieve a win rate of 99% against the difficulty level-1 built-in ai. through the curriculum transfer learning algorithm and a mixture of combat models, we achieve a 93% win rate against the most difficult non-cheating level built-in ai (level-7). in this extended version of the paper, we improve our architecture to train the agent against the most difficult cheating level ais (level-8, level-9, and level-10). we also test our method on different maps to evaluate the extensibility of our approach. by a final 3-layer hierarchical architecture and applying significant tricks to train sc2 agents, we increase the win rate against the level-8, level-9, and level-10 to 96%, 97%, and 94%, respectively. our codes and models are all open-sourced now at https://github.com/liuruoze/hiernet-sc2.
to provide a baseline referring the alphastar for our work as well as the research and open-source community, we reproduce a scaled-down version of it, mini-alphastar (mas). the latest version of mas is 1.07, which can be trained using supervised learning and reinforcement learning on the raw action space which has 564 actions. it is designed to run training on a single common machine, by making the hyper-parameters adjustable and some settings simplified. we then can compare our work with mas using the same computing resources and training time. by experiment results, we show that our method is more effective when using limited resources. the inference and training codes of mini-alphastar are all open-sourced at https://github.com/liuruoze/mini-alphastar. we hope our study could shed some light on the future research of efficient reinforcement learning on sc2 and other large-scale games."
2022,can we automate scientific reviewing?,https://www.jair.org/index.php/jair/article/view/12862,"the rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. at the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. thus, providing high-quality reviews of this growing number of papers is a significant challenge. in this work, we ask the question ""can we automate scientific reviewing? "", discussing the possibility of using natural language processing (nlp) models to generate peer reviews for scientific papers. because it is non-trivial to define what a ""good"" review is in the first place, we first discuss possible evaluation metrics that could be used to judge success in this task. we then focus on the machine learning domain and collect a dataset of papers in the domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers as input and generate reviews as output. comprehensive experimental results on the test set show that while system-generated reviews are comprehensive, touching upon more aspects of the paper than human-written reviews, the generated texts are less constructive and less factual than human-written reviews for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. given these results, we pose eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research in this direction.
we make relevant resource publicly available for use by future research: https://github. com/neulab/reviewadvisor. in addition, while our conclusion is that the technology is not yet ready for use in high-stakes review settings we provide a system demo, reviewadvisor (http://review.nlpedia.ai/), showing the current capabilities and failings of state-of-the-art nlp models at this task (see demo screenshot in a.2). a review of this paper written by the system proposed in this paper can be found in a.1."
2022,altruistic hedonic games,https://www.jair.org/index.php/jair/article/view/13706,"hedonic games are coalition formation games in which players have preferences over the coalitions they can join. for a long time, all models of representing hedonic games were based upon selfish players only. among the known ways of representing hedonic games compactly, we focus on friend-oriented hedonic games and propose a novel model for them that takes into account not only the players' own preferences but also their friends' preferences. depending on the order in which players look at their own or their friends' preferences, we distinguish three degrees of altruism: selfish-first, equal-treatment, and altruistic-treatment preferences. we study both the axiomatic properties of these games and the computational complexity of problems related to various common stability concepts."
2022,semg-based upper limb movement classifier: current scenario and upcoming challenges,https://www.jair.org/index.php/jair/article/view/13999,"despite achieving accuracies higher than 90% on recognizing upper-limb movements through semg (surface electromyography) signal with the state of art classifiers in the laboratory environment, there are still issues to be addressed for a myo-controlled prosthesis achieve similar performance in real environment conditions. thereby, the main goal of this review is to expose the latest researches in terms of strategies in each block of the system, giving a global view of the current state of academic research. a systematic review was conducted, and the retrieved papers were organized according to the system step related to the proposed method. then, for each stage of the upper limb motion recognition system, the works were described and compared in terms of strategy, methodology and issue addressed. an additional section was destined for the description of works related to signal contamination that is often neglected in reviews focused on semg based motion classifiers. therefore, this section is the main contribution of this paper. deep learning methods are a current trend for classification stage, providing strategies based on time-series and transfer learning to address the issues related to limb position, temporal/inter-subject variation, and electrode displacement. despite the promising strategies presented for contaminant detection, identification, and removal, there are still some factors to be considered, such as the occurrence of simultaneous contaminants. this review exposes the current scenario of the movement classification system, providing valuable information for new researchers and guiding future works towards myo-controlled devices."
2022,motion planning under uncertainty with complex agents and environments via hybrid search,https://www.jair.org/index.php/jair/article/view/13361,"as autonomous systems and robots are applied to more real world situations, they must reason about uncertainty when planning actions. mission success oftentimes cannot be guaranteed and the planner must reason about the probability of failure. unfortunately, computing a trajectory that satisfies mission goals while constraining the probability of failure is difficult because of the need to reason about complex, multidimensional probability distributions. recent methods have seen success using chance-constrained, model-based planning. however, the majority of these methods can only handle simple environment and agent models. we argue that there are two main drawbacks of current approaches to goal-directed motion planning under uncertainty. first, current methods suffer from an inability to deal with expressive environment models such as 3d non-convex obstacles. second, most planners rely on considerable simplifications when computing trajectory risk including approximating the agent's dynamics, geometry, and uncertainty. in this article, we apply hybrid search to the risk-bound, goal-directed planning problem. the hybrid search consists of a region planner and a trajectory planner. the region planner makes discrete choices by reasoning about geometric regions that the autonomous agent should visit in order to accomplish its mission. in formulating the region planner, we propose landmark regions that help produce obstacle-free paths. the region planner passes paths through the environment to a trajectory planner; the task of the trajectory planner is to optimize trajectories that respect the agent's dynamics and the user's desired risk of mission failure. we discuss three approaches to modeling trajectory risk: a cdf-based approach, a sampling-based collocation method, and an algorithm named shooting method monte carlo. these models allow computation of trajectory risk with more complex environments, agent dynamics, geometries, and models of uncertainty than past approaches. a variety of 2d and 3d test cases are presented including a linear case, a dubins car model, and an underwater autonomous vehicle. the method is shown to outperform other methods in terms of speed and utility of the solution. additionally, the models of trajectory risk are shown to better approximate risk in simulation."
2022,multi-agent advisor q-learning,https://www.jair.org/index.php/jair/article/view/13445,"in the last decade, there have been significant advances in multi-agent reinforcement learning (marl) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possible. however, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. an interesting question that arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. in this paper, we provide a principled framework for incorporating action recommendations from online suboptimal advisors in multi-agent settings. we describe the problem of advising multiple intelligent reinforcement agents (admiral) in nonrestrictive general-sum stochastic game environments and present two novel q-learning based algorithms: admiral - decision making (admiral-dm) and admiral - advisor evaluation (admiral-ae), which allow us to improve learning by appropriately incorporating advice from an advisor (admiral-dm), and evaluate the effectiveness of an advisor (admiral-ae). we analyze the algorithms theoretically and provide fixed point guarantees regarding their learning in general-sum stochastic games. furthermore, extensive experiments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors."
2022,rethinking fairness: an interdisciplinary survey of critiques of hegemonic ml fairness approaches,https://www.jair.org/index.php/jair/article/view/13196,"this survey article assesses and compares existing critiques of current fairness-enhancing technical interventions in machine learning (ml) that draw from a range of non-computing disciplines, including philosophy, feminist studies, critical race and ethnic studies, legal studies, anthropology, and science and technology studies. it bridges epistemic divides in order to offer an interdisciplinary understanding of the possibilities and limits of hegemonic computational approaches to ml fairness for producing just outcomes for society's most marginalized. the article is organized according to nine major themes of critique wherein these different fields intersect: 1) how ""fairness"" in ai fairness research gets defined; 2) how problems for ai systems to address get formulated; 3) the impacts of abstraction on how ai tools function and its propensity to lead to technological solutionism; 4) how racial classification operates within ai fairness research; 5) the use of ai fairness measures to avoid regulation and engage in ethics washing; 6) an absence of participatory design and democratic deliberation in ai fairness considerations; 7) data collection practices that entrench ""bias,"" are non-consensual, and lack transparency; 8) the predatory inclusion of marginalized groups into ai systems; and 9) a lack of engagement with ai's long-term social and ethical outcomes. drawing from these critiques, the article concludes by imagining future ml fairness research directions that actively disrupt entrenched power dynamics and structural injustices in society."
2022,fair division of indivisible goods for a class of concave valuations,https://www.jair.org/index.php/jair/article/view/12911,"we study the fair and efficient allocation of a set of indivisible goods among agents, where each good has several copies, and each agent has an additively separable concave valuation function with a threshold. these valuations capture the property of diminishing marginal returns, and they are more general than the well-studied case of additive valuations. we present a polynomial-time algorithm that approximates the optimal nash social welfare (nsw) up to a factor of e1/e 1.445. this matches with the state-of-the-art approximation factor for additive valuations. the computed allocation also satisfies the popular fairness guarantee of envy-freeness up to one good (ef1) up to a factor of 2 + e. for instances without thresholds, it is also approximately pareto-optimal. for instances satisfying a large market property, we show an improved approximation factor. lastly, we show that the upper bounds on the optimal nsw introduced in cole and gkatzelis (2018) and barman et al. (2018) have the same value."
2022,avoiding negative side effects of autonomous systems in the open world,https://www.jair.org/index.php/jair/article/view/13581,"autonomous systems that operate in the open world often use incomplete models of their environment. model incompleteness is inevitable due to the practical limitations in precise model specification and data collection about open-world environments. due to the limited fidelity of the model, agent actions may produce negative side effects (nses) when deployed. negative side effects are undesirable, unmodeled effects of agent actions on the environment. nses are inherently challenging to identify at design time and may affect the reliability, usability and safety of the system. we present two complementary approaches to mitigate the nse via: (1) learning from feedback, and (2) environment shaping. the solution approaches target settings with different assumptions and agent responsibilities. in learning from feedback, the agent learns a penalty function associated with a nse. we investigate the efficiency of different feedback mechanisms, including human feedback and autonomous exploration. the problem is formulated as a multi-objective markov decision process such that optimizing the agent's assigned task is prioritized over mitigating nse. a slack parameter denotes the maximum allowed deviation from the optimal expected reward for the agent's task in order to mitigate nse. in environment shaping, we examine how a human can assist an agent, beyond providing feedback, and utilize their broader scope of knowledge to mitigate the impacts of nse. we formulate the problem as a human-agent collaboration with decoupled objectives. the agent optimizes its assigned task and may produce nse during its operation. the human assists the agent by performing modest reconfigurations of the environment so as to mitigate the impacts of nse, without affecting the agent's ability to complete its assigned task. we present an algorithm for shaping and analyze its properties. empirical evaluations demonstrate the trade-offs in the performance of different approaches in mitigating nse in different settings."
2022,proactive dynamic distributed constraint optimization problems,https://www.jair.org/index.php/jair/article/view/13499,"the distributed constraint optimization problem (dcop) formulation is a powerful tool for modeling multi-agent coordination problems. to solve dcops in a dynamic environment, dynamic dcops (d-dcops) have been proposed to model the inherent dynamism present in many coordination problems. d-dcops solve a sequence of static problems by reacting to changes in the environment as the agents observe them. such reactive approaches ignore knowledge about future changes of the problem. to overcome this limitation, we introduce proactive dynamic dcops (pd-dcops), a novel formalism to model d-dcops in the presence of exogenous uncertainty. in contrast to reactive approaches, pd-dcops are able to explicitly model possible changes of the problem and take such information into account when solving the dynamically changing problem in a proactive manner. the additional expressivity of this formalism allows it to model a wider variety of distributed optimization problems. our work presents both theoretical and practical contributions that advance current dynamic dcop models: (i) we introduce proactive dynamic dcops (pd-dcops), which explicitly model how the dcop will change over time; (ii) we develop exact and heuristic algorithms to solve pd-dcops in a proactive manner; (iii) we provide theoretical results about the complexity of this new class of dcops; and (iv) we empirically evaluate both proactive and reactive algorithms to determine the trade-offs between the two classes. the final contribution is important as our results are the first that identify the characteristics of the problems that the two classes of algorithms excel in."
2022,a few queries go a long way: information-distortion tradeoffs in matching,https://www.jair.org/index.php/jair/article/view/12690,"we consider the one-sided matching problem, where n agents have preferences over n items, and these preferences are induced by underlying cardinal valuation functions. the goal is to match every agent to a single item so as to maximize the social welfare. most of the related literature, however, assumes that the values of the agents are not a priori known, and only access to the ordinal preferences of the agents over the items is provided. consequently, this incomplete information leads to loss of efficiency, which is measured by the notion of distortion. in this paper, we further assume that the agents can answer a small number of queries, allowing us partial access to their values. we study the interplay between elicited cardinal information (measured by the number of queries per agent) and distortion for one-sided matching, as well as a wide range of well-studied related problems. qualitatively, our results show that with a limited number of queries, it is possible to obtain significant improvements over the classic setting, where only access to ordinal information is given."
2022,constraint solving approaches to the business-to-business meeting scheduling problem,https://www.jair.org/index.php/jair/article/view/12670,"the business-to-business meeting scheduling problem consists of scheduling a set of meetings between given pairs of participants to an event, while taking into account participants' availability and accommodation capacity. a crucial aspect of this problem is that breaks in participants' schedules should be avoided as much as possible. it constitutes a challenging combinatorial problem that needs to be solved for many real world brokerage events.
in this paper we present a comparative study of constraint programming (cp), mixedinteger programming (mip) and maximum satisfiability (maxsat) approaches to this problem. the cp approach relies on using global constraints and has been implemented in minizinc to be able to compare cp, lazy clause generation and mip as solving technologies in this setting. we also present a pure mip encoding. finally, an alternative viewpoint is considered under maxsat, showing best performance when considering some implied constraints. experiments conducted on real world instances, as well as on crafted ones, show that the maxsat approach is the one with the best performance for this problem, exhibiting better solving times, sometimes even orders of magnitude smaller than cp and mip."
2022,adaptive greedy versus non-adaptive greedy for influence maximization,https://www.jair.org/index.php/jair/article/view/12997,"we consider the adaptive influence maximization problem: given a network and a budget k, iteratively select k seeds in the network to maximize the expected number of adopters. in the full-adoption feedback model, after selecting each seed, the seed-picker observes all the resulting adoptions. in the myopic feedback model, the seed-picker only observes whether each neighbor of the chosen seed adopts. motivated by the extreme success of greedy-based algorithms/heuristics for influence maximization, we propose the concept of greedy adaptivity gap, which compares the performance of the adaptive greedy algorithm to its non-adaptive counterpart. our first result shows that, for submodular influence maximization, the adaptive greedy algorithm can perform up to a (1 - 1/e)-fraction worse than the non-adaptive greedy algorithm, and that this ratio is tight. more specifically, on one side we provide examples where the performance of the adaptive greedy algorithm is only a (1-1/e) fraction of the performance of the non-adaptive greedy algorithm in four settings: for both feedback models and both the independent cascade model and the linear threshold model. on the other side, we prove that in any submodular cascade, the adaptive greedy algorithm always outputs a (1 - 1/e)-approximation to the expected number of adoptions in the optimal non-adaptive seed choice. our second result shows that, for the general submodular diffusion model with full-adoption feedback, the adaptive greedy algorithm can outperform the non-adaptive greedy algorithm by an unbounded factor. finally, we propose a risk-free variant of the adaptive greedy algorithm that always performs no worse than the non-adaptive greedy algorithm."
2022,ordinal maximin share approximation for goods,https://www.jair.org/index.php/jair/article/view/13317,"in fair division of indivisible goods, l-out-of-d maximin share (mms) is the value that an agent can guarantee by partitioning the goods into d bundles and choosing the l least preferred bundles. most existing works aim to guarantee to all agents a constant fraction of their 1-out-of-n mms. but this guarantee is sensitive to small perturbation in agents' cardinal valuations. we consider a more robust approximation notion, which depends only on the agents' ordinal rankings of bundles. we prove the existence of l-out-of-(l + 1/2)n mms allocations of goods for any integer l >= 1, and present a polynomial-time algorithm that finds a 1-out-of-3n/2 mms allocation when l=1. we further develop an algorithm that provides a weaker ordinal approximation to mms for any l > 1."
2022,objective bayesian nets for integrating consistent datasets,https://www.jair.org/index.php/jair/article/view/13363,"this paper addresses a data integration problem: given several mutually consistent datasets each of which measures a subset of the variables of interest, how can one construct a probabilistic model that fits the data and gives reasonable answers to questions which are under-determined by the data? here we show how to obtain a bayesian network model which represents the unique probability function that agrees with the probability distributions measured by the datasets and otherwise has maximum entropy. we provide a general algorithm, obn-cds, which offers substantial efficiency savings over the standard brute-force approach to determining the maximum entropy probability function. furthermore, we develop modifications to the general algorithm which enable further efficiency savings but which are only applicable in particular situations. we show that there are circumstances in which one can obtain the model (i) directly from the data; (ii) by solving algebraic problems; and (iii) by solving relatively simple independent optimisation problems."
2022,core challenges in embodied vision-language planning,https://www.jair.org/index.php/jair/article/view/13646,"recent advances in the areas of multimodal machine learning and artificial intelligence (ai) have led to the development of challenging tasks at the intersection of computer vision, natural language processing, and embodied ai. whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. in this survey paper, we discuss embodied vision-language planning (evlp) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. we propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for evlp tasks. finally, we present the core challenges that we believe new evlp works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment."
2022,automated reinforcement learning (autorl): a survey and open problems,https://www.jair.org/index.php/jair/article/view/13596,"the combination of reinforcement learning (rl) with deep learning has led to a series of impressive feats, with many believing (deep) rl provides a path towards generally capable agents. however, the success of rl agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. this makes it challenging to use rl for new problems and also limits its full potential. in many other areas of machine learning, automl has shown that it is possible to automate such design choices, and automl has also yielded promising initial results when applied to rl. however, automated reinforcement learning (autorl) involves not only standard applications of automl but also includes additional challenges unique to rl, that naturally produce a different set of methods. as such, autorl has been emerging as an important area of research in rl, providing promise in a variety of applications from rna design to playing games, such as go. given the diversity of methods and environments considered in rl, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. in this survey, we seek to unify the field of autorl, provide a common taxonomy, discuss each area in detail and pose open problems of interest to researchers going forward."
2022,finding and recognizing popular coalition structures,https://www.jair.org/index.php/jair/article/view/13470,"an important aspect of multi-agent systems concerns the formation of coalitions that are stable or optimal in some well-defined way. the notion of popularity has recently received a lot of attention in this context. a partition is popular if there is no other partition in which more agents are better off than worse off. in this paper, we study popularity, strong popularity, and mixed popularity (which is particularly attractive because existence is guaranteed by the minimax theorem) in a variety of coalition formation settings. extending previous work on marriage games, we show that mixed popular partitions in roommate games can be found efficiently via linear programming and a separation oracle. this approach is quite universal, leading to efficient algorithms for verifying whether a given partition is popular and for finding strongly popular partitions (resolving an open problem). by contrast, we prove that both problems become computationally intractable when moving from coalitions of size 2 to coalitions of size 3, even when preferences are strict and globally ranked. moreover, we show that finding popular, strongly popular, and mixed popular partitions in symmetric additively separable hedonic games and symmetric fractional hedonic games is np-hard. together, these results indicate strong boundaries to the tractability of popularity in both ordinal and cardinal models of hedonic games."
2022,out of context: a new clue for context modeling of aspect-based sentiment analysis,https://www.jair.org/index.php/jair/article/view/13410,"aspect-based sentiment analysis (absa) aims to predict the sentiment expressed in a review with respect to a given aspect. the core of absa is to model the interaction between the context and given aspect to extract aspect-related information. in prior work, attention mechanisms and dependency graph networks are commonly adopted to capture the relations between the context and given aspect. and the weighted sum of context hidden states is used as the final representation fed to the classifier. however, the information related to the given aspect may be already discarded and adverse information may be retained in the context modeling processes of existing models. such a problem cannot be solved by subsequent modules due to two reasons. first, their operations are conducted on the encoder-generated context hidden states, whose value cannot be changed after the encoder. second, existing encoders only consider the context while not the given aspect. to address this problem, we argue the given aspect should be considered as a new clue out of context in the context modeling process. as for solutions, we design three streams of aspect-aware context encoders: an aspect-aware lstm, an aspect-aware gcn, and three aspect-aware berts. they are dedicated to generating aspect-aware hidden states which are tailored for the absa task. in these aspect-aware context encoders, the semantics of the given aspect is used to regulate the information flow. consequently, the aspect-related information can be retained and aspect-irrelevant information can be excluded in the generated hidden states. we conduct extensive experiments on several benchmark datasets with empirical analysis, demonstrating the efficacies and advantages of our proposed aspect-aware context encoders."
2022,fast adaptive non-monotone submodular maximization subject to a knapsack constraint,https://www.jair.org/index.php/jair/article/view/13472,"constrained submodular maximization problems encompass a wide variety of applications, including personalized recommendation, team formation, and revenue maximization via viral marketing. the massive instances occurring in modern-day applications can render existing algorithms prohibitively slow. moreover, frequently those instances are also inherently stochastic. focusing on these challenges, we revisit the classic problem of maximizing a (possibly non-monotone) submodular function subject to a knapsack constraint. we present a simple randomized greedy algorithm that achieves a 5.83-approximation and runs in o(n log n) time, i.e., at least a factor n faster than other state-of-the-art algorithms. the versatility of our approach allows us to further transfer it to a stochastic version of the problem. there, we obtain a (9 + e)-approximation to the best adaptive policy, which is the first constant approximation for non-monotone objectives. experimental evaluation of our algorithms showcases their improved performance on real and synthetic data."
2022,planning with critical section macros: theory and practice,https://www.jair.org/index.php/jair/article/view/13269,"macro-operators (macros) are a well-known technique for enhancing performance of planning engines by providing ""short-cuts"" in the state space. existing macro learning systems usually generate macros by considering most frequent action sequences in training plans. unfortunately, frequent action sequences might not capture meaningful activities as a whole, leading to a limited beneficial impact for the planning process.
in this paper, inspired by resource locking in critical sections in parallel computing, we propose a technique that generates macros able to capture whole activities in which limited resources (e.g., a robotic hand, or a truck) are used. specifically, such a critical section macro starts by locking the resource (e.g., grabbing an object), continues by using the resource (e.g., manipulating the object) and finishes by releasing the resource (e.g., dropping the object). hence, such a macro bridges states in which the resource is locked and cannot be used. we also introduce versions of critical section macros dealing with multiple resources and phased locks. usefulness of macros is evaluated using a range of state-of-the-art planners, and a large number of benchmarks from the deterministic and learning tracks of recent editions of the international planning competition."
2022,cooperation and learning dynamics under wealth inequality and diversity in individual risk,https://www.jair.org/index.php/jair/article/view/13519,"we examine how wealth inequality and diversity in the perception of risk of a collective disaster impact cooperation levels in the context of a public goods game with uncertain and non-linear returns. in this game, individuals face a collective-risk dilemma where they may contribute or not to a common pool to reduce their chances of future losses. we draw our conclusions based on social simulations with populations of independent reinforcement learners with diverse levels of risk and wealth. we find that both wealth inequality and diversity in risk assessment can hinder cooperation and augment collective losses. additionally, wealth inequality further exacerbates long term inequality, causing rich agents to become richer and poor agents to become poorer. on the other hand, diversity in risk only amplifies inequality when combined with bias in group assortment--i.e., high probability that agents from the same risk class play together. our results also suggest that taking wealth inequality into account can help to design effective policies aiming at leveraging cooperation in large group sizes, a configuration where collective action is harder to achieve. finally, we characterize the circumstances under which risk perception alignment is crucial and those under which reducing wealth inequality constitutes a deciding factor for collective welfare."
2022,inductive logic programming at 30: a new introduction,https://www.jair.org/index.php/jair/article/view/13507,"inductive logic programming (ilp) is a form of machine learning. the goal of ilp is to induce a hypothesis (a set of logical rules) that generalises training examples. as ilp turns 30, we provide a new introduction to the field. we introduce the necessary logical notation and the main learning settings; describe the building blocks of an ilp system; compare several systems on several dimensions; describe four systems (aleph, tilde, aspal, and metagol); highlight key application areas; and, finally, summarise current limitations and directions for future research."
2022,on the tractability of shap explanations,https://www.jair.org/index.php/jair/article/view/13283,"shap explanations are a popular feature-attribution mechanism for explainable ai. they use game-theoretic notions to measure the influence of individual features on the prediction of a machine learning model. despite a lot of recent interest from both academia and industry, it is not known whether shap explanations of common machine learning models can be computed efficiently. in this paper, we establish the complexity of computing the shap explanation in three important settings. first, we consider fully-factorized data distributions, and show that the complexity of computing the shap explanation is the same as the complexity of computing the expected value of the model. this fully-factorized setting is often used to simplify the shap computation, yet our results show that the computation can be intractable for commonly used models such as logistic regression. going beyond fully-factorized distributions, we show that computing shap explanations is already intractable for a very simple setting: computing shap explanations of trivial classifiers over naive bayes distributions. finally, we show that even computing shap over the empirical distribution is #p-hard."
2022,fond planning with explicit fairness assumptions,https://www.jair.org/index.php/jair/article/view/13599,"we consider the problem of reaching a propositional goal condition in fully-observable nondeterministic (fond) planning under a general class of fairness assumptions that are given explicitly. the fairness assumptions are of the form a/b and say that state trajectories that contain infinite occurrences of an action a from a in a state s and finite occurrence of actions from b, must also contain infinite occurrences of action a in s followed by each one of its possible outcomes. the infinite trajectories that violate this condition are deemed as unfair, and the solutions are policies for which all the fair trajectories reach a goal state. we show that strong and strong-cyclic fond planning, as well as qnp planning, a planning model introduced recently for generalized planning, are all special cases of fond planning with fairness assumptions of this form which can also be combined. fond+ planning, as this form of planning is called, combines the syntax of fond planning with some of the versatility of ltl for expressing fairness constraints. a sound and complete fond+ planner is implemented by reducing fond+ planning to answer set programs, and its performance is evaluated in comparison with fond and qnp planners, and ltl synthesis tools. two other fond+ planners are introduced as well which are more scalable but are not complete."
2022,path counting for grid-based navigation,https://www.jair.org/index.php/jair/article/view/13544,"counting the number of shortest paths on a grid is a simple procedure with close ties to pascal's triangle. we show how path counting can be used to select relatively direct grid paths for ai-related applications involving navigation through spatial environments. typical implementations of dijkstra's algorithm and a* prioritize grid moves in an arbitrary manner, producing paths which stray conspicuously far from line-of-sight trajectories. we find that by counting the number of paths which traverse each vertex, then selecting the vertices with the highest counts, one obtains a path that is reasonably direct in practice and can be improved by refining the grid resolution. central dijkstra and central a* are introduced as the basic methods for computing these central grid paths. theoretical analysis reveals that the proposed grid-based navigation approach is related to an existing grid-based visibility approach, and establishes that central grid paths converge on clear sightlines as the grid spacing approaches zero. a more general property, that central paths converge on direct paths, is formulated as a conjecture."
2022,admissibility in probabilistic argumentation,https://www.jair.org/index.php/jair/article/view/13530,"abstract argumentation is a prominent reasoning framework. it comes with a variety of semantics and has lately been enhanced by probabilities to enable a quantitative treatment of argumentation. while admissibility is a fundamental notion for classical reasoning in abstract argumentation frameworks, it has barely been reflected so far in the probabilistic setting. in this paper, we address the quantitative treatment of abstract argumentation based on probabilistic notions of admissibility. our approach follows the natural idea of defining probabilistic semantics for abstract argumentation by systematically imposing constraints on the joint probability distribution on the sets of arguments, rather than on probabilities of single arguments. as a result, there might be either a uniquely defined distribution satisfying the constraints, but also none, many, or even an infinite number of satisfying distributions are possible. we provide probabilistic semantics corresponding to the classical complete and stable semantics and show how labeling schemes provide a bridge from distributions back to argument labelings. in relation to existing work on probabilistic argumentation, we present a taxonomy of semantic notions. enabled by the constraint-based approach, standard reasoning problems for probabilistic semantics can be tackled by smt solvers, as we demonstrate by a proof-of-concept implementation."
2022,impact of imputation strategies on fairness in machine learning,https://www.jair.org/index.php/jair/article/view/13197,"research on fairness and bias mitigation in machine learning often uses a set of reference datasets for the design and evaluation of novel approaches or definitions. while these datasets are well structured and useful for the comparison of various approaches, they do not reflect that datasets commonly used in real-world applications can have missing values. when such missing values are encountered, the use of imputation strategies is commonplace. however, as imputation strategies potentially alter the distribution of data they can also affect the performance, and potentially the fairness, of the resulting predictions, a topic not yet well understood in the fairness literature. in this article, we investigate the impact of different imputation strategies on classical performance and fairness in classification settings. we find that the selected imputation strategy, along with other factors including the type of classification algorithm, can significantly affect performance and fairness outcomes. the results of our experiments indicate that the choice of imputation strategy is an important factor when considering fairness in machine learning. we also provide some insights and guidance for researchers to help navigate imputation approaches for fairness."
2022,two-phase multi-document event summarization on core event graphs,https://www.jair.org/index.php/jair/article/view/13267,"succinct event description based on multiple documents is critical to news systems as well as search engines. different from existing summarization or event tasks, multi-document event summarization (mes) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. to address these challenges, we propose a two-phase framework for the mes task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. for experiments in the new task, we construct two large-scale real-world datasets for training and assessment. extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized."
2022,supervised visual attention for simultaneous multimodal machine translation,https://www.jair.org/index.php/jair/article/view/13546,"there has been a surge in research in multimodal machine translation (mmt), where additional modalities such as images are used to improve translation quality of textual systems. a particular use for such multimodal systems is the task of simultaneous machine translation, where visual context has been shown to complement the partial information provided by the source sentence, especially in the early phases of translation. in this paper, we propose the first transformer-based simultaneous mmt architecture, which has not been previously explored in simultaneous translation. additionally, we extend this model with an auxiliary supervision signal that guides the visual attention mechanism using labelled phrase-region alignments. we perform comprehensive experiments on three language directions and conduct thorough quantitative and qualitative analyses using both automatic metrics and manual inspection. our results show that (i) supervised visual attention consistently improves the translation quality of the simultaneous mmt models, and (ii) fine-tuning the mmt with supervision loss enabled leads to better performance than training the mmt from scratch. compared to the state-of-the-art, our proposed model achieves improvements of up to 2.3 bleu and 3.5 meteor points."
2022,a comprehensive framework for learning declarative action models,https://www.jair.org/index.php/jair/article/view/13073,"a declarative action model is a compact representation of the state transitions of dynamic systems that generalizes over world objects. the specification of declarative action models is often a complex hand-crafted task. in this paper we formulate declarative action models via state constraints, and present the learning of such models as a combinatorial search. the comprehensive framework presented here allows us to connect the learning of declarative action models to well-known problem solving tasks. in addition, our framework allows us to characterize the existing work in the literature according to four dimensions: (1) the target action models, in terms of the state transitions they define; (2) the available learning examples; (3) the functions used to guide the learning process, and to evaluate the quality of the learned action models; (4) the learning algorithm. last, the paper lists relevant successful applications of the learning of declarative actions models and discusses some open challenges with the aim of encouraging future research work."
2022,evolutionary dynamics and phi-regret minimization in games,https://www.jair.org/index.php/jair/article/view/13187,"regret has been established as a foundational concept in online learning, and likewise has important applications in the analysis of learning dynamics in games. regret quantifies the difference between a learner's performance against a baseline in hindsight. it is well known that regret-minimizing algorithms converge to certain classes of equilibria in games; however, traditional forms of regret used in game theory predominantly consider baselines that permit deviations to deterministic actions or strategies. in this paper, we revisit our understanding of regret from the perspective of deviations over partitions of the full mixed strategy space (i.e., probability distributions over pure strategies), under the lens of the previously-established ph-regret framework, which provides a continuum of stronger regret measures. importantly, ph-regret enables learning agents to consider deviations from and to mixed strategies, generalizing several existing notions of regret such as external, internal, and swap regret, and thus broadening the insights gained from regret-based analysis of learning algorithms. we prove here that the well-studied evolutionary learning algorithm of replicator dynamics (rd) seamlessly minimizes the strongest possible form of ph-regret in generic 2 x 2 games, without any modification of the underlying algorithm itself. we subsequently conduct experiments validating our theoretical results in a suite of 144 2 x 2 games wherein rd exhibits a diverse set of behaviors. we conclude by providing empirical evidence of ph-regret minimization by rd in some larger games, hinting at further opportunity for ph-regret based study of such algorithms from both a theoretical and empirical perspective."
2022,autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey,https://www.jair.org/index.php/jair/article/view/13554,"building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. developmental approaches argue that this can only be achieved by autotelic agents: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. in recent years, the convergence of developmental approaches with deep reinforcement learning (rl) methods has been leading to the emergence of a new field: developmental reinforcement learning. developmental rl is concerned with the use of deep rl algorithms to tackle a developmental problem-- the intrinsically motivated acquisition of open-ended repertoires of skills. the self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. this raises new challenges compared to standard rl algorithms originally designed to tackle pre-defined sets of goals using external reward signals. the present paper introduces developmental rl and proposes a computational framework based on goal-conditioned rl to tackle the intrinsically motivated skills acquisition problem. it proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. we finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition."
2022,collie: continual learning of language grounding from language-image embeddings,https://www.jair.org/index.php/jair/article/view/13689,"this paper presents collie: a simple, yet effective model for continual learning of how language is grounded in vision. given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case clip by openai), collie learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. this is done by predicting the difference vector that needs to be applied, as well as a scaling factor for this vector, so that the adjustment is only applied when needed. unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use and leverage semantic compositionality. we verify the model's performance on two different tasks of identifying the targets of referring expressions, where it has to learn new language use. the results show that the model can efficiently learn and generalize from only a few examples, with little interference with the model's original zero-shot performance."
2022,learning bayesian networks under sparsity constraints: a parameterized complexity analysis,https://www.jair.org/index.php/jair/article/view/13138,"we study the problem of learning the structure of an optimal bayesian network when additional constraints are posed on the network or on its moralized graph. more precisely, we consider the constraint that the network or its moralized graph are close, in terms of vertex or edge deletions, to a sparse graph class p. for example, we show that learning an optimal network whose moralized graph has vertex deletion distance at most k from a graph with maximum degree 1 can be computed in polynomial time when k is constant. this extends previous work that gave an algorithm with such a running time for the vertex deletion distance to edgeless graphs. we then show that further extensions or improvements are presumably impossible. for example, we show that learning optimal networks where the network or its moralized graph have maximum degree 2 or connected components of size at most c, c >= 3, is np-hard. finally, we show that learning an optimal network with at most k edges in the moralized graph presumably has no f(k) * |i|o(1)-time algorithm and that, in contrast, an optimal network with at most k arcs can be computed in 2o(k) * |i|o(1) time where |i| is the total input size."
2022,hebo: pushing the limits of sample-efficient hyper-parameter optimisation,https://www.jair.org/index.php/jair/article/view/13643,"in this work we rigorously analyse assumptions inherent to black-box optimisation hyper-parameter tuning tasks. our results on the bayesmark benchmark indicate that heteroscedasticity and non-stationarity pose significant challenges for black-box optimisers. based on these findings, we propose a heteroscedastic and evolutionary bayesian optimisation solver (hebo). hebo performs non-linear input and output warping, admits exact marginal log-likelihood optimisation and is robust to the values of learned parameters. we demonstrate hebo's empirical efficacy on the neurips 2020 black-box optimisation challenge, where hebo placed first. upon further analysis, we observe that hebo significantly outperforms existing black-box optimisers on 108 machine learning hyperparameter tuning tasks comprising the bayesmark benchmark. our findings indicate that the majority of hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity, multiobjective acquisition ensembles with pareto front solutions improve queried configurations, and robust acquisition maximisers afford empirical advantages relative to their non-robust counterparts. we hope these findings may serve as guiding principles for practitioners of bayesian optimisation."
2022,crossing the conversational chasm: a primer on natural language processing for multilingual task-oriented dialogue systems,https://www.jair.org/index.php/jair/article/view/13083,"in task-oriented dialogue (tod), a user holds a conversation with an artificial agent with the aim of completing a concrete task. although this technology represents one of the central objectives of ai and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., english, chinese). this work provides an extensive overview of existing methods and resources in multilingual tod as an entry point to this exciting and emerging field. we find that the most critical factor preventing the creation of truly multilingual tod systems is the lack of datasets in most languages for both training and evaluation. in fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. hence, state-of-the-art approaches to multilingual tod mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively english), either by means of (i) machine translation or (ii) multilingual representations. these approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. on the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). to overcome this limitation, we draw parallels between components of the tod pipeline and other nlp tasks, which can inspire solutions for learning in low-resource scenarios. finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current tod systems."
2022,recursion in abstract argumentation is hard --- on the complexity of semantics based on weak admissibility,https://www.jair.org/index.php/jair/article/view/13603,"we study the computational complexity of abstract argumentation semantics based on weak admissibility, a recently introduced concept to deal with arguments of self-defeating nature. our results reveal that semantics based on weak admissibility are of much higher complexity (under typical assumptions) compared to all argumentation semantics which have been analysed in terms of complexity so far. in fact, we show pspace-completeness of all non-trivial standard decision problems for weak-admissible based semantics. we then investigate potential tractable fragments and show that restricting the frameworks under consideration to certain graph-classes significantly reduces the complexity. we also show that weak-admissibility based extensions can be computed by dividing the given graph into its strongly connected components (sccs). this technique ensures that the bottleneck when computing extensions is the size of the largest scc instead of the size of the graph itself and therefore contributes to the search for fixed-parameter tractable implementations for reasoning with weak admissibility."
2022,metric-distortion bounds under limited information,https://www.jair.org/index.php/jair/article/view/13338,"in this work, we study the metric distortion problem in voting theory under a limited amount of ordinal information. our primary contribution is threefold. first, we consider mechanisms that perform a sequence of pairwise comparisons between candidates. we show that a popular deterministic mechanism employed in many knockout phases yields distortion o(log m) while eliciting only m - 1 out of the th(m2 ) possible pairwise comparisons, where m represents the number of candidates. our analysis for this mechanism leverages a powerful technical lemma developed by kempe (aaai '20). we also provide a matching lower bound on its distortion. in contrast, we prove that any mechanism which performs fewer than m-1 pairwise comparisons is destined to have unbounded distortion. moreover, we study the power of deterministic mechanisms under incomplete rankings. most notably, when agents provide their k-top preferences we show an upper bound of 6m/k + 1 on the distortion, for any k {1, 2, . . . , m}. thus, we substantially improve over the previous bound of 12m/k established by kempe (aaai '20), and we come closer to matching the best-known lower bound. finally, we are concerned with the sample complexity required to ensure near-optimal distortion with high probability. our main contribution is to show that a random sample of th(m/2 ) voters suffices to guarantee distortion 3 + with high probability, for any sufficiently small > 0. this result is based on analyzing the sensitivity of the deterministic mechanism introduced by gkatzelis, halpern, and shah (focs '20). importantly, all of our sample-complexity bounds are distribution-independent.
from an experimental standpoint, we present several empirical findings on real-life voting applications, comparing the scoring systems employed in practice with a mechanism explicitly minimizing (metric) distortion. interestingly, for our case studies, we find that the winner in the actual competition is typically the candidate who minimizes the distortion."
2022,improving simulated annealing for clique partitioning problems,https://www.jair.org/index.php/jair/article/view/13382,"the clique partitioning problem (cpp) is essential in graph theory with a number of important applications. due to its np-hardness, efficient algorithms for solving this problem are very crucial for practical purposes, and simulated annealing is proved to be effective in state-of-the-art cpp algorithms. however, to make simulated annealing more efficient to solve large-scale cpps, in this paper, we propose a new iterated simulated annealing algorithm. several methods are proposed in our algorithm to improve simulated annealing. first, a new configuration checking strategy based on timestamp is presented and incorporated into simulated annealing to avoid search cycles. afterwards, to enhance the local search ability of simulated annealing and speed up convergence, we combine our simulated annealing with a descent search method to solve the cpp. this method further improves solutions found by simulated annealing, and thus compensates for the local search effect. to further accelerate the convergence speed, we introduce a shrinking factor to decline initial temperature and then propose an iterated local search algorithm based on simulated annealing. additionally, a restart strategy is adopted when the search procedure converges. extensive experiments on benchmark instances of the cpp were carried out, and the results suggest that the proposed simulated annealing algorithm outperforms all the existing heuristic algorithms, including five state-of-the-art algorithms. thus the best-known solutions for 34 instances out of 94 are updated. we also conduct comparative analyses of the proposed strategies and show their effectiveness."
2022,better decision heuristics in cdcl through local search and target phases,https://www.jair.org/index.php/jair/article/view/13666,"on practical applications, state-of-the-art sat solvers dominantly use the conflict-driven clause learning (cdcl) paradigm. an alternative for satisfiable instances is local search solvers, which is more successful on random and hard combinatorial instances. although there have been attempts to combine these methods in one framework, a tight integration which improves the state of the art on a broad set of application instances has been missing. we present a combination of techniques that achieves such an improvement. our first contribution is to maximize in a local search fashion the assignment trail in cdcl, by sticking to and extending promising assignments via a technique called target phases. second, we relax the cdcl framework by again extending promising branches to complete assignments while ignoring conflicts. these assignments are then used as starting point of local search which tries to find improved assignments with fewer unsatisfied clauses. third, these improved assignments are imported back to the cdcl loop where they are used to determine the value assigned to decision variables. finally, the conflict frequency of variables in local search can be exploited during variable selection in branching heuristics of cdcl. we implemented these techniques to improve three representative cdcl solvers (glucose, maplelcm distchronobt, and kissat). experiments on benchmarks from the main tracks of the last three sat competitions from 2019 to 2021 and an additional benchmark set from spectrum allocation show that the techniques bring significant improvements, particularly and not surprisingly, on satisfiable real-world application instances. we claim that these techniques were essential to the large increase in performance witnessed in the sat competition 2020 where kissat and relaxed lcmdcbdl newtech were leading the field followed by cryptominisat-ccnr, which also incorporated similar ideas."
2022,joint optimization of concave scalarized multi-objective reinforcement learning with policy gradient based algorithm,https://www.jair.org/index.php/jair/article/view/13981,"many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. in this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. a policy-gradient based model-free algorithm is proposed for the problem. to compute an estimate of the gradient, an asymptotically biased estimator is proposed. the proposed algorithm is shown to achieve convergence to within an e of the global optima after sampling o(m4 s2/(1-g)8e4) trajectories where g is the discount factor and m is the number of the agents, thus achieving the same dependence on e as the policy gradient algorithm for the standard reinforcement learning."
2022,classical planning in deep latent space,https://www.jair.org/index.php/jair/article/view/13768,"current domain-independent, classical planners require symbolic models of the problem domain and instance as input, resulting in a knowledge acquisition bottleneck. meanwhile, although deep learning has achieved significant success in many fields, the knowledge is encoded in a subsymbolic representation which is incompatible with symbolic systems such as planners. we propose latplan, an unsupervised architecture combining deep learning and classical planning. given only an unlabeled set of image pairs showing a subset of transitions allowed in the environment (training inputs), latplan learns a complete propositional pddl action model of the environment. later, when a pair of images representing the initial and the goal states (planning inputs) is given, latplan finds a plan to the goal state in a symbolic latent space and returns a visualized plan execution. we evaluate latplan using image-based versions of 6 planning domains: 8-puzzle, 15-puzzle, blocksworld, sokoban and two variations of lightsout."
2022,threshold treewidth and hypertree width,https://www.jair.org/index.php/jair/article/view/13661,"treewidth and hypertree width have proven to be highly successful structural parameters in the context of the constraint satisfaction problem (csp). when either of these parameters is bounded by a constant, then csp becomes solvable in polynomial time. however, here the order of the polynomial in the running time depends on the width, and this is known to be unavoidable; therefore, the problem is not fixed-parameter tractable parameterized by either of these width measures. here we introduce an enhancement of tree and hypertree width through a novel notion of thresholds, allowing the associated decompositions to take into account information about the computational costs associated with solving the given csp instance. aside from introducing these notions, we obtain efficient theoretical as well as empirical algorithms for computing threshold treewidth and hypertree width and show that these parameters give rise to fixed-parameter algorithms for csp as well as other, more general problems. we complement our theoretical results with experimental evaluations in terms of heuristics as well as exact methods based on sat/smt encodings."
2022,c-face: using compare face on face hallucination for low-resolution face recognition,https://www.jair.org/index.php/jair/article/view/13816,"face hallucination is a task of generating high-resolution (hr) face images from low-resolution (lr) inputs, which is a subfield of the general image super-resolution. however, most of the previous methods only consider the visual effect, ignoring how to maintain the identity of the face. in this work, we propose a novel face hallucination model, called c-face network, which can generate hr images with high visual quality while preserving the identity information. a face recognition network is used to extract the identity features in the training process. in order to make the reconstructed face images keep the identity information to a great extent, a novel metric, i.e., c-face loss, is proposed. we also propose a new training algorithm to deal with the convergence problem. moreover, since our work mainly focuses on the recognition accuracy of the output, we integrate face recognition into the face hallucination process which ensures that the model can be used in real scenarios. extensive experiments on two large scale face datasets demonstrate that our c-face network has the best performance compared with other state-of-the-art methods."
2022,synthesis and properties of optimally value-aligned normative systems,https://www.jair.org/index.php/jair/article/view/13487,"the value alignment problem is concerned with the design of systems that provably abide by our human values. one approach to this challenge is through the leverage of prescriptive norms that, if carefully designed, are able to steer a multiagent system away from harmful outcomes and towards more beneficial ones. in this work, we first present a general methodology for the automated synthesis of value aligned normative systems, based on a consequentialist view of values. in the second part, we provide analytical tools to examine such value aligned normative systems, namely the shapley value of individual norms and the compatibility of several values under a fixed set of norms. we illustrate all of our contributions with a running example of a society of agents where taxes are collected and redistributed according to a set of parametrised norms."
2022,the computational complexity of relu network training parameterized by data dimensionality,https://www.jair.org/index.php/jair/article/view/13547,"understanding the computational complexity of training simple neural networks with rectified linear units (relus) has recently been a subject of intensive research. closing gaps and complementing results from the literature, we present several results on the parameterized complexity of training two-layer relu networks with respect to various loss functions. after a brief discussion of other parameters, we focus on analyzing the influence of the dimension d of the training data on the computational complexity. we provide running time lower bounds in terms of w[1]-hardness for parameter d and prove that known brute-force strategies are essentially optimal (assuming the exponential time hypothesis). in comparison with previous work, our results hold for a broad(er) range of loss functions, including lp-loss for all p [0, ]. in particular, we improve a known polynomial-time algorithm for constant d and convex loss functions to a more general class of loss functions, matching our running time lower bounds also in these cases."
2022,pricing problems with buyer preselection,https://www.jair.org/index.php/jair/article/view/13704,"we investigate the problem of preselecting a subset of buyers (also called agents) participating in a market so as to optimize the performance of stable outcomes. we consider four scenarios arising from the combination of two stability notions, namely market envy-freeness and agent envy-freeness, with the two state-of-the-art objective functions of social welfare and seller's revenue. when insisting on market envy-freeness, we prove that the problem cannot be approximated within n 1-e (with n being the number of buyers) for any e > 0, under both objective functions; we also provide approximation algorithms with an approximation ratio tight up to subpolynomial multiplicative factors for social welfare and the seller's revenue. the negative result, in particular, holds even for markets with single-minded buyers. we also prove that maximizing the seller's revenue is np-hard even for a single buyer, thus closing a previous open question. under agent envy-freeness and for both objective functions, instead, we design a polynomial time algorithm transforming any stable outcome for a market involving any subset of buyers into a stable outcome for the whole market without worsening its performance. this result creates an interesting middle-ground situation where, if on the one hand buyer preselection cannot improve the performance of agent envy-free outcomes, on the other one it can be used as a tool for simplifying the combinatorial structure of the buyers' valuation functions in a given market. finally, we consider the restricted case of multi-unit markets, where all items are of the same type and are assigned the same price. for these markets, we show that preselection may improve the performance of stable outcomes in all of the four considered scenarios, and design corresponding approximation algorithms."
2022,efficient learning of interpretable classification rules,https://www.jair.org/index.php/jair/article/view/13482,"machine learning has become omnipresent with applications in various safety-critical domains such as medical, law, and transportation. in these domains, high-stake decisions provided by machine learning necessitate researchers to design interpretable models, where the prediction is understandable to a human. in interpretable machine learning, rule-based classifiers are particularly effective in representing the decision boundary through a set of rules comprising input features. examples of such classifiers include decision trees, decision lists, and decision sets. the interpretability of rule-based classifiers is in general related to the size of the rules, where smaller rules are considered more interpretable. to learn such a classifier, the brute-force direct approach is to consider an optimization problem that tries to learn the smallest classification rule that has close to maximum accuracy. this optimization problem is computationally intractable due to its combinatorial nature and thus, the problem is not scalable in large datasets. to this end, in this paper we study the triangular relationship among the accuracy, interpretability, and scalability of learning rule-based classifiers.
the contribution of this paper is an interpretable learning framework imli, that is based on maximum satisfiability (maxsat) for synthesizing classification rules expressible in proposition logic. imli considers a joint objective function to optimize the accuracy and the interpretability of classification rules and learns an optimal rule by solving an appropriately designed maxsat query. despite the progress of maxsat solving in the last decade, the straightforward maxsat-based solution cannot scale to practical classification datasets containing thousands to millions of samples. therefore, we incorporate an efficient incremental learning technique inside the maxsat formulation by integrating mini-batch learning and iterative rule-learning. the resulting framework learns a classifier by iteratively covering the training data, wherein in each iteration, it solves a sequence of smaller maxsat queries corresponding to each mini-batch. in our experiments, imli achieves the best balance among prediction accuracy, interpretability, and scalability. for instance, imli attains a competitive prediction accuracy and interpretability w.r.t. existing interpretable classifiers and demonstrates impressive scalability on large datasets where both interpretable and non-interpretable classifiers fail. as an application, we deploy imli in learning popular interpretable classifiers such as decision lists and decision sets. the source code is available at https://github.com/meelgroup/mlic."
2022,ranking sets of objects: the complexity of avoiding impossibility results,https://www.jair.org/index.php/jair/article/view/13030,"the problem of lifting a preference order on a set of objects to a preference order on a family of subsets of this set is a fundamental problem with a wide variety of applications in ai. the process is often guided by axioms postulating properties the lifted order should have. well-known impossibility results by kannai and peleg and by barbera and pattanaik tell us that some desirable axioms - namely dominance and (strict) independence - are not jointly satisfiable for any linear order on the objects if all non-empty sets of objects are to be ordered. on the other hand, if not all non-empty sets of objects are to be ordered, the axioms are jointly satisfiable for all linear orders on the objects for some families of sets. such families are very important for applications as they allow for the use of lifted orders, for example, in combinatorial voting. in this paper, we determine the computational complexity of recognizing such families. we show that it is \pi_2^p-complete to decide for a given family of subsets whether dominance and independence or dominance and strict independence are jointly satisfiable for all linear orders on the objects if the lifted order needs to be total. furthermore, we show that the problem remains conp-complete if the lifted order can be incomplete. additionally, we show that the complexity of these problems can increase exponentially if the family of sets is not given explicitly but via a succinct domain restriction. finally, we show that it is np-complete to decide for a family of subsets whether dominance and independence or dominance and strict independence are jointly satisfiable for at least one linear order on the objects."
2022,"online relaxation refinement for satisficing planning: on partial delete relaxation, complete hill-climbing, and novelty pruning",https://www.jair.org/index.php/jair/article/view/13153,"in classical ai planning, heuristic functions typically base their estimates on a relaxation of the input task. such relaxations can be more or less precise, and many heuristic functions have a refinement procedure that can be iteratively applied until the desired degree of precision is reached. traditionally, such refinement is performed offline to instantiate the heuristic for the search. however, a natural idea is to perform such refinement online instead, in situations where the heuristic is not sufficiently accurate. we introduce several online-refinement search algorithms, based on hill-climbing and greedy best-first search. our hill-climbing algorithms perform a bounded lookahead, proceeding to a state with lower heuristic value than the root state of the lookahead if such a state exists, or refining the heuristic otherwise to remove such a local minimum from the search space surface. these algorithms are complete if the refinement procedure satisfies a suitable convergence property. we transfer the idea of bounded lookaheads to greedy best-first search with a lightweight lookahead after each expansion, serving both as a method to boost search progress and to detect when the heuristic is inaccurate, identifying an opportunity for online refinement. we evaluate our algorithms with the partial delete relaxation heuristic hcff, which can be refined by treating additional conjunctions of facts as atomic, and whose refinement operation satisfies the convergence property required for completeness. on both the ipc domains as well as on the recently published autoscale benchmarks, our online-refinement search algorithms significantly beat state-of-the-art satisficing planners, and are competitive even with complex portfolios."
2022,jointly learning environments and control policies with projected stochastic gradient ascent,https://www.jair.org/index.php/jair/article/view/13350,"we consider the joint design and control of discrete-time stochastic dynamical systems over a finite time horizon. we formulate the problem as a multi-step optimization problem under uncertainty seeking to identify a system design and a control policy that jointly maximize the expected sum of rewards collected over the time horizon considered. the transition function, the reward function and the policy are all parametrized, assumed known and differentiable with respect to their parameters. we then introduce a deep reinforcement learning algorithm combining policy gradient methods with model-based optimization techniques to solve this problem. in essence, our algorithm iteratively approximates the gradient of the expected return via monte-carlo sampling and automatic differentiation and takes projected gradient ascent steps in the space of environment and policy parameters. this algorithm is referred to as direct environment and policy search (deps). we assess the performance of our algorithm in three environments concerned with the design and control of a mass-spring-damper system, a small-scale off-grid power system and a drone, respectively. in addition, our algorithm is benchmarked against a state-of-the-art deep reinforcement learning algorithm used to tackle joint design and control problems. we show that deps performs at least as well or better in all three environments, consistently yielding solutions with higher returns in fewer iterations. finally, solutions produced by our algorithm are also compared with solutions produced by an algorithm that does not jointly optimize environment and policy parameters, highlighting the fact that higher returns can be achieved when joint optimization is performed."
2022,reward machines: exploiting reward function structure in reinforcement learning,https://www.jair.org/index.php/jair/article/view/12440,"reinforcement learning (rl) methods usually treat reward functions as black boxes. as such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. in most rl applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible - to show the reward function's code to the rl agent so it can exploit the function's internal structure to learn optimal policies in a more sample efficient manner. in this paper, we show how to accomplish this idea in two steps. first, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. we then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. experiments on tabular and continuous domains, across different tasks and rl agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-markovian reward specification."
2022,doubly robust crowdsourcing,https://www.jair.org/index.php/jair/article/view/13304,"large-scale labeled dataset is the indispensable fuel that ignites the ai revolution as we see today. most such datasets are constructed using crowdsourcing services such as amazon mechanical turk which provides noisy labels from non-experts at a fair price. the sheer size of such datasets mandates that it is only feasible to collect a few labels per data point. we formulate the problem of test-time label aggregation as a statistical estimation problem of inferring the expected voting score. by imitating workers with supervised learners and using them in a doubly robust estimation framework, we prove that the variance of estimation can be substantially reduced, even if the learner is a poor approximation. synthetic and real-world experiments show that by combining the doubly robust approach with adaptive worker/item selection rules, we often need much lower label cost to achieve nearly the same accuracy as in the ideal world where all workers label all data points."
2022,preferences single-peaked on a tree: multiwinner elections and structural results,https://www.jair.org/index.php/jair/article/view/12332,"a preference profile is single-peaked on a tree if the candidate set can be equipped with a tree structure so that the preferences of each voter are decreasing from their top candidate along all paths in the tree. this notion was introduced by demange (1982), and subsequently trick (1989b) described an efficient algorithm for deciding if a given profile is single-peaked on a tree. we study the complexity of multiwinner elections under several variants of the chamberlin-courant rule for preferences single-peaked on trees. we show that in this setting the egalitarian version of this rule admits a polynomial-time winner determination algorithm. for the utilitarian version, we prove that winner determination remains np-hard for the borda scoring function; indeed, this hardness results extends to a large family of scoring functions. however, a winning committee can be found in polynomial time if either the number of leaves or the number of internal vertices of the underlying tree is bounded by a constant. to benefit from these positive results, we need a procedure that can determine whether a given profile is single-peaked on a tree that has additional desirable properties (such as, e.g., a small number of leaves). to address this challenge, we develop a structural approach that enables us to compactly represent all trees with respect to which a given profile is single-peaked. we show how to use this representation to efficiently find the best tree for a given profile for use with our winner determination algorithms: given a profile, we can efficiently find a tree with the minimum number of leaves, or a tree with the minimum number of internal vertices among trees on which the profile is single-peaked. we then explore the power and limitations of this framework: we develop polynomial-time algorithms to find trees with the smallest maximum degree, diameter, or pathwidth, but show that it is np-hard to check whether a given profile is single-peaked on a tree that is isomorphic to a given tree, or on a regular tree."
2022,a survey of opponent modeling in adversarial domains,https://www.jair.org/index.php/jair/article/view/12889,"opponent modeling is the ability to use prior knowledge and observations in order to predict the behavior of an opponent. this survey presents a comprehensive overview of existing opponent modeling techniques for adversarial domains, many of which must address stochastic, continuous, or concurrent actions, and sparse, partially observable payoff structures. we discuss all the components of opponent modeling systems, including feature extraction, learning algorithms, and strategy abstractions. these discussions lead us to propose a new form of analysis for describing and predicting the evolution of game states over time. we then introduce a new framework that facilitates method comparison, analyze a representative selection of techniques using the proposed framework, and highlight common trends among recently proposed methods. finally, we list several open problems and discuss future research directions inspired by ai research on opponent modeling and related research in other disciplines."
2022,explainable deep learning: a field guide for the uninitiated,https://www.jair.org/index.php/jair/article/view/13200,"deep neural networks (dnns) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model's input drive its decisions. in countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that dnn decisions are driven by aspects appropriate in the context of its use. the development of methods and studies enabling the explanation of a dnn's decisions has thus blossomed into an active and broad area of research. the field's complexity is exacerbated by competing definitions of what it means ""to explain"" the actions of a dnn and to evaluate an approach's ""ability to explain"". this article offers a field guide to explore the space of explainable deep learning for those in the ai/ml field who are uninitiated. the field guide: i) introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) discusses user-oriented explanation design and future directions. we hope the guide is seen as a starting point for those embarking on this research field."
2022,automatic recognition of the general-purpose communicative functions defined by the iso 24617-2 standard for dialog act annotation,https://www.jair.org/index.php/jair/article/view/13280,"from the perspective of a dialog system, it is important to identify the intention behind the segments in a dialog, since it provides an important cue regarding the information that is present in the segments and how they should be interpreted. iso 24617-2, the standard for dialog act annotation, defines a hierarchically organized set of general-purpose communicative functions which correspond to different intentions that are relevant in the context of a dialog. we explore the automatic recognition of these communicative functions in the dialogbank, which is a reference set of dialogs annotated according to this standard. to do so, we propose adaptations of existing approaches to flat dialog act recognition that allow them to deal with the hierarchical classification problem. more specifically, we propose the use of an end-to-end hierarchical network with cascading outputs and maximum a posteriori path estimation to predict the communicative function at each level of the hierarchy, preserve the dependencies between the functions in the path, and decide at which level to stop. furthermore, since the amount of dialogs in the dialogbank is small, we rely on transfer learning processes to reduce overfitting and improve performance. the results of our experiments show that our approach outperforms both a flat one and hierarchical approaches based on multiple classifiers and that each of its components plays an important role towards the recognition of general-purpose communicative functions."
2022,image captioning as an assistive technology: lessons learned from vizwiz 2020 challenge,https://www.jair.org/index.php/jair/article/view/13113,"image captioning has recently demonstrated impressive progress largely owing to the introduction of neural network algorithms trained on curated dataset like ms-coco. often work in this field is motivated by the promise of deployment of captioning systems in practical applications. however, the scarcity of data and contexts in many competition datasets renders the utility of systems trained on these datasets limited as an assistive technology in real-world settings, such as helping visually impaired people navigate and accomplish everyday tasks. this gap motivated the introduction of the novel vizwiz dataset, which consists of images taken by the visually impaired and captions that have useful, task-oriented information. in an attempt to help the machine learning computer vision field realize its promise of producing technologies that have positive social impact, the curators of the vizwiz dataset host several competitions, including one for image captioning. this work details the theory and engineering from our winning submission to the 2020 captioning competition. our work provides a step towards improved assistive image captioning systems.
this article appears in the special track on ai & society."
2022,neural character-level syntactic parsing for chinese,https://www.jair.org/index.php/jair/article/view/13052,"in this work, we explore character-level neural syntactic parsing for chinese with two typical syntactic formalisms: the constituent formalism and a dependency formalism based on a newly released character-level dependency treebank. prior works in chinese parsing have struggled with whether to de ne words when modeling character interactions. we choose to integrate full character-level syntactic dependency relationships using neural representations from character embeddings and richer linguistic syntactic information from human-annotated character-level parts-of-speech and dependency labels. this has the potential to better understand the deeper structure of chinese sentences and provides a better structural formalism for avoiding unnecessary structural ambiguities. specifically, we first compare two different character-level syntax annotation styles: constituency and dependency. then, we discuss two key problems for character-level parsing: (1) how to combine constituent and dependency syntactic structure in full character-level trees and (2) how to convert from character-level to word-level for both constituent and dependency trees. in addition, we also explore several other key parsing aspects, including di erent character-level dependency annotations and joint learning of parts-of-speech and syntactic parsing. finally, we evaluate our models on the chinese penn treebank (ctb) and our published shanghai jiao tong university chinese character dependency treebank (scdt). the results show the e effectiveness of our model on both constituent and dependency parsing. we further provide empirical analysis and suggest several directions for future study."
2022,casa: conversational aspect sentiment analysis for dialogue understanding,https://www.jair.org/index.php/jair/article/view/12802,"dialogue understanding has always been a bottleneck for many conversational tasks, such as dialogue response generation and conversational question answering. to expedite the progress in this area, we introduce the task of conversational aspect sentiment analysis (casa) that can provide useful fine-grained sentiment information for dialogue understanding and planning. overall, this task extends the standard aspect-based sentiment analysis to the conversational scenario with several major adaptations. to aid the training and evaluation of data-driven methods, we annotate 3,000 chit-chat dialogues (27,198 sentences) with fine-grained sentiment information, including all sentiment expressions, their polarities and the corresponding target mentions. we also annotate an out-of-domain test set of 200 dialogues for robustness evaluation. besides, we develop multiple baselines based on either pretrained bert or self-attention for preliminary study. experimental results show that our bert-based model has strong performances for both in-domain and out-of-domain datasets, and thorough analysis indicates several potential directions for further improvements."
2022,sum-of-products with default values: algorithms and complexity results,https://www.jair.org/index.php/jair/article/view/12370,"weighted counting for constraint satisfaction with default values (#cspd) is a powerful special case of the sum-of-products problem that admits succinct encodings of #csp, #sat, and inference in probabilistic graphical models. we investigate #cspd under the fundamental parameter of incidence treewidth (i.e., the treewidth of the incidence graph of the constraint hypergraph). we show that if the incidence treewidth is bounded, #cspd can be solved in polynomial time. more specifically, we show that the problem is fixed-parameter tractable for the combined parameter incidence treewidth, domain size, and support size (the maximum number of non-default tuples in a constraint). this generalizes known results on the fixed-parameter tractability of #cspd under the combined parameter primal treewidth and domain size. we further prove that the problem is not fixed-parameter tractable if any of the three components is dropped from the parameterization."
2022,migrating techniques from search-based multi-agent path finding solvers to sat-based approach,https://www.jair.org/index.php/jair/article/view/13318,"in the multi-agent path finding problem (mapf) we are given a set of agents each with respective start and goal positions. the task is to find paths for all agents while avoiding collisions, aiming to minimize a given objective function. many mapf solvers were introduced in the past decade for optimizing two specific objective functions: sum-of-costs and makespan. two prominent categories of solvers can be distinguished: search-based solvers and compilation-based solvers. search-based solvers were developed and tested for the sum-of-costs objective, while the most prominent compilation-based solvers that are built around boolean satisfiability (sat) were designed for the makespan objective. very little is known on the performance and relevance of solvers from the compilation-based approach on the sum-of-costs objective. in this paper, we start to close the gap between these cost functions in the compilation-based approach. our main contribution is a new sat-based mapf solver called mdd-sat, that is directly aimed to optimally solve the mapf problem under the sum-of-costs objective function. using both a lower bound on the sum-of-costs and an upper bound on the makespan, mdd-sat is able to generate a reasonable number of boolean variables in our sat encoding. we then further improve the encoding by borrowing ideas from icts, a search-based solver. in addition, we show that concepts applicable in search-based solvers like icts and icbs are applicable in the sat-based approach as well. specifically, we integrate independence detection, a generic technique for decomposing an mapf instance into independent subproblems, into our sat-based approach, and we design a relaxation of our optimal sat-based solver that results in a bounded suboptimal sat-based solver. experimental evaluation on several domains shows that there are many scenarios where our sat-based methods outperform state-of-the-art sum-of-costs search-based solvers, such as variants of the icts and icbs algorithms."
2022,viewpoint: ethical by designer - how to grow ethical designers of artificial intelligence,https://www.jair.org/index.php/jair/article/view/13135,"ethical concerns regarding artificial intelligence (ai) technology have fueled discussions around the ethics training received by ai designers. we claim that training designers for ethical behaviour, understood as habitual application of ethical principles in any situation, can make a significant difference in the practice of research, development, and application of ai systems. building on interdisciplinary knowledge and practical experience from computer science, moral psychology and development, and pedagogy, we propose a functional way to provide this training.
this article appears in the special track on ai & society."
2022,fine-grained prediction of political leaning on social media with unsupervised deep learning,https://www.jair.org/index.php/jair/article/view/13112,"predicting the political leaning of social media users is an increasingly popular task, given its usefulness for electoral forecasts, opinion dynamics models and for studying the political dimension of polarization and disinformation.
here, we propose a novel unsupervised technique for learning fine-grained political leaning from the textual content of social media posts. our technique leverages a deep neural network for learning latent political ideologies in a representation learning task. then, users are projected in a low-dimensional ideology space where they are subsequently clustered. the political leaning of a user is automatically derived from the cluster to which the user is assigned. we evaluated our technique in two challenging classification tasks and we compared it to baselines and other state-of-the-art approaches. our technique obtains the best results among all unsupervised techniques, with micro f1 = 0.426 in the 8-class task and micro f1 = 0.772 in the 3-class task. other than being interesting on their own, our results also pave the way for the development of new and better unsupervised approaches for the detection of fine-grained political leaning."
2022,"visually grounded models of spoken language: a survey of datasets, architectures and evaluation techniques",https://www.jair.org/index.php/jair/article/view/12967,"this survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. several fields have made important contributions to this approach to modeling or mimicking the process of learning language: machine learning, natural language and speech processing, computer vision and cognitive science. the current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. we discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. we then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques."
2022,some inapproximability results of map inference and exponentiated determinantal point processes,https://www.jair.org/index.php/jair/article/view/13288,"we study the computational complexity of two hard problems on determinantal point processes (dpps). one is maximum a posteriori (map) inference, i.e., to find a principal submatrix having the maximum determinant. the other is probabilistic inference on exponentiated dpps (e-dpps), which can sharpen or weaken the diversity preference of dpps with an exponent parameter p. we present several complexity-theoretic hardness results that explain the difficulty in approximating map inference and the normalizing constant for e-dpps. we first prove that unconstrained map inference for an n x n matrix is np-hard to approximate within a factor of 2bn, where b = 10-1013 . this result improves upon the best-known inapproximability factor of (9/8 - ), and rules out the existence of any polynomial-factor approximation algorithm assuming p np. we then show that log-determinant maximization is np-hard to approximate within a factor of 5/4 for the unconstrained case and within a factor of 1 + 10-1013 for the size-constrained monotone case. in particular, log-determinant maximization does not admit a polynomial-time approximation scheme unless p = np. as a corollary of the first result, we demonstrate that the normalizing constant for e-dpps of any (fixed) constant exponent p >= b-1 = 101013 is np-hard to approximate within a factor of 2bpn, which is in contrast to the case of p <= 1 admitting a fully polynomial-time randomized approximation scheme."
2022,samba: a generic framework for secure federated multi-armed bandits,https://www.jair.org/index.php/jair/article/view/13163,"the multi-armed bandit is a reinforcement learning model where a learning agent repeatedly chooses an action (pull a bandit arm) and the environment responds with a stochastic outcome (reward) coming from an unknown distribution associated with the chosen arm. bandits have a wide-range of application such as web recommendation systems. we address the cumulative reward maximization problem in a secure federated learning setting, where multiple data owners keep their data stored locally and collaborate under the coordination of a central orchestration server. we rely on cryptographic schemes and propose samba, a generic framework for secure federated multi-armed bandits. each data owner has data associated to a bandit arm and the bandit algorithm has to sequentially select which data owner is solicited at each time step. we instantiate samba for five bandit algorithms. we show that samba returns the same cumulative reward as the nonsecure versions of bandit algorithms, while satisfying formally proven security properties. we also show that the overhead due to cryptographic primitives is linear in the size of the input, which is confirmed by our proof-of-concept implementation."
2022,survey and evaluation of causal discovery methods for time series,https://www.jair.org/index.php/jair/article/view/13428,"we introduce in this survey the major concepts, models, and algorithms proposed so far to infer causal relations from observational time series, a task usually referred to as causal discovery in time series. to do so, after a description of the underlying concepts and modelling assumptions, we present different methods according to the family of approaches they belong to: granger causality, constraint-based approaches, noise-based approaches, score-based approaches, logic-based approaches, topology-based approaches, and difference-based approaches. we then evaluate several representative methods to illustrate the behaviour of different families of approaches. this illustration is conducted on both artificial and real datasets, with different characteristics. the main conclusions one can draw from this survey is that causal discovery in times series is an active research field in which new methods (in every family of approaches) are regularly proposed, and that no family or method stands out in all situations. indeed, they all rely on assumptions that may or may not be appropriate for a particular dataset."
2022,scalable online planning for multi-agent mdps,https://www.jair.org/index.php/jair/article/view/13261,"we present a scalable tree search planning algorithm for large multi-agent sequential decision problems that require dynamic collaboration. teams of agents need to coordinate decisions in many domains, but naive approaches fail due to the exponential growth of the joint action space with the number of agents. we circumvent this complexity through an approach that allows us to trade computation for approximation quality and dynamically coordinate actions. our algorithm comprises three elements: online planning with monte carlo tree search (mcts), factored representations of local agent interactions with coordination graphs, and the iterative max-plus method for joint action selection. we evaluate our approach on the benchmark sysadmin domain with static coordination graphs and achieve comparable performance with much lower computation cost than our mcts baselines. we also introduce a multi-drone delivery domain with dynamic coordination graphs, and demonstrate how our approach scales to large problems on this domain that are intractable for other mcts methods. we provide an open-source implementation of our algorithm at https://github.com/juliapomdp/factoredvaluemcts.jl."
2022,computational benefits of intermediate rewards for goal-reaching policy learning,https://www.jair.org/index.php/jair/article/view/13326,"many goal-reaching reinforcement learning (rl) tasks have empirically verified that rewarding the agent on subgoals improves convergence speed and practical performance. we attempt to provide a theoretical framework to quantify the computational benefits of rewarding the completion of subgoals, in terms of the number of synchronous value iterations. in particular, we consider subgoals as one-way intermediate states, which can only be visited once per episode and propose two settings that consider these one-way intermediate states: the one-way single-path (owsp) and the one-way multi-path (owmp) settings. in both owsp and owmp settings, we demonstrate that adding intermediate rewards to subgoals is more computationally efficient than only rewarding the agent once it completes the goal of reaching a terminal state. we also reveal a trade-off between computational complexity and the pursuit of the shortest path in the owmp setting: adding intermediate rewards significantly reduces the computational complexity of reaching the goal but the agent may not find the shortest path, whereas with sparse terminal rewards, the agent finds the shortest path at a significantly higher computational cost. we also corroborate our theoretical results with extensive experiments on the minigrid environments using q-learning and some popular deep rl algorithms."
2022,approximating perfect recall when model checking strategic abilities: theory and applications,https://www.jair.org/index.php/jair/article/view/12539,"the model checking problem for multi-agent systems against specifications in the alternating-time temporal logic atl, hence atl*, under perfect recall and imperfect information is known to be undecidable. to tackle this problem, in this paper we investigate a notion of bounded recall under incomplete information. we present a novel three-valued semantics for atl* in this setting and analyse the corresponding model checking problem. we show that the three-valued semantics here introduced is an approximation of the classic two-valued semantics, then give a sound, albeit partial, algorithm for model checking two-valued perfect recall via its approximation as three-valued bounded recall. finally, we extend mcmas, an open-source model checker for atl and other agent specifications, to incorporate bounded recall; we illustrate its use and present experimental results."
2022,get out of the bag! silos in ai ethics education: unsupervised topic modeling analysis of global ai curricula,https://www.jair.org/index.php/jair/article/view/13550,"the domain of artificial intelligence (ai) ethics is not new, with discussions going back at least 40 years. teaching the principles and requirements of ethical ai to students is considered an essential part of this domain, with an increasing number of technical ai courses taught at several higher-education institutions around the globe including content related to ethics. by using latent dirichlet allocation (lda), a generative probabilistic topic model, this study uncovers topics in teaching ethics in ai courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to bloom's taxonomy. in this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from north american universities, 11 from asia, 36 from europe, and 10 from other regions. based on this analysis, we were able to synthesize a model of teaching approaches, which we call bag (build, assess, and govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. we critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. we challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how.
this article appears in the ai & society track."
2022,incremental event calculus for run-time reasoning,https://www.jair.org/index.php/jair/article/view/12695,"we present a system for online, incremental composite event recognition. in streaming environments, the usual case is for data to arrive with a (variable) delay from, and to be revised by, the underlying sources. we propose rtecinc, an incremental version of rtec, a composite event recognition engine with formal, declarative semantics, that has been shown to scale to several real-world data streams. rtec deals with delayed arrival and revision of events by computing all queries from scratch. this is often inefficient since it results in redundant computations. instead, rtecinc deals with delays and revisions in a more efficient way, by updating only the affected queries. we examine rtecinc theoretically, presenting a complexity analysis, and show the conditions in which it outperforms rtec. moreover, we compare rtecinc and rtec experimentally using real-world and synthetic datasets. the results are compatible with our theoretical analysis and show that rtecinc outperforms rtec in many practical cases."
2022,predicting decisions in language based persuasion games,https://www.jair.org/index.php/jair/article/view/13510,"sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. however, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. this paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions.
for this purpose, we conduct an online repeated interaction experiment. at each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. while the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. the expert's payoff, in turn, depends on the number of times the decision-maker chooses the hotel. we also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews' text, and observe substantial differences which can be explained through an equilibrium analysis of the game.
we consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network (dnn) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (dnn-based vs. hand-crafted). our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup."
2022,on the indecisiveness of kelly-strategyproof social choice functions,https://www.jair.org/index.php/jair/article/view/13449,"social choice functions (scfs) map the preferences of a group of agents over some set of alternatives to a non-empty subset of alternatives. the gibbard-satterthwaite theorem has shown that only extremely restrictive scfs are strategyproof when there are more than two alternatives. for set-valued scfs, or so-called social choice correspondences, the situation is less clear. there are miscellaneous -- mostly negative -- results using a variety of strategyproofness notions and additional requirements. the simple and intuitive notion of kelly-strategyproofness has turned out to be particularly compelling because it is weak enough to still allow for positive results. for example, the pareto rule is strategyproof even when preferences are weak, and a number of attractive scfs (such as the top cycle, the uncovered set, and the essential set) are strategyproof for strict preferences. in this paper, we show that, for weak preferences, only indecisive scfs can satisfy strategyproofness. in particular, (i) every strategyproof rank-based scf violates pareto-optimality, (ii) every strategyproof support-based scf (which generalize fishburn's c2 scfs) that satisfies pareto-optimality returns at least one most preferred alternative of every voter, and (iii) every strategyproof non-imposing scf returns the condorcet loser in at least one profile. we also discuss the consequences of these results for randomized social choice."
2022,"neural natural language generation: a survey on multilinguality, multimodality, controllability and learning",https://www.jair.org/index.php/jair/article/view/12918,"developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (nlg). these methods combine generative language learning techniques with neural-networks based frameworks. with a wide range of applications in natural language processing, neural nlg (nnlg) is a new and fast growing field of research. in this state-of-the-art report, we investigate the recent developments and applications of nnlg in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. we summarize the fundamental building blocks of nnlg approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. this report also focuses on the seminal applications of these nnlg models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions."
2022,multiobjective tree-structured parzen estimator,https://www.jair.org/index.php/jair/article/view/13188,"practitioners often encounter challenging real-world problems that involve a simultaneous optimization of multiple objectives in a complex search space. to address these problems, we propose a practical multiobjective bayesian optimization algorithm. it is an extension of the widely used tree-structured parzen estimator (tpe) algorithm, called multiobjective tree-structured parzen estimator (motpe). we demonstrate that motpe approximates the pareto fronts of a variety of benchmark problems and a convolutional neural network design problem better than existing methods through the numerical results. we also investigate how the configuration of motpe affects the behavior and the performance of the method and the effectiveness of asynchronous parallelization of the method based on the empirical results."
2022,fairness in influence maximization through randomization,https://www.jair.org/index.php/jair/article/view/13367,"the influence maximization paradigm has been used by researchers in various fields in order to study how information spreads in social networks. while previously the attention was mostly on efficiency, more recently fairness issues have been taken into account in this scope. in the present paper, we propose to use randomization as a mean for achieving fairness. while this general idea is not new, it has not been applied in this area.
similar to previous works like fish et al. (www '19) and tsang et al. (ijcai '19), we study the maximin criterion for (group) fairness. in contrast to their work however, we model the problem in such a way that, when choosing the seed sets, probabilistic strategies are possible rather than only deterministic ones. we introduce two different variants of this probabilistic problem, one that entails probabilistic strategies over nodes (node-based problem) and a second one that entails probabilistic strategies over sets of nodes (set-based problem). after analyzing the relation between the two probabilistic problems, we show that, while the original deterministic maximin problem was inapproximable, both probabilistic variants permit approximation algorithms that achieve a constant multiplicative factor of 1 - 1/e minus an additive arbitrarily small error that is due to the simulation of the information spread. for the node-based problem, the approximation is achieved by observing that a polynomial-sized linear program approximates the problem well. for the set-based problem, we show that a multiplicative-weight routine can yield the approximation result.
for an experimental study, we provide implementations of multiplicative-weight routines for both the set-based and the node-based problems and compare the achieved fairness values to existing methods. maybe non-surprisingly, we show that the ex-ante values, i.e., minimum expected value of an individual (or group) to obtain the information, of the computed probabilistic strategies are significantly larger than the (ex-post) fairness values of previous methods. this indicates that studying fairness via randomization is a worthwhile path to follow. interestingly and maybe more surprisingly, we observe that even the ex-post fairness values, i.e., fairness values of sets sampled according to the probabilistic strategies computed by our routines, dominate over the fairness achieved by previous methods on many of the instances tested."
2022,the application of machine learning techniques for predicting match results in team sport: a review,https://www.jair.org/index.php/jair/article/view/13509,"predicting the results of matches in sport is a challenging and interesting task. in this paper, we review a selection of studies from 1996 to 2019 that used machine learning for predicting match results in team sport. considering both invasion sports and striking/fielding sports, we discuss commonly applied machine learning algorithms, as well as common approaches related to data and evaluation. our study considers accuracies that have been achieved across different sports, and explores whether evidence exists to support the notion that outcomes of some sports may be inherently more difficult to predict. we also uncover common themes of future research directions and propose recommendations for future researchers. although there remains a lack of benchmark datasets (apart from in soccer), and the differences between sports, datasets and features makes between-study comparisons difficult, as we discuss, it is possible to evaluate accuracy performance in other ways. artificial neural networks were commonly applied in early studies, however, our findings suggest that a range of models should instead be compared. selecting and engineering an appropriate feature set appears to be more important than having a large number of instances. for feature selection, we see potential for greater inter-disciplinary collaboration between sport performance analysis, a sub-discipline of sport science, and machine learning."
2022,a metric space for point process excitations,https://www.jair.org/index.php/jair/article/view/13610,"a multivariate hawkes process enables self- and cross-excitations through a triggering matrix that behaves like an asymmetrical covariance structure, characterizing pairwise interactions between the event types. full-rank estimation of all interactions is often infeasible in empirical settings. models that specialize on a spatiotemporal application alleviate this obstacle by exploiting spatial locality, allowing the dyadic relationships between events to depend only on separation in time and relative distances in real euclidean space. here we generalize this framework to any multivariate hawkes process, and harness it as a vessel for embedding arbitrary event types in a hidden metric space. specifically, we propose a hidden hawkes geometry (hhg) model to uncover the hidden geometry between event excitations in a multivariate point process. the low dimensionality of the embedding regularizes the structure of the inferred interactions. we develop a number of estimators and validate the model by conducting several experiments. in particular, we investigate regional infectivity dynamics of covid-19 in an early south korean record and recent los angeles confirmed cases. by additionally performing synthetic experiments on short records as well as explorations into options markets and the ebola epidemic, we demonstrate that learning the embedding alongside a point process uncovers salient interactions in a broad range of applications."
2022,marginal distance and hilbert-schmidt covariances-based independence tests for multivariate functional data,https://www.jair.org/index.php/jair/article/view/13233,"we study the pairwise and mutual independence testing problem for multivariate functional data. using a basis representation of functional data, we reduce this problem to testing the independence of multivariate data, which may be high-dimensional. for pairwise independence, we apply tests based on distance and hilbert-schmidt covariances as well as their marginal versions, which aggregate these covariances for coordinates of random processes. in the case of mutual independence, we study asymmetric and symmetric aggregating measures of pairwise dependence. a theoretical justification of the test procedures is established. in extensive simulation studies and examples based on a real economic data set, we investigate and compare the performance of the tests in terms of size control and power. an important finding is that tests based on distance and hilbert-schmidt covariances are usually more powerful than their marginal versions under linear dependence, while the reverse is true under non-linear dependence."
2022,agent-based modeling for predicting pedestrian trajectories around an autonomous vehicle,https://www.jair.org/index.php/jair/article/view/13425,"this paper addresses modeling and simulating pedestrian trajectories when interacting with an autonomous vehicle in a shared space. most pedestrian-vehicle interaction models are not suitable for predicting individual trajectories. data-driven models yield accurate predictions but lack generalizability to new scenarios, usually do not run in real time and produce results that are poorly explainable. current expert models do not deal with the diversity of possible pedestrian interactions with the vehicle in a shared space and lack microscopic validation. we propose an expert pedestrian model that combines the social force model and a new decision model for anticipating pedestrian-vehicle interactions. the proposed model integrates different observed pedestrian behaviors, as well as the behaviors of the social groups of pedestrians, in diverse interaction scenarios with a car. we calibrate the model by fitting the parameters values on a training set. we validate the model and evaluate its predictive potential through qualitative and quantitative comparisons with ground truth trajectories. the proposed model reproduces observed behaviors that have not been replicated by the social force model and outperforms the social force model at predicting pedestrian behavior around the vehicle on the used dataset. the model generates explainable and real-time trajectory predictions. additional evaluation on a new dataset shows that the model generalizes well to new scenarios and can be applied to an autonomous vehicle embedded prediction."
2022,adversarial framework with certified robustness for time-series domain via statistical features,https://www.jair.org/index.php/jair/article/view/13543,"time-series data arises in many real-world applications (e.g., mobile health) and deep neural networks (dnns) have shown great success in solving them. despite their success, little is known about their robustness to adversarial attacks. in this paper, we propose a novel adversarial framework referred to as time-series attacks via statistical features (tsa-stat). to address the unique challenges of time-series domain, tsa-stat employs constraints on statistical features of the time-series data to construct adversarial examples. optimized polynomial transformations are used to create attacks that are more effective (in terms of successfully fooling dnns) than those based on additive perturbations. we also provide certified bounds on the norm of the statistical features for constructing adversarial examples. our experiments on diverse real-world benchmark datasets show the effectiveness of tsa-stat in fooling dnns for time-series domain and in improving their robustness."
2022,a logic-based explanation generation framework for classical and hybrid planning problems,https://www.jair.org/index.php/jair/article/view/13431,"in human-aware planning systems, a planning agent might need to explain its plan to a human user when that plan appears to be non-feasible or sub-optimal. a popular approach, called model reconciliation, has been proposed as a way to bring the model of the human user closer to the agent's model. to do so, the agent provides an explanation that can be used to update the model of human such that the agent's plan is feasible or optimal to the human user. existing approaches to solve this problem have been based on automated planning methods and have been limited to classical planning problems only.
in this paper, we approach the model reconciliation problem from a different perspective, that of knowledge representation and reasoning, and demonstrate that our approach can be applied not only to classical planning problems but also hybrid systems planning problems with durative actions and events/processes. in particular, we propose a logic-based framework for explanation generation, where given a knowledge base kba (of an agent) and a knowledge base kbh (of a human user), each encoding their knowledge of a planning problem, and that kba entails a query q (e.g., that a proposed plan of the agent is valid), the goal is to identify an explanation e kba such that when it is used to update kbh, then the updated kbh also entails q. more specifically, we make the following contributions in this paper: (1) we formally define the notion of logic-based explanations in the context of model reconciliation problems; (2) we introduce a number of cost functions that can be used to reflect preferences between explanations; (3) we present algorithms to compute explanations for both classical planning and hybrid systems planning problems; and (4) we empirically evaluate their performance on such problems. our empirical results demonstrate that, on classical planning problems, our approach is faster than the state of the art when the explanations are long or when the size of the knowledge base is small (e.g., the plans to be explained are short). they also demonstrate that our approach is efficient for hybrid systems planning problems.
finally, we evaluate the real-world efficacy of explanations generated by our algorithms through a controlled human user study, where we develop a proof-of-concept visualization system and use it as a medium for explanation communication."
2022,multilingual machine translation: deep analysis of language-specific encoder-decoders,https://www.jair.org/index.php/jair/article/view/12699,"state-of-the-art multilingual machine translation relies on a shared encoder-decoder. in this paper, we propose an alternative approach based on language-specific encoder-decoders, which can be easily extended to new languages by learning their corresponding modules. to establish a common interlingua representation, we simultaneously train n initial languages. our experiments show that the proposed approach improves over the shared encoder-decoder for the initial languages and when adding new languages, without the need to retrain the remaining modules. all in all, our work closes the gap between shared and language-specific encoder-decoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings."
2022,ffci: a framework for interpretable automatic evaluation of summarization,https://www.jair.org/index.php/jair/article/view/13167,"in this paper, we propose ffci, a framework for fine-grained summarization evaluation that comprises four elements: faithfulness (degree of factual consistency with the source), focus (precision of summary content relative to the reference), coverage (recall of summary content relative to the reference), and inter-sentential coherence (document fluency between adjacent sentences). we construct a novel dataset for focus, coverage, and inter-sentential coherence, and develop automatic methods for evaluating each of the four dimensions of ffci based on cross-comparison of evaluation metrics and model-based evaluation methods, including question answering (qa) approaches, semantic textual similarity (sts), next-sentence prediction (nsp), and scores derived from 19 pre-trained language models. we then apply the developed metrics in evaluating a broad range of summarization models across two datasets, with some surprising findings."
2021,optimizing for interpretability in deep neural networks with tree regularization,https://www.jair.org/index.php/jair/article/view/12558,"deep models have advanced prediction in many domains, but their lack of interpretability remains a key barrier to the adoption in many real world applications. there exists a large body of work aiming to help humans understand these black box functions to varying levels of granularity - for example, through distillation, gradients, or adversarial examples. these methods however, all tackle interpretability as a separate process after training. in this work, we take a different approach and explicitly regularize deep models so that they are well-approximated by processes that humans can step through in little time. specifically, we train several families of deep neural networks to resemble compact, axis-aligned decision trees without significant compromises in accuracy. the resulting axis-aligned decision functions uniquely make tree regularized models easy for humans to interpret. moreover, for situations in which a single, global tree is a poor estimator, we introduce a regional tree regularizer that encourages the deep model to resemble a compact, axis-aligned decision tree in predefined, human-interpretable contexts. using intuitive toy examples, benchmark image datasets, and medical tasks for patients in critical care and with hiv, we demonstrate that this new family of tree regularizers yield models that are easier for humans to simulate than l1 or l2 penalties without sacrificing predictive power."
2021,a semi-exact algorithm for quickly computing a maximum weight clique in large sparse graphs,https://www.jair.org/index.php/jair/article/view/12327,"this paper explores techniques to quickly solve the maximum weight clique problem (mwcp) in very large scale sparse graphs. due to their size, and the hardness of mwcp, it is infeasible to solve many of these graphs with exact algorithms. although recent heuristic algorithms make progress in solving mwcp in large graphs, they still need considerable time to get a high-quality solution. in this work, we focus on solving mwcp for large sparse graphs within a short time limit. we propose a new method for mwcp which interleaves clique finding with data reduction rules. we propose novel ideas to make this process efficient, and develop an algorithm called fastwclq. experiments on a broad range of large sparse graphs show that fastwclq finds better solutions than state-of-the-art algorithms while the running time of fastwclq is much shorter than the competitors for most instances. further, fastwclq proves the optimality of its solutions for roughly half of the graphs, all with at least 105 vertices, with an average time of 21 seconds."
2021,finding the hardest formulas for resolution,https://www.jair.org/index.php/jair/article/view/12589,"a cnf formula is harder than another cnf formula with the same number of clauses if it requires a longer resolution proof. in this paper we introduce resolution hardness numbers; they give for m=1,2,... the length of a shortest proof of a hardest formula on m clauses. we compute the first ten resolution hardness numbers, along with the corresponding hardest formulas. to achieve this, we devise a candidate filtering and symmetry breaking search scheme for limiting the number of potential candidates for hardest for- mulas, and an efficient sat encoding for computing a shortest resolution proof of a given candidate formula."
2021,worst-case bounds on power vs. proportion in weighted voting games with an application to false-name manipulation,https://www.jair.org/index.php/jair/article/view/13136,"weighted voting games apply to a wide variety of multi-agent settings. they enable the formalization of power indices which quantify the coalitional power of players. we take a novel approach to the study of the power of big vs. small players in these games. we model small (big) players as having single (multiple) votes. the aggregate relative power of big players is measured w.r.t. their votes proportion. for this ratio, we show small constant worst-case bounds for the shapley-shubik and the deegan-packel indices. in sharp contrast, this ratio is unbounded for the banzhaf index. as an application, we define a false-name strategic normal form game where each big player may split its votes between false identities, and study its various properties. together, our results provide foundations for the implications of players' size, modeled as their ability to split, on their relative power."
2021,on the computational complexity of non-dictatorial aggregation,https://www.jair.org/index.php/jair/article/view/12476,"we investigate when non-dictatorial aggregation is possible from an algorithmic perspective, where non-dictatorial aggregation means that the votes cast by the members of a society can be aggregated in such a way that there is no single member of the society that always dictates the collective outcome. we consider the setting in which the members of a society take a position on a fixed collection of issues, where for each issue several different alternatives are possible, but the combination of choices must belong to a given set x of allowable voting patterns. such a set x is called a possibility domain if there is an aggregator that is non-dictatorial, operates separately on each issue, and returns values among those cast by the society on each issue. we design a polynomial-time algorithm that decides, given a set x of voting patterns, whether or not x is a possibility domain. furthermore, if x is a possibility domain, then the algorithm constructs in polynomial time a non-dictatorial aggregator for x. furthermore, we show that the question of whether a boolean domain x is a possibility domain is in nlogspace. we also design a polynomial-time algorithm that decides whether x is a uniform possibility domain, that is, whether x admits an aggregator that is non-dictatorial even when restricted to any two positions for each issue. as in the case of possibility domains, the algorithm also constructs in polynomial time a uniform non-dictatorial aggregator, if one exists. then, we turn our attention to the case where x is given implicitly, either as the set of assignments satisfying a propositional formula, or as a set of consistent evaluations of a sequence of propositional formulas. in both cases, we provide bounds to the complexity of deciding if x is a (uniform) possibility domain. finally, we extend our results to four types of aggregators that have appeared in the literature: generalized dictatorships, whose outcome is always an element of their input, anonymous aggregators, whose outcome is not affected by permutations of their input, monotone, whose outcome does not change if more individuals agree with it and systematic, which aggregate every issue in the same way."
2021,pure nash equilibria in resource graph games,https://www.jair.org/index.php/jair/article/view/12668,"this paper studies the existence of pure nash equilibria in resource graph games, a general class of strategic games succinctly representing the players' private costs. these games are defined relative to a finite set of resources and the strategy set of each player corresponds to a set of subsets of resources. the cost of a resource is an arbitrary function of the load vector of a certain subset of resources. as our main result, we give complete characterizations of the cost functions guaranteeing the existence of pure nash equilibria for weighted and unweighted players, respectively.
for unweighted players, pure nash equilibria are guaranteed to exist for any choice of the players' strategy space if and only if the cost of each resource is an arbitrary function of the load of the resource itself and linear in the load of all other resources where the linear coefficients of mutual influence of different resources are symmetric. this implies in particular that for any other cost structure there is a resource graph game that does not have a pure nash equilibrium.
for weighted games where players have intrinsic weights and the cost of each resource depends on the aggregated weight of its users, pure nash equilibria are guaranteed to exist if and only if the cost of a resource is linear in all resource loads, and the linear factors of mutual influence are symmetric, or there is no interaction among resources and the cost is an exponential function of the local resource load.
we further discuss the computational complexity of pure nash equilibria in resource graph games showing that for unweighted games where pure nash equilibria are guaranteed to exist, it is conp-complete to decide for a given strategy profile whether it is a pure nash equilibrium. for general resource graph games, we prove that the decision whether a pure nash equilibrium exists is s p 2 -complete."
2021,a theoretical perspective on hyperdimensional computing,https://www.jair.org/index.php/jair/article/view/12664,"hyperdimensional (hd) computing is a set of neurally inspired methods for obtaining highdimensional, low-precision, distributed representations of data. these representations can be combined with simple, neurally plausible algorithms to effect a variety of information processing tasks. hd computing has recently garnered significant interest from the computer hardware community as an energy-efficient, low-latency, and noise-robust tool for solving learning problems. in this review, we present a unified treatment of the theoretical foundations of hd computing with a focus on the suitability of representations for learning."
2021,relevance in belief update,https://www.jair.org/index.php/jair/article/view/12772,"it has been pointed out by katsuno and mendelzon that the so-called agm revision operators, defined by alchourron, gardenfors and makinson, do not behave well in dynamically-changing applications. on that premise, katsuno and mendelzon formally characterized a different type of belief-change operators, typically referred to as km update operators, which, to this date, constitute a benchmark in belief update. in this article, we show that there exist km update operators that yield the same counter-intuitive results as any agm revision operator. against this non-satisfactory background, we prove that a translation of parikh's relevance-sensitive axiom (p), in the realm of belief update, suffices to block this liberal behaviour of km update operators. it is shown, both axiomatically and semantically, that axiom (p) for belief update, essentially, encodes a type of relevance that acts at the possible-worlds level, in the context of which each possible world is locally modified, in the light of new information. interestingly, relevance at the possible-worlds level is shown to be equivalent to a form of relevance that acts at the sentential level, by considering the building blocks of relevance to be the sentences of the language. furthermore, we concretely demonstrate that parikh's notion of relevance in belief update can be regarded as (at least a partial) solution to the frame, ramification and qualification problems, encountered in dynamically-changing worlds. last but not least, a whole new class of well-behaved, relevance-sensitive km update operators is introduced, which generalize forbus' update operator and are perfectly-suited for real-world implementations."
2021,on quantifying literals in boolean logic and its applications to explainable ai,https://www.jair.org/index.php/jair/article/view/12756,"quantified boolean logic results from adding operators to boolean logic for existentially and universally quantifying variables. this extends the reach of boolean logic by enabling a variety of applications that have been explored over the decades. the existential quantification of literals (variable states) and its applications have also been studied in the literature. in this paper, we complement this by introducing and studying universal literal quantification and its applications, particularly to explainable ai. we also provide a novel semantics for quantification, discuss the interplay between variable/literal and existential/universal quantification, and identify some classes of boolean formulas and circuits on which quantification can be done efficiently. literal quantification is more fine-grained than variable quantification as the latter can be defined in terms of the former, leading to a refinement of quantified boolean logic with literal quantification as its primitive."
2021,sunny-as2: enhancing sunny for algorithm selection,https://www.jair.org/index.php/jair/article/view/13116,"sunny is an algorithm selection (as) technique originally tailored for constraint programming (cp). sunny is based on the k-nearest neighbors algorithm and enables one to schedule, from a portfolio of solvers, a subset of solvers to be run on a given cp problem. this approach has proved to be effective for cp problems.
in 2015, the aslib benchmarks were released for comparing as systems coming from disparate fields (e.g., asp, qbf, and sat) and sunny was extended to deal with generic as problems. this led to the development of sunny-as, a prototypical algorithm selector based on sunny for aslib scenarios. a major improvement of sunny-as, called sunny-as2, was then submitted to the open algorithm selection challenge (oasc) in 2017, where it turned out to be the best approach for the runtime minimization of decision problems.
in this work we present the technical advancements of sunny-as2, by detailing through several empirical evaluations and by providing new insights. its current version, built on the top of the preliminary version submitted to oasc, is able to outperform sunny-as and other state-of-the-art as methods, including those who did not attend the challenge."
2021,a survey of algorithms for black-box safety validation of cyber-physical systems,https://www.jair.org/index.php/jair/article/view/12716,"autonomous cyber-physical systems (cps) can improve safety and efficiency for safety-critical applications, but require rigorous testing before deployment. the complexity of these systems often precludes the use of formal verification and real-world testing can be too dangerous during development. therefore, simulation-based techniques have been developed that treat the system under test as a black box operating in a simulated environment. safety validation tasks include finding disturbances in the environment that cause the system to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques for cps with a focus on applied algorithms and their modifications for the safety validation problem. we present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. problem decomposition techniques are presented to help scale algorithms to large state spaces, which are common for cps. a brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. finally, we present a survey of existing academic and commercially available safety validation tools."
2021,nlp methods for extraction of symptoms from unstructured data for use in prognostic covid-19 analytic models,https://www.jair.org/index.php/jair/article/view/12631,"statistical modeling of outcomes based on a patient's presenting symptoms (symptomatology) can help deliver high quality care and allocate essential resources, which is especially important during the covid-19 pandemic. patient symptoms are typically found in unstructured notes, and thus not readily available for clinical decision making. in an attempt to fill this gap, this study compared two methods for symptom extraction from emergency department (ed) admission notes. both methods utilized a lexicon derived by expanding the center for disease control and prevention's (cdc) symptoms of coronavirus list. the first method utilized a word2vec model to expand the lexicon using a dictionary mapping to the uni ed medical language system (umls). the second method utilized the expanded lexicon as a rule-based gazetteer and the umls. these methods were evaluated against a manually annotated reference (f1-score of 0.87 for umls-based ensemble; and 0.85 for rule-based gazetteer with umls). through analyses of associations of extracted symptoms used as features against various outcomes, salient risks among the population of covid-19 patients, including increased risk of in-hospital mortality (or 1.85, p-value < 0.001), were identified for patients presenting with dyspnea. disparities between english and non-english speaking patients were also identified, the most salient being a concerning finding of opposing risk signals between fatigue and in-hospital mortality (non-english: or 1.95, p-value = 0.02; english: or 0.63, p-value = 0.01). while use of symptomatology for modeling of outcomes is not unique, unlike previous studies this study showed that models built using symptoms with the outcome of in-hospital mortality were not significantly different from models using data collected during an in-patient encounter (auc of 0.9 with 95% ci of [0.88, 0.91] using only vital signs; auc of 0.87 with 95% ci of [0.85, 0.88] using only symptoms). these findings indicate that prognostic models based on symptomatology could aid in extending covid-19 patient care through telemedicine, replacing the need for in-person options. the methods presented in this study have potential for use in development of symptomatology-based models for other diseases, including for the study of post-acute sequelae of covid-19 (pasc)."
2021,optimal any-angle pathfinding on a sphere,https://www.jair.org/index.php/jair/article/view/12483,"pathfinding in euclidean space is a common problem faced in robotics and computer games. for long-distance navigation on the surface of the earth or in outer space however, approximating the geometry as euclidean can be insufficient for real-world applications such as the navigation of spacecraft, aeroplanes, drones and ships. this article describes an any-angle pathfinding algorithm for calculating the shortest path between point pairs over the surface of a sphere. introducing several novel adaptations, it is shown that anya as described by harabor & grastien for euclidean space can be extended to spherical geometry. there, where the shortest-distance line between coordinates is defined instead by a great-circle path, the optimal solution is typically a curved line in euclidean space. in addition the turning points for optimal paths in spherical geometry are not necessarily corner points as they are in euclidean space, as will be shown, making further substantial adaptations to anya necessary. spherical anya returns the optimal path on the sphere, given these different properties of world maps defined in spherical geometry. it preserves all primary benefits of anya in euclidean geometry, namely the spherical anya algorithm always returns an optimal path on a sphere and does so entirely on-line, without any preprocessing or large memory overheads. performance benchmarks are provided for several game maps including starcraft and warcraft iii as well as for sea navigation on earth using the noaa bathymetric dataset. always returning the shorter path compared with the euclidean approximation yielded by anya, spherical anya is shown to be faster than anya for the majority of sea routes and slower for game maps and random maps."
2021,optimally deceiving a learning leader in stackelberg games,https://www.jair.org/index.php/jair/article/view/12542,"recent results have shown that algorithms for learning the optimal commitment in a stackelberg game are susceptible to manipulation by the follower. these learning algorithms operate by querying the best responses of the follower, who consequently can deceive the algorithm by using fake best responses, typically by responding according to fake payoffs that are different from the actual ones. for this strategic behavior to be successful, the main challenge faced by the follower is to pinpoint the fake payoffs that would make the learning algorithm output a commitment that benefits them the most. while this problem has been considered before, the related literature has only focused on a simple setting where the follower can only choose from a finite set of payoff matrices, thus leaving the general version of the problem unanswered. in this paper, we fill this gap by showing that it is always possible for the follower to efficiently compute (near-)optimal fake payoffs, for various scenarios of learning interaction between the leader and the follower. our results also establish an interesting connection between the follower's deception and the leader's maximin utility: through deception, the follower can induce almost any (fake) stackelberg equilibrium if and only if the leader obtains at least their maximin utility in this equilibrium."
2021,contrastive explanations of plans through model restrictions,https://www.jair.org/index.php/jair/article/view/12813,"in automated planning, the need for explanations arises when there is a mismatch between a proposed plan and the user's expectation. we frame explainable ai planning as an iterative plan exploration process, in which the user asks a succession of contrastive questions that lead to the generation and solution of hypothetical planning problems that are restrictions of the original problem. the object of the exploration is for the user to understand the constraints that govern the original plan and, ultimately, to arrive at a satisfactory plan. we present the results of a user study that demonstrates that when users ask questions about plans, those questions are usually contrastive, i.e. ""why a rather than b?"". we use the data from this study to construct a taxonomy of user questions that often arise during plan exploration. our approach to iterative plan exploration is a process of successive model restriction. each contrastive user question imposes a set of constraints on the planning problem, leading to the construction of a new hypothetical planning problem as a restriction of the original. solving this restricted problem results in a plan that can be compared with the original plan, admitting a contrastive explanation. we formally define model-based compilations in pddl2.1 for each type of constraint derived from a contrastive user question in the taxonomy, and empirically evaluate the compilations in terms of computational complexity. the compilations were implemented as part of an explanation framework supporting iterative model restriction. we demonstrate its benefits in a second user study."
2021,multilabel classification with partial abstention: bayes-optimal prediction under label independence,https://www.jair.org/index.php/jair/article/view/12610,"in contrast to conventional (single-label) classification, the setting of multilabel classification (mlc) allows an instance to belong to several classes simultaneously. thus, instead of selecting a single class label, predictions take the form of a subset of all labels. in this paper, we study an extension of the setting of mlc, in which the learner is allowed to partially abstain from a prediction, that is, to deliver predictions on some but not necessarily all class labels. this option is useful in cases of uncertainty, where the learner does not feel confident enough on the entire label set. adopting a decision-theoretic perspective, we propose a formal framework of mlc with partial abstention, which builds on two main building blocks: first, the extension of underlying mlc loss functions so as to accommodate abstention in a proper way, and second the problem of optimal prediction, that is, finding the bayes-optimal prediction minimizing this generalized loss in expectation. it is well known that different (generalized) loss functions may have different risk-minimizing predictions, and finding the bayes predictor typically comes down to solving a computationally complexity optimization problem. in the most general case, given a prediction of the (conditional) joint distribution of possible labelings, the minimizer of the expected loss needs to be found over a number of candidates which is exponential in the number of class labels. we elaborate on properties of risk minimizers for several commonly used (generalized) mlc loss functions, show them to have a specific structure, and leverage this structure to devise efficient methods for computing bayes predictors. experimentally, we show mlc with partial abstention to be effective in the sense of reducing loss when being allowed to abstain."
2021,output space entropy search framework for multi-objective bayesian optimization,https://www.jair.org/index.php/jair/article/view/12966,"we consider the problem of black-box multi-objective optimization (moo) using expensive function evaluations (also referred to as experiments), where the goal is to approximate the true pareto set of solutions by minimizing the total resource cost of experiments. for example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. the key challenge is to select the sequence of experiments to uncover high-quality solutions using minimal resources. in this paper, we propose a general framework for solving moo problems based on the principle of output space entropy (ose) search: select the experiment that maximizes the information gained per unit resource cost about the true pareto front. we appropriately instantiate the principle of ose search to derive efficient algorithms for the following four moo problem settings: 1) the most basic single-fidelity setting, where experiments are expensive and accurate; 2) handling black-box constraints which cannot be evaluated without performing experiments; 3) the discrete multi-fidelity setting, where experiments can vary in the amount of resources consumed and their evaluation accuracy; and 4) the continuous-fidelity setting, where continuous function approximations result in a huge space of experiments. experiments on diverse synthetic and real-world benchmarks show that our ose search based algorithms improve over state-of-the-art methods in terms of both computational-efficiency and accuracy of moo solutions."
2021,analysis of the impact of randomization of search-control parameters in monte-carlo tree search,https://www.jair.org/index.php/jair/article/view/12065,"monte-carlo tree search (mcts) has been applied successfully in many domains, including games. however, its performance is not uniform on all domains, and it also depends on how parameters that control the search are set. parameter values that are optimal for a task might be sub-optimal for another. in a domain that tackles many games with different characteristics, like general game playing (ggp), selecting appropriate parameter settings is not a trivial task. games are unknown to the player, thus, finding optimal parameters for a given game in advance is not feasible. previous work has looked into tuning parameter values online, while the game is being played, showing some promising results. this tuning approach looks for optimal parameter values, balancing exploitation of values that performed well so far in the search and exploration of less sampled values. continuously changing parameter values while performing the search, combined also with exploration of multiple values, introduces some randomization in the process. in addition, previous research indicates that adding randomization to certain components of mcts might increase the diversification of the search and improve the performance. therefore, this article investigates the effect of randomly selecting values for mcts search-control parameters online among predefined sets of reasonable values. for the ggp domain, this article evaluates four different online parameter randomization strategies by comparing them with other methods to set parameter values: online parameter tuning, offline parameter tuning and sub-optimal parameter choices. results on a set of 14 heterogeneous abstract games show that randomizing parameter values before each simulation has a positive effect on the search in some of the tested games, with respect to using fixed offline-tuned parameters. moreover, results show a clear distinction between games for which online parameter tuning works best and games for which online randomization works best. in addition, the overall performance of online parameter randomization is closer to the one of online parameter turning than the one of sub-optimal parameter values, showing that online randomization is a reasonable parameter selection strategy. when analyzing the structure of the search trees generated by agents that use the different parameters selection strategies, it is clear that randomization causes mcts to become more explorative, which is helpful for alignment games that present many winning paths in their trees. online parameter tuning, instead, seems more suitable for games that present narrow winning paths and many losing paths."
2021,multi-label classification neural networks with hard logical constraints,https://www.jair.org/index.php/jair/article/view/12850,"multi-label classification (mc) is a standard machine learning problem in which a data point can be associated with a set of classes. a more challenging scenario is given by hierarchical multi-label classification (hmc) problems, in which every prediction must satisfy a given set of hard constraints expressing subclass relationships between classes. in this article, we propose c-hmcnn(h), a novel approach for solving hmc problems, which, given a network h for the underlying mc problem, exploits the hierarchy information in order to produce predictions coherent with the constraints and to improve performance. furthermore, we extend the logic used to express hmc constraints in order to be able to specify more complex relations among the classes and propose a new model ccn(h), which extends c-hmcnn(h) and is again able to satisfy and exploit the constraints to improve performance. we conduct an extensive experimental analysis showing the superior performance of both c-hmcnn(h) and ccn(h) when compared to state-of-the-art models in both the hmc and the general mc setting with hard logical constraints."
2021,task-aware verifiable rnn-based policies for partially observable markov decision processes,https://www.jair.org/index.php/jair/article/view/12963,"partially observable markov decision processes (pomdps) are models for sequential decision-making under uncertainty and incomplete information. machine learning methods typically train recurrent neural networks (rnn) as effective representations of pomdp policies that can efficiently process sequential data. however, it is hard to verify whether the pomdp driven by such rnn-based policies satisfies safety constraints, for instance, given by temporal logic specifications. we propose a novel method that combines techniques from machine learning with the field of formal methods: training an rnn-based policy and then automatically extracting a so-called finite-state controller (fsc) from the rnn. such fscs offer a convenient way to verify temporal logic constraints. implemented on a pomdp, they induce a markov chain, and probabilistic verification methods can efficiently check whether this induced markov chain satisfies a temporal logic specification. using such methods, if the markov chain does not satisfy the specification, a byproduct of verification is diagnostic information about the states in the pomdp that are critical for the specification. the method exploits this diagnostic information to either adjust the complexity of the extracted fsc or improve the policy by performing focused retraining of the rnn. the method synthesizes policies that satisfy temporal logic specifications for pomdps with up to millions of states, which are three orders of magnitude larger than comparable approaches."
2021,experimental comparison and survey of twelve time series anomaly detection algorithms,https://www.jair.org/index.php/jair/article/view/12698,"the existence of an anomaly detection method that is optimal for all domains is a myth. thus, there exists a plethora of anomaly detection methods which increases every year for a wide variety of domains. but a strength can also be a weakness; given this massive library of methods, how can one select the best method for their application? current literature is focused on creating new anomaly detection methods or large frameworks for experimenting with multiple methods at the same time. however, and especially as the literature continues to expand, an extensive evaluation of every anomaly detection method is simply not feasible. to reduce this evaluation burden, we present guidelines to intelligently choose the optimal anomaly detection methods based on the characteristics the time series displays such as seasonality, trend, level change concept drift, and missing time steps. we provide a comprehensive experimental validation and survey of twelve anomaly detection methods over different time series characteristics to form guidelines based on several metrics: the auc (area under the curve), windowed f-score, and numenta anomaly benchmark (nab) scoring model. applying our methodologies can save time and effort by surfacing the most promising anomaly detection methods instead of experimenting extensively with a rapidly expanding library of anomaly detection methods, especially in an online setting."
2021,flexible bayesian nonlinear model configuration,https://www.jair.org/index.php/jair/article/view/13047,"regression models are used in a wide range of applications providing a powerful scientific tool for researchers from different fields. linear, or simple parametric, models are often not sufficient to describe complex relationships between input variables and a response. such relationships can be better described through flexible approaches such as neural networks, but this results in less interpretable models and potential overfitting. alternatively, specific parametric nonlinear functions can be used, but the specification of such functions is in general complicated. in this paper, we introduce a flexible approach for the construction and selection of highly flexible nonlinear parametric regression models. nonlinear features are generated hierarchically, similarly to deep learning, but have additional flexibility on the possible types of features to be considered. this flexibility, combined with variable selection, allows us to find a small set of important features and thereby more interpretable models. within the space of possible functions, a bayesian approach, introducing priors for functions based on their complexity, is considered. a genetically modified mode jumping markov chain monte carlo algorithm is adopted to perform bayesian inference and estimate posterior probabilities for model averaging. in various applications, we illustrate how our approach is used to obtain meaningful nonlinear models. additionally, we compare its predictive performance with several machine learning algorithms."
2021,graph kernels: a survey,https://www.jair.org/index.php/jair/article/view/13225,"graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. during the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. the goal of this survey is to provide a unifying view of the literature on graph kernels. in particular, we present a comprehensive overview of a wide range of graph kernels. furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed."
2021,steady-state planning in expected reward multichain mdps,https://www.jair.org/index.php/jair/article/view/12611,"the planning domain has experienced increased interest in the formal synthesis of decision-making policies. this formal synthesis typically entails finding a policy which satisfies formal specifications in the form of some well-defined logic. while many such logics have been proposed with varying degrees of expressiveness and complexity in their capacity to capture desirable agent behavior, their value is limited when deriving decision-making policies which satisfy certain types of asymptotic behavior in general system models. in particular, we are interested in specifying constraints on the steady-state behavior of an agent, which captures the proportion of time an agent spends in each state as it interacts for an indefinite period of time with its environment. this is sometimes called the average or expected behavior of the agent and the associated planning problem is faced with significant challenges unless strong restrictions are imposed on the underlying model in terms of the connectivity of its graph structure. in this paper, we explore this steady-state planning problem that consists of deriving a decision-making policy for an agent such that constraints on its steady-state behavior are satisfied. a linear programming solution for the general case of multichain markov decision processes (mdps) is proposed and we prove that optimal solutions to the proposed programs yield stationary policies with rigorous guarantees of behavior."
2021,teaching people by justifying tree search decisions: an empirical study in curling,https://www.jair.org/index.php/jair/article/view/13219,"in this research note we show that a simple justification system can be used to teach humans non-trivial strategies of the olympic sport of curling. this is achieved by justifying the decisions of kernel regression uct (kr-uct), a tree search algorithm that derives curling strategies by playing the game with itself. given an action returned by kr-uct and the expected outcome of that action, we use a decision tree to produce a counterfactual justification of kr-uct's decision. the system samples other possible outcomes and selects for presentation the outcomes that are most similar to the expected outcome in terms of visual features and most different in terms of expected end-game value. a user study with 122 people shows that the participants who had access to the justifications produced by our system achieved much higher scores in a curling test than those who only observed the decision made by kr-uct and those with access to the justifications of a baseline system. this is, to the best of our knowledge, the first work showing that a justification system is able to teach humans non-trivial strategies learned by an algorithm operating in self play."
2021,reasoning with pcp-nets,https://www.jair.org/index.php/jair/article/view/13009,"we introduce pcp-nets, a formalism to model qualitative conditional preferences with probabilistic uncertainty. pcp-nets generalise cp-nets by allowing for uncertainty over the preference orderings. we define and study both optimality and dominance queries in pcp-nets, and we propose a tractable approximation of dominance which we show to be very accurate in our experimental setting. since pcp-nets can be seen as a way to model a collection of weighted cp-nets, we also explore the use of pcp-nets in a multi-agent context, where individual agents submit cp-nets which are then aggregated into a single pcp-net. we consider various ways to perform such aggregation and we compare them via two notions of scores, based on well known voting theory concepts. experimental results allow us to identify the aggregation method that better represents the given set of cp-nets and the most efficient dominance procedure to be used in the multi-agent context."
2021,learning realistic patterns from visually unrealistic stimuli: generalization and data anonymization,https://www.jair.org/index.php/jair/article/view/13252,"good training data is a prerequisite to develop useful machine learning applications. however, in many domains existing data sets cannot be shared due to privacy regulations (e.g., from medical studies). this work investigates a simple yet unconventional approach for anonymized data synthesis to enable third parties to benefit from such anonymized data. we explore the feasibility of learning implicitly from visually unrealistic, task-relevant stimuli, which are synthesized by exciting the neurons of a trained deep neural network. as such, neuronal excitation can be used to generate synthetic stimuli. the stimuli data is used to train new classification models. furthermore, we extend this framework to inhibit representations that are associated with specific individuals. we use sleep monitoring data from both an open and a large closed clinical study, and electroencephalogram sleep stage classification data, to evaluate whether (1) end-users can create and successfully use customized classification models, and (2) the identity of participants in the study is protected. extensive comparative empirical investigation shows that different algorithms trained on the stimuli are able to generalize successfully on the same task as the original model. architectural and algorithmic similarity between new and original models play an important role in performance. for similar architectures, the performance is close to that of using the original data (e.g., accuracy difference of 0.56%-3.82%, kappa coefficient difference of 0.02-0.08). further experiments show that the stimuli can provide state-ofthe-art resilience against adversarial association and membership inference attacks."
2021,on the online coalition structure generation problem,https://www.jair.org/index.php/jair/article/view/12989,"we consider the online version of the coalition structure generation problem, in which agents, corresponding to the vertices of a graph, appear in an online fashion and have to be partitioned into coalitions by an authority (i.e., an online algorithm). when an agent appears, the algorithm has to decide whether to put the agent into an existing coalition or to create a new one containing, at this moment, only her. the decision is irrevocable. the objective is partitioning agents into coalitions so as to maximize the resulting social welfare that is the sum of all coalition values. we consider two cases for the value of a coalition: (1) the sum of the weights of its edges, and (2) the sum of the weights of its edges divided by its size.
coalition structures appear in a variety of application in ai, multi-agent systems, networks, as well as in social networks, data analysis, computational biology, game theory, and scheduling. for each of the coalition value functions we consider the bounded and unbounded cases depending on whether or not the size of a coalition can exceed a given value a. furthermore, we consider the case of a limited number of coalitions and various weight functions for the edges, i.e., unrestricted, positive and constant weights. we show tight or nearly tight bounds for the competitive ratio in each case."
2021,learning optimal decision sets and lists with sat,https://www.jair.org/index.php/jair/article/view/12719,"decision sets and decision lists are two of the most easily explainable machine learning models. given the renewed emphasis on explainable machine learning decisions, both of these machine learning models are becoming increasingly attractive, as they combine small size and clear explainability. in this paper, we define size as the total number of literals in the sat encoding of these rule-based models as opposed to earlier work that concentrates on the number of rules. in this paper, we develop approaches to computing minimum-size ""perfect"" decision sets and decision lists, which are perfectly accurate on the training data, and minimal in size, making use of modern sat solving technology. we also provide a new method for determining optimal sparse alternatives, which trade off size and accuracy. the experiments in this paper demonstrate that the optimal decision sets computed by the sat-based approach are comparable with the best heuristic methods, but much more succinct, and thus, more explainable. we contrast the size and test accuracy of optimal decisions lists versus optimal decision sets, as well as other state-of-the-art methods for determining optimal decision lists. finally, we examine the size of average explanations generated by decision sets and decision lists."
2021,a word selection method for producing interpretable distributional semantic word vectors,https://www.jair.org/index.php/jair/article/view/13353,"distributional semantic models represent the meaning of words as vectors. we introduce a selection method to learn a vector space that each of its dimensions is a natural word. the selection method starts from the most frequent words and selects a subset, which has the best performance. the method produces a vector space that each of its dimensions is a word. this is the main advantage of the method compared to fusion methods such as nmf, and neural embedding models. we apply the method to the ukwac corpus and train a vector space of n=1500 basis words. we report tests results on word similarity tasks for men, rg-65, simlex-999, and wordsim353 gold datasets. also, results show that reducing the number of basis vectors from 5000 to 1500 reduces accuracy by about 1.5-2%. so, we achieve good interpretability without a large penalty. interpretability evaluation results indicate that the word vectors obtained by the proposed method using n=1500 are more interpretable than word embedding models, and the baseline method. we report the top 15 words of 1500 selected basis words in this paper."
2021,quantum mathematics in artificial intelligence,https://www.jair.org/index.php/jair/article/view/12702,"in the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. at the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news.
the mathematical techniques underlying both these areas have more in common than is sometimes realized. vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks.
this paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (ai), particularly in automated reasoning and natural language processing (nlp). techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition.
some of these approaches can potentially be implemented on quantum hardware. many of the practical steps in this implementation are in early stages, and some are already realized. explaining some of the common mathematical tools can help researchers in both ai and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way."
2021,the rediscovery hypothesis: language models need to meet linguistics,https://www.jair.org/index.php/jair/article/view/12788,"there is an ongoing debate in the nlp community whether modern language models contain linguistic knowledge, recovered through so-called probes. in this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the rediscovery hypothesis.
in the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. this result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. this framework also provides a metric to measure the impact of linguistic information on the word prediction task. we reinforce our analytical results with various experiments, both on synthetic and on real nlp tasks in english."
2021,learning from disagreement: a survey,https://www.jair.org/index.php/jair/article/view/12752,"many tasks in natural language processing (nlp) and computer vision (cv) offer evidence that humans disagree, from objective tasks such as part-of-speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises. while most learning in artificial intelligence (ai) still relies on the assumption that a single (gold) interpretation exists for each item, a growing body of research aims to develop learning methods that do not rely on this assumption. in this survey, we review the evidence for disagreements on nlp and cv tasks, focusing on tasks for which substantial datasets containing this information have been created. we discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement. we systematically compare these different approaches by training them with each of the available datasets, considering several ways to evaluate the resulting models. finally, we discuss the results in depth, focusing on four key research questions, and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions. our results suggest, first of all, that even if we abandon the assumption of a gold standard, it is still essential to reach a consensus on how to evaluate models. this is because the relative performance of the various training methods is critically affected by the chosen form of evaluation. secondly, we observed a strong dataset effect. with substantial datasets, providing many judgments by high-quality coders for each item, training directly with soft labels achieved better results than training from aggregated or even gold labels. this result holds for both hard and soft evaluation. but when the above conditions do not hold, leveraging both gold and soft labels generally achieved the best results in the hard evaluation. all datasets and models employed in this paper are freely available as supplementary materials."
2021,constraint-based diversification of jop gadgets,https://www.jair.org/index.php/jair/article/view/12848,"modern software deployment process produces software that is uniform, and hence vulnerable to large-scale code-reuse attacks, such as jump-oriented programming (jop) attacks. compiler-based diversification improves the resilience and security of software systems by automatically generating different assembly code versions of a given program. existing techniques are efficient but do not have a precise control over the quality, such as the code size or speed, of the generated code variants.
this paper introduces diversity by construction (divcon), a constraint-based compiler approach to software diversification. unlike previous approaches, divcon allows users to control and adjust the conflicting goals of diversity and code quality. a key enabler is the use of large neighborhood search (lns) to generate highly diverse assembly code efficiently. for larger problems, we propose a combination of lns with a structural decomposition of the problem. to further improve the diversification efficiency of divcon against jop attacks, we propose an application-specific distance measure tailored to the characteristics of jop attacks.
we evaluate divcon with 20 functions from a popular benchmark suite for embedded systems. these experiments show that divcon's combination of lns and our application-specific distance measure generates binary programs that are highly resilient against jop attacks (they share between 0.15% to 8% of jop gadgets) with an optimality gap of 10%. our results confirm that there is a trade-off between the quality of each assembly code version and the diversity of the entire pool of versions. in particular, the experiments show that divcon is able to generate binary programs that share a very small number of gadgets, while delivering near-optimal code.
for constraint programming researchers and practitioners, this paper demonstrates that lns is a valuable technique for finding diverse solutions. for security researchers and software engineers, divcon extends the scope of compiler-based diversification to performance-critical and resource-constrained applications."
2021,on the cluster admission problem for cloud computing,https://www.jair.org/index.php/jair/article/view/12346,"cloud computing providers face the problem of matching heterogeneous customer workloads to resources that will serve them. this is particularly challenging if customers, who are already running a job on a cluster, scale their resource usage up and down over time. the provider therefore has to continuously decide whether she can add additional workloads to a given cluster or if doing so would impact existing workloads' ability to scale. currently, this is often done using simple threshold policies to reserve large parts of each cluster, which leads to low efficiency (i.e., low average utilization of the cluster). we propose more sophisticated policies for controlling admission to a cluster and demonstrate that they significantly increase cluster utilization. we first introduce the cluster admission problem and formalize it as a constrained partially observable markov decision process (pomdp). as it is infeasible to solve the pomdp optimally, we then systematically design admission policies that estimate moments of each workload's distribution of future resource usage. via extensive simulations grounded in a trace from microsoft azure, we show that our admission policies lead to a substantial improvement over the simple threshold policy. we then show that substantial further gains are possible if high-quality information is available about arriving workloads. based on this, we propose an information elicitation approach to incentivize users to provide this information and simulate its effects."
2021,"game plan: what ai can do for football, and what football can do for ai",https://www.jair.org/index.php/jair/article/view/12505,"the rapid progress in artificial intelligence (ai) and machine learning has opened unprecedented analytics possibilities in various team and individual sports, including baseball, basketball, and tennis. more recently, ai techniques have been applied to football, due to a huge increase in data collection by professional teams, increased computational power, and advances in machine learning, with the goal of better addressing new scientific challenges involved in the analysis of both individual players' and coordinated teams' behaviors. the research challenges associated with predictive and prescriptive football analytics require new developments and progress at the intersection of statistical learning, game theory, and computer vision. in this paper, we provide an overarching perspective highlighting how the combination of these fields, in particular, forms a unique microcosm for ai research, while offering mutual benefits for professional teams, spectators, and broadcasters in the years to come. we illustrate that this duality makes football analytics a game changer of tremendous value, in terms of not only changing the game of football itself, but also in terms of what this domain can mean for the field of ai. we review the state-of-the-art and exemplify the types of analysis enabled by combining the aforementioned fields, including illustrative examples of counterfactual analysis using predictive models, and the combination of game-theoretic analysis of penalty kicks with statistical learning of player attributes. we conclude by highlighting envisioned downstream impacts, including possibilities for extensions to other sports (real and virtual)."
2021,efficient local search based on dynamic connectivity maintenance for minimum connected dominating set,https://www.jair.org/index.php/jair/article/view/12618,"the minimum connected dominating set (mcds) problem is an important extension of the minimum dominating set problem, with wide applications, especially in wireless networks. most previous works focused on solving mcds problem in graphs with relatively small size, mainly due to the complexity of maintaining connectivity. this paper explores techniques for solving mcds problem in massive real-world graphs with wide practical importance. firstly, we propose a local greedy construction method with reasoning rule called 1hopreason. secondly and most importantly, a hybrid dynamic connectivity maintenance method (hdc+) is designed to switch alternately between a novel fast connectivity maintenance method based on spanning tree and its previous counterpart. thirdly, we adopt a two-level vertex selection heuristic with a newly proposed scoring function called chronosafety to make the algorithm more considerate when selecting vertices. we design a new local search algorithm called fastcds based on the three ideas. experiments show that fastcds significantly outperforms five state-of-the-art mcds algorithms on both massive graphs and classic benchmarks."
2021,learning over no-preferred and preferred sequence of items for robust recommendation,https://www.jair.org/index.php/jair/article/view/12562,"in this paper, we propose a theoretically supported sequential strategy for training a large-scale recommender system (rs) over implicit feedback, mainly in the form of clicks. the proposed approach consists in minimizing pairwise ranking loss over blocks of consecutive items constituted by a sequence of non-clicked items followed by a clicked one for each user. we present two variants of this strategy where model parameters are updated using either the momentum method or a gradient-based approach. to prevent updating the parameters for an abnormally high number of clicks over some targeted items (mainly due to bots), we introduce an upper and a lower threshold on the number of updates for each user. these thresholds are estimated over the distribution of the number of blocks in the training set. they affect the decision of rs by shifting the distribution of items that are shown to the users. furthermore, we provide a convergence analysis of both algorithms and demonstrate their practical efficiency over six large-scale collections with respect to various ranking measures and computational time."
2021,welfare guarantees in schelling segregation,https://www.jair.org/index.php/jair/article/view/12771,"schelling's model is an influential model that reveals how individual perceptions and incentives can lead to residential segregation. inspired by a recent stream of work, we study welfare guarantees and complexity in this model with respect to several welfare measures. first, we show that while maximizing the social welfare is np-hard, computing an assignment of agents to the nodes of any topology graph with approximately half of the maximum welfare can be done in polynomial time. we then consider pareto optimality, introduce two new optimality notions based on it, and establish mostly tight bounds on the worst-case welfare loss for assignments satisfying these notions as well as the complexity of computing such assignments. in addition, we show that for tree topologies, it is possible to decide whether there exists an assignment that gives every agent a positive utility in polynomial time; moreover, when every node in the topology has degree at least 2, such an assignment always exists and can be found efficiently."
2021,viewpoint: ai as author - bridging the gap between machine learning and literary theory,https://www.jair.org/index.php/jair/article/view/12593,"anticipating the rise in artificial intelligence's ability to produce original works of literature, this study suggests that literariness, or that which constitutes a text as literary, is understudied in relation to text generation. from a computational perspective, literature is particularly challenging because it typically employs figurative and ambiguous language. literary expertise would be beneficial to understanding how meaning and emotion are conveyed in this art form but is often overlooked. we propose placing experts from two dissimilar disciplines - machine learning and literary studies - in conversation to improve the quality of ai writing. concentrating on evaluation as a vital stage in the text generation process, the study demonstrates that benefit could be derived from literary theoretical perspectives. this knowledge would improve algorithm design and enable a deeper understanding of how ai learns and generates.
this article appears in the special track on ai and society."
2021,"measuring the occupational impact of ai: tasks, cognitive abilities and ai benchmarks",https://www.jair.org/index.php/jair/article/view/12647,"in this paper we develop a framework for analysing the impact of artificial intelligence (ai) on occupations. this framework maps 59 generic tasks from worker surveys and an occupational database to 14 cognitive abilities (that we extract from the cognitive science literature) and these to a comprehensive list of 328 ai benchmarks used to evaluate research intensity across a broad range of different ai areas. the use of cognitive abilities as an intermediate layer, instead of mapping work tasks to ai benchmarks directly, allows for an identification of potential ai exposure for tasks for which ai applications have not been explicitly created. an application of our framework to occupational databases gives insights into the abilities through which ai is most likely to affect jobs and allows for a ranking of occupations with respect to ai exposure. moreover, we show that some jobs that were not known to be affected by previous waves of automation may now be subject to higher ai exposure. finally, we find that some of the abilities where ai research is currently very intense are linked to tasks with comparatively limited labour input in the labour markets of advanced economies (e.g., visual and auditory processing using deep learning, and sensorimotor interaction through (deep) reinforcement learning).
this article appears in the special track on ai and society."
2021,rwne: a scalable random-walk based network embedding framework with personalized higher-order proximity preserved,https://www.jair.org/index.php/jair/article/view/12567,"higher-order proximity preserved network embedding has attracted increasing attention. in particular, due to the superior scalability, random-walk-based network embedding has also been well developed, which could efficiently explore higher-order neighborhoods via multi-hop random walks. however, despite the success of current random-walk-based methods, most of them are usually not expressive enough to preserve the personalized higher-order proximity and lack a straightforward objective to theoretically articulate what and how network proximity is preserved. in this paper, to address the above issues, we present a general scalable random-walk-based network embedding framework, in which random walk is explicitly incorporated into a sound objective designed theoretically to preserve arbitrary higher-order proximity. further, we introduce the random walk with restart process into the framework to naturally and effectively achieve personalized-weighted preservation of proximities of different orders. we conduct extensive experiments on several real-world networks and demonstrate that our proposed method consistently and substantially outperforms the state-of-the-art network embedding methods."
2021,declarative algorithms and complexity results for assumption-based argumentation,https://www.jair.org/index.php/jair/article/view/12479,"the study of computational models for argumentation is a vibrant area of artificial intelligence and, in particular, knowledge representation and reasoning research. arguments most often have an intrinsic structure made explicit through derivations from more basic structures. computational models for structured argumentation enable making the internal structure of arguments explicit. assumption-based argumentation (aba) is a central structured formalism for argumentation in ai. in this article, we make both algorithmic and complexity-theoretic advances in the study of aba. in terms of algorithms, we propose a new approach to reasoning in a commonly studied fragment of aba (namely the logic programming fragment) with and without preferences. while previous approaches to reasoning over aba frameworks apply either specialized algorithms or translate aba reasoning to reasoning over abstract argumentation frameworks, we develop a direct declarative approach to aba reasoning by encoding aba reasoning tasks in answer set programming. we show via an extensive empirical evaluation that our approach significantly improves on the empirical performance of current aba reasoning systems. in terms of computational complexity, while the complexity of reasoning over aba frameworks is well-understood, the complexity of reasoning in the aba+ formalism integrating preferences into aba is currently not fully established. towards bridging this gap, our results suggest that the integration of preferential information into aba via so-called reverse attacks results in increased problem complexity for several central argumentation semantics."
2021,playing codenames with language graphs and word embeddings,https://www.jair.org/index.php/jair/article/view/12665,"although board games and video games have been studied for decades in artificial intelligence research, challenging word games remain relatively unexplored. word games are not as constrained as games like chess or poker. instead, word game strategy is defined by the players' understanding of the way words relate to each other. the word game codenames provides a unique opportunity to investigate common sense understanding of relationships between words, an important open challenge. we propose an algorithm that can generate codenames clues from the language graph babelnet or from any of several embedding methods - word2vec, glove, fasttext or bert. we introduce a new scoring function that measures the quality of clues, and we propose a weighting term called detect that incorporates dictionary-based word representations and document frequency to improve clue selection. we develop babelnet-word selection framework (babelnet-wsf) to improve babelnet clue quality and overcome the computational barriers that previously prevented leveraging language graphs for codenames. extensive experiments with human evaluators demonstrate that our proposed innovations yield state-of-the-art performance, with up to 102.8% improvement in precision@2 in some cases. overall, this work advances the formal study of word games and approaches for common sense language understanding."
2021,a tight bound for stochastic submodular cover,https://www.jair.org/index.php/jair/article/view/12368,"we show that the adaptive greedy algorithm of golovin and krause achieves an approximation bound of (ln(q/e)+1) for stochastic submodular cover: here q is the ""goal value"" and e is the minimum gap between q and any attainable utility value q'<q. although this bound was claimed by golovin and krause in the original version of their paper, the proof was later shown to be incorrect by nan & saligrama. the subsequent corrected proof of golovin and krause gives a quadratic bound of (ln(q/e)+1)2. a bound of 56(ln(q/e)+1) is implied by work of im et al. other bounds for the problem depend on quantities other than q and e. our bound restores the original bound claimed by golovin and krause, generalizing the well-known (ln m + 1) approximation bound on the greedy algorithm for the classical set cover problem, where m is the size of the ground set."
2021,multi-document summarization with determinantal point process attention,https://www.jair.org/index.php/jair/article/view/12522,"the ability to convey relevant and diverse information is critical in multi-document summarization and yet remains elusive for neural seq-to-seq models whose outputs are often redundant and fail to correctly cover important details. in this work, we propose an attention mechanism which encourages greater focus on relevance and diversity. attention weights are computed based on (proportional) probabilities given by determinantal point processes (dpps) defined on the set of content units to be summarized. dpps have been successfully used in extractive summarisation, here we use them to select relevant and diverse content for neural abstractive summarisation. we integrate dpp-based attention with various seq-to-seq architectures ranging from cnns to lstms, and transformers. experimental evaluation shows that our attention mechanism consistently improves summarization and delivers performance comparable with the state-of-the-art on the multinews dataset"
2021,representative committees of peers,https://www.jair.org/index.php/jair/article/view/12521,"a population of voters must elect representatives among themselves to decide on a sequence of possibly unforeseen binary issues. voters care only about the final decision, not the elected representatives. the disutility of a voter is proportional to the fraction of issues, where his preferences disagree with the decision. while an issue-by-issue vote by all voters would maximize social welfare, we are interested in how well the preferences of the population can be approximated by a small committee.
we show that a k-sortition (a random committee of k voters with the majority vote within the committee) leads to an outcome within the factor 1+o(1/ k) of the optimal social cost for any number of voters n, any number of issues m, and any preference profile. for a small number of issues m, the social cost can be made even closer to optimal by delegation procedures that weigh committee members according to their number of followers. however, for large m, we demonstrate that the k-sortition is the worst-case optimal rule within a broad family of committee-based rules that take into account metric information about the preference profile of the whole population."
2021,confronting abusive language online: a survey from the ethical and human rights perspective,https://www.jair.org/index.php/jair/article/view/12590,"the pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. significant effort in natural language processing (nlp) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. we review a large body of nlp research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. in many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. we highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and ai-driven public education applications.evaluation, to application deployment. guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and ai-driven public education applications."
2021,epidemioptim: a toolbox for the optimization of control policies in epidemiological models,https://www.jair.org/index.php/jair/article/view/12588,"modeling the dynamics of epidemics helps to propose control strategies based on pharmaceuticaland non-pharmaceutical interventions (contact limitation, lockdown, vaccination,etc). hand-designing such strategies is not trivial because of the number of possibleinterventions and the difficulty to predict long-term effects. this task can be cast as an optimization problem where state-of-the-art machine learning methods such as deep reinforcement learning might bring significant value. however, the specificity of each domain|epidemic modeling or solving optimization problems|requires strong collaborationsbetween researchers from different fields of expertise. this is why we introduce epidemioptim, a python toolbox that facilitates collaborations between researchers inepidemiology and optimization. epidemioptim turns epidemiological models and cost functions into optimization problems via a standard interface commonly used by optimization practitioners (openai gym). reinforcement learning algorithms based on qlearning with deep neural networks (dqn) and evolutionary algorithms (nsga-ii) are already implemented. we illustrate the use of epidemioptim to find optimal policies fordynamical on-o lockdown control under the optimization of the death toll and economic recess using a susceptible-exposed-infectious-removed (seir) model for covid-19. using epidemioptim and its interactive visualization platform in jupyter notebooks, epidemiologists, optimization practitioners and others (e.g. economists) can easily compare epidemiological models, costs functions and optimization algorithms to address important choicesto be made by health decision-makers. trained models can be explored by experts and non-experts via a web interface.
this article is part of the special track on ai and covid-19."
2021,intelligence in strategic games,https://www.jair.org/index.php/jair/article/view/12883,"if an agent, or a coalition of agents, has a strategy, knows that she has a strategy, and knows what the strategy is, then she has a know-how strategy. several modal logics of coalition power for know-how strategies have been studied before.
the contribution of the article is three-fold. first, it proposes a new class of know-how strategies that depend on the intelligence information about the opponents' actions. second, it shows that the coalition power modality for the proposed new class of strategies cannot be expressed through the standard know-how modality. third, it gives a sound and complete logical system that describes the interplay between the coalition power modality with intelligence and the distributed knowledge modality in games with imperfect information."
2021,conceptual modeling of explainable recommender systems: an ontological formalization to guide their design and development,https://www.jair.org/index.php/jair/article/view/12789,"with the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. this is one of the main goals of recommender systems. however, users' trust may be compromised if they do not understand how or why the recommendation was achieved. here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful.
providing explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation's goal, the user's expectation, the knowledge available, or the presentation method. therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user's needs. although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems."
2021,ethics and governance of artificial intelligence: evidence from a survey of machine learning researchers,https://www.jair.org/index.php/jair/article/view/12895,"machine learning (ml) and artificial intelligence (ai) researchers play an important role in the ethics and governance of ai, including through their work, advocacy, and choice of employment. nevertheless, this influential group's attitudes are not well understood, undermining our ability to discern consensuses or disagreements between ai/ml researchers. to examine these researchers' views, we conducted a survey of those who published in two top ai/ml conferences (n = 524). we compare these results with those from a 2016 survey of ai/ml researchers (grace et al., 2018) and a 2018 survey of the us public (zhang & dafoe, 2020). we find that ai/ml researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of ai in the public interest; moderate trust in most western tech companies; and low trust in national militaries, chinese tech companies, and facebook. while the respondents were overwhelmingly opposed to ai/ml researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of ai, particularly logistics algorithms. a strong majority of respondents think that ai safety research should be prioritized and that ml institutions should conduct pre-publication review to assess potential harms. being closer to the technology itself, ai/ml researchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. the findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for ai.
this article appears in the special track on ai & society."
2021,improving the effectiveness and efficiency of stochastic neighbour embedding with isolation kernel,https://www.jair.org/index.php/jair/article/view/12904,"this paper presents a new insight into improving the performance of stochastic neighbour embedding (t-sne) by using isolation kernel instead of gaussian kernel. isolation kernel outperforms gaussian kernel in two aspects. first, the use of isolation kernel in t-sne overcomes the drawback of misrepresenting some structures in the data, which often occurs when gaussian kernel is applied in t-sne. this is because gaussian kernel determines each local bandwidth based on one local point only, while isolation kernel is derived directly from the data based on space partitioning. second, the use of isolation kernel yields a more efficient similarity computation because data-dependent isolation kernel has only one parameter that needs to be tuned. in contrast, the use of data-independent gaussian kernel increases the computational cost by determining n bandwidths for a dataset of n points. as the root cause of these deficiencies in t-sne is gaussian kernel, we show that simply replacing gaussian kernel with isolation kernel in t-sne significantly improves the quality of the final visualisation output (without creating misrepresented structures) and removes one key obstacle that prevents t-sne from processing large datasets. moreover, isolation kernel enables t-sne to deal with large-scale datasets in less runtime without trading off accuracy, unlike existing methods in speeding up t-sne."
2021,goal recognition for deceptive human agents through planning and gaze,https://www.jair.org/index.php/jair/article/view/12518,"eye gaze has the potential to provide insight into the minds of individuals, and this idea has been used in prior research to improve human goal recognition by combining human's actions and gaze. however, most existing research assumes that people are rational and honest. in adversarial scenarios, people may deliberately alter their actions and gaze, which presents a challenge to goal recognition systems. in this paper, we present new models for goal recognition under deception using a combination of gaze behaviour and observed movements of the agent. these models aim to detect when a person is deceiving by analysing their gaze patterns and use this information to adjust the goal recognition. we evaluated our models in two human-subject studies: (1) using data collected from 30 individuals playing a navigation game inspired by an existing deception study and (2) using data collected from 40 individuals playing a competitive game (ticket to ride). we found that one of our models (modulated deception gaze+ontic) offers promising results compared to the previous state-of-the-art model in both studies. our work complements existing adversarial goal recognition systems by equipping these systems with the ability to tackle ambiguous gaze behaviours."
2021,dimensional inconsistency measures and postulates in spatio-temporal databases,https://www.jair.org/index.php/jair/article/view/12435,"the problem of managing spatio-temporal data arises in many applications, such as location-based services, environmental monitoring, geographic information systems, and many others. often spatio-temporal data arising from such applications turn out to be inconsistent, i.e., representing an impossible situation in the real world. though several inconsistency measures have been proposed to quantify in a principled way inconsistency in propositional knowledge bases, little effort has been done so far on inconsistency measures tailored for the spatio-temporal setting.
in this paper, we define and investigate new measures that are particularly suitable for dealing with inconsistent spatio-temporal information, because they explicitly take into account the spatial and temporal dimensions, as well as the dimension concerning the identifiers of the monitored objects. specifically, we first define natural measures that look at individual dimensions (time, space, and objects), and then propose measures based on the notion of a repair. we then analyze their behavior w.r.t. common postulates defined for classical propositional knowledge bases, and find that the latter are not suitable for spatio-temporal databases, in that the proposed inconsistency measures do not often satisfy them. in light of this, we argue that also postulates should explicitly take into account the spatial, temporal, and object dimensions and thus define ""dimension-aware"" counterparts of common postulates, which are indeed often satisfied by the new inconsistency measures. finally, we study the complexity of the proposed inconsistency measures."
2021,merge-and-shrink: a compositional theory of transformations of factored transition systems,https://www.jair.org/index.php/jair/article/view/12557,"the merge-and-shrink framework has been introduced as a general approach for defining abstractions of large state spaces arising in domain-independent planning and related areas. the distinguishing characteristic of the merge-and-shrink approach is that it operates directly on the factored representation of state spaces, repeatedly modifying this representation through transformations such as shrinking (abstracting a factor of the representation), merging (combining two factors), label reduction (abstracting the way in which different factors interact), and pruning (removing states or transitions of a factor).
we provide a novel view of the merge-and-shrink framework as a ""toolbox"" or ""algebra"" of transformations on factored transition systems, with the construction of abstractions as only one possible application. for each transformation, we study desirable properties such as conservativeness (overapproximating the original transition system), inducedness (absence of spurious states and transitions), and refinability (reconstruction of paths in the original transition system from the transformed one). we provide the first complete characterizations of the conditions under which these desirable properties can be achieved. we also provide the first full formal account of factored mappings, the mechanism used within the merge-and-shrink framework to establish the relationship between states in the original and transformed factored transition system.
unlike earlier attempts to develop a theory for merge-and-shrink, our approach is fully compositional: the properties of a sequence of transformations can be entirely understood by the properties of the individual transformations involved. this aspect is key to the use of merge-and-shrink as a general toolbox for transforming factored transition systems. new transformations can easily be added to our theory, with compositionality taking care of the seamless integration with the existing components. similarly, new properties of transformations can be integrated into the theory by showing their compositionality and studying under which conditions they are satisfied by the building blocks of merge-and-shrink."
2021,set-to-sequence methods in machine learning: a review,https://www.jair.org/index.php/jair/article/view/12839,"machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modelling and meta-learning to multi-agent strategy games and power grid optimization. combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. this paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures."
2021,evaluating strategic structures in multi-agent inverse reinforcement learning,https://www.jair.org/index.php/jair/article/view/12594,"a core question in multi-agent systems is understanding the motivations for an agent's actions based on their behavior. inverse reinforcement learning provides a framework for extracting utility functions from observed agent behavior, casting the problem as finding domain parameters which induce such a behavior from rational decision makers. we show how to efficiently and scalably extend inverse reinforcement learning to multi-agent settings, by reducing the multi-agent problem to n single-agent problems while still satisfying rationality conditions such as strong rationality. however, we observe that rewards learned naively tend to lack insightful structure, which causes them to produce undesirable behavior when optimized in games with different players from those encountered during training. we further investigate conditions under which rewards or utility functions can be precisely identified, on problem domains such as normal-form and markov games, as well as auctions, where we show we can learn reward functions that properly generalize to new settings."
2021,agent-based markov modeling for improved covid-19 mitigation policies,https://www.jair.org/index.php/jair/article/view/12632,"the year 2020 saw the covid-19 virus lead to one of the worst global pandemics in history. as a result, governments around the world have been faced with the challenge of protecting public health while keeping the economy running to the greatest extent possible. epidemiological models provide insight into the spread of these types of diseases and predict the effects of possible intervention policies. however, to date, even the most data-driven intervention policies rely on heuristics. in this paper, we study how reinforcement learning (rl) and bayesian inference can be used to optimize mitigation policies that minimize economic impact without overwhelming hospital capacity. our main contributions are (1) a novel agent-based pandemic simulator which, unlike traditional models, is able to model fine-grained interactions among people at specific locations in a community; (2) an rlbased methodology for optimizing fine-grained mitigation policies within this simulator; and (3) a hidden markov model for predicting infected individuals based on partial observations regarding test results, presence of symptoms, and past physical contacts.
this article is part of the special track on ai and covid-19."
2021,bribery and control in stable marriage,https://www.jair.org/index.php/jair/article/view/12755,"we initiate the study of external manipulations in stable marriage by considering several manipulative actions as well as several manipulation goals. for instance, one goal is to make sure that a given pair of agents is matched in a stable solution, and this may be achieved by the manipulative action of reordering some agents' preference lists. we present a comprehensive study of the computational complexity of all problems arising in this way. we find several polynomial-time solvable cases as well as np-hard ones. for the np-hard cases, focusing on the natural parameter ""budget"" (that is, the number of manipulative actions one is allowed to perform), we also conduct a parameterized complexity analysis and encounter mostly parameterized hardness results."
2021,"election manipulation on social networks: seeding, edge removal, edge addition",https://www.jair.org/index.php/jair/article/view/12826,"we focus on the election manipulation problem through social influence, where a manipulator exploits a social network to make her most preferred candidate win an election. influence is due to information in favor of and/or against one or multiple candidates, sent by seeds and spreading through the network according to the independent cascade model. we provide a comprehensive theoretical study of the election control problem, investigating two forms of manipulations: seeding to buy influencers given a social network and removing or adding edges in the social network given the set of the seeds and the information sent. in particular, we study a wide range of cases distinguishing in the number of candidates or the kind of information spread over the network.
our main result shows that the election manipulation problem is not affordable in the worst-case, even when one accepts to get an approximation of the optimal margin of victory, except for the case of seeding when the number of hard-to-manipulate voters is not too large, and the number of uncertain voters is not too small, where we say that a voter that does not vote for the manipulator's candidate is hard-to-manipulate if there is no way to make her vote for this candidate, and uncertain otherwise.
we also provide some results showing the hardness of the problems in special cases. more precisely, in the case of seeding, we show that the manipulation is hard even if the graph is a line and that a large class of algorithms, including most of the approaches recently adopted for social-influence problems (e.g., greedy, degree centrality, pagerank, voterank), fails to compute a bounded approximation even on elementary networks, such as undirected graphs with every node having a degree at most two or directed trees. in the case of edge removal or addition, our hardness results also apply to election manipulation when the manipulator has an unlimited budget, being allowed to remove or add an arbitrary number of edges, and to the basic case of social influence maximization/minimization in the restricted case of finite budget.
interestingly, our hardness results for seeding and edge removal/addition still hold in a re-optimization variant, where the manipulator already knows an optimal solution to the problem and computes a new solution once a local modification occurs, e.g., the removal/addition of a single edge."
2021,"probabilistic temporal networks with ordinary distributions: theory, robustness and expected utility",https://www.jair.org/index.php/jair/article/view/13019,"most existing works in probabilistic simple temporal networks (pstns) base their frameworks on well-defined, parametric probability distributions. under the operational contexts of both strong and dynamic control, this paper addresses robustness measure of pstns, i.e. the execution success probability, where the probability distributions of the contingent durations are ordinary, not necessarily parametric, nor symmetric (e.g. histograms, pert), as long as these can be discretized. in practice, one would obtain ordinary distributions by considering empirical observations (compiled as histograms), or even hand-drawn by field experts. in this new realm of pstns, we study and formally define concepts such as degree of weak/strong/dynamic controllability, robustness under a predefined dispatching protocol, and introduce the concept of pstn expected execution utility. we also discuss the limitation of existing controllability levels, and propose new levels within dynamic controllability, to better characterize dynamic controllable pstns based on based practical complexity considerations. we propose a novel fixed-parameter pseudo-polynomial time computation method to obtain both the success probability and expected utility measures. we apply our computation method to various pstn datasets, including realistic planetary exploration scenarios in the context of the mars 2020 rover. moreover, we propose additional original applications of the method."
2021,"socially responsible ai algorithms: issues, purposes, and challenges",https://www.jair.org/index.php/jair/article/view/12814,"in the current era, people and society have grown increasingly reliant on artificial intelligence (ai) technologies. ai has the potential to drive us towards a future in which all of humanity flourishes. it also comes with substantial risks for oppression and calamity. discussions about whether we should (re)trust ai have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. technologists and ai researchers have a responsibility to develop trustworthy ai systems. they have responded with great effort to design more responsible ai algorithms. however, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. to build long-lasting trust between ai and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of ai that potentially cause ai's indifferent behavior. in this survey, we provide a systematic framework of socially responsible ai algorithms that aims to examine the subjects of ai indifference and the need for socially responsible ai algorithms, define the objectives, and introduce the means by which we may achieve these objectives. we further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation.
this article appears in the special track on ai & society."
2021,"trends in integration of vision and language research: a survey of tasks, datasets, and methods",https://www.jair.org/index.php/jair/article/view/11688,"interest in artificial intelligence (ai) and its applications has seen unprecedented growth in the last few years. this success can be partly attributed to the advancements made in the sub-fields of ai such as machine learning, computer vision, and natural language processing. much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. this has created significant interest in the integration of vision and language. in this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications."
2021,on the decomposition of abstract dialectical frameworks and the complexity of naive-based semantics,https://www.jair.org/index.php/jair/article/view/11348,"abstract dialectical frameworks (adfs) are a recently introduced powerful generalization of dung's popular abstract argumentation frameworks (afs). inspired by similar work for afs, we introduce a decomposition scheme for adfs, which proceeds along the adf's strongly connected components. we find that, for several semantics, the decomposition-based version coincides with the original semantics, whereas for others, it gives rise to a new semantics. these new semantics allow us to deal with pertinent problems such as odd-length negative cycles in a more general setting, that for instance also encompasses logic programs. we perform an exhaustive analysis of the computational complexity of these new, so-called naive-based semantics. the results are quite interesting, for some of them involve little-known classes of the so-called boolean hierarchy (another hierarchy in between classes of the polynomial hierarchy). furthermore, in credulous and sceptical entailment, the complexity can be different depending on whether we check for truth or falsity of a specific statement."
2021,superintelligence cannot be contained: lessons from computability theory,https://www.jair.org/index.php/jair/article/view/12202,"superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. in light of recent advances in machine intelligence, a number of scientists, philosophers and technologists have revived the discussion about the potentially catastrophic risks entailed by such an entity. in this article, we trace the origins and development of the neo-fear of superintelligence, and some of the major proposals for its containment. we argue that total containment is, in principle, impossible, due to fundamental limits inherent to computing itself. assuming that a superintelligence will contain a program that includes all the programs that can be executed by a universal turing machine on input potentially as complex as the state of the world, strict containment requires simulations of such a program, something theoretically (and practically) impossible.
this article is part of the special track on ai and society."
2021,integrated offline and online decision making under uncertainty,https://www.jair.org/index.php/jair/article/view/12333,"this paper considers multi-stage optimization problems under uncertainty that involve distinct offline and online phases. in particular it addresses the issue of integrating these phases to show how the two are often interrelated in real-world applications. our methods are applicable under two (fairly general) conditions: 1) the uncertainty is exogenous; 2) it is possible to define a greedy heuristic for the online phase that can be modeled as a parametric convex optimization problem. we start with a baseline composed by a two-stage offline approach paired with the online greedy heuristic. we then propose multiple methods to tighten the offline/online integration, leading to significant quality improvements, at the cost of an increased computation effort either in the offline or the online phase. overall, our methods provide multiple options to balance the solution quality/time trade-off, suiting a variety of practical application scenarios. to test our methods, we ground our approaches on two real cases studies with both offline and online decisions: an energy management problem with uncertain renewable generation and demand, and a vehicle routing problem with uncertain travel times. the application domains feature respectively continuous and discrete decisions. an extensive analysis of the experimental results shows that indeed offline/online integration may lead to substantial benefits."
2021,zone path construction (zac) based approaches for effective real-time ridesharing,https://www.jair.org/index.php/jair/article/view/11998,"real-time ridesharing systems such as uberpool, lyft line and grabshare have become hugely popular as they reduce the costs for customers, improve per trip revenue for drivers and reduce traffic on the roads by grouping customers with similar itineraries. the key challenge in these systems is to group the ""right"" requests to travel together in the ""right"" available vehicles in real-time, so that the objective (e.g., requests served, revenue or delay) is optimized. this challenge has been addressed in existing work by: (i) generating as many relevant feasible combinations of requests (with respect to the available delay for customers) as possible in real-time; and then (ii) optimizing assignment of the feasible request combinations to vehicles. since the number of request combinations increases exponentially with the increase in vehicle capacity and number of requests, unfortunately, such approaches have to employ ad hoc heuristics to identify a subset of request combinations for assignment.
our key contribution is in developing approaches that employ zone (abstraction of individual locations) paths instead of request combinations. zone paths allow for generation of significantly more ""relevant"" combinations (in comparison to ad hoc heuristics) in real-time than competing approaches due to two reasons: (i) each zone path can typically represent multiple request combinations; (ii) zone paths are generated using a combination of offline and online methods. specifically, we contribute both myopic (ridesharing assignment focussed on current requests only) and non-myopic (ridesharing assignment considers impact on expected future requests) approaches that employ zone paths. in our experimental results, we demonstrate that our myopic approach outperforms the current best myopic approach for ridesharing on both real-world and synthetic datasets (with respect to both objective and runtime). we also show that our non-myopic approach obtains 14.7% improvement over existing myopic approach. our non-myopic approach gets improvements of up to 12.48% over a recent non-myopic approach, neuradp. even when neuradp is allowed to optimize learning over test settings, results largely remain comparable except in a couple of cases, where neuradp performs better."
2021,"cost-optimal planning, delete relaxation, approximability, and heuristics",https://www.jair.org/index.php/jair/article/view/12278,"cost-optimal planning is a very well-studied topic within planning, and it has proven to be computationally hard both in theory and in practice. since cost-optimal planning is an optimisation problem, it is natural to analyse it through the lens of approximation. an important reason for studying cost-optimal planning is heuristic search; heuristic functions that guide the search in planning can often be viewed as algorithms solving or approximating certain optimisation problems. many heuristic functions (such as the ubiquitious h+ heuristic) are based on delete relaxation, which ignores negative effects of actions. planning for instances where the actions have no negative effects is often referred to as monotone planning. the aim of this article is to analyse the approximability of cost-optimal monotone planning, and thus the performance of relevant heuristic functions. our findings imply that it may be beneficial to study these kind of problems within the framework of parameterised complexity and we initiate work in this direction."
2021,learning temporal causal sequence relationships from real-time time-series,https://www.jair.org/index.php/jair/article/view/12395,"we aim to mine temporal causal sequences that explain observed events (consequents) in time-series traces. causal explanations of key events in a time-series have applications in design debugging, anomaly detection, planning, root-cause analysis and many more. we make use of decision trees and interval arithmetic to mine sequences that explain defining events in the time-series. we propose modified decision tree construction metrics to handle the non-determinism introduced by the temporal dimension. the mined sequences are expressed in a readable temporal logic language that is easy to interpret. the application of the proposed methodology is illustrated through various examples."
2021,a survey on the explainability of supervised machine learning,https://www.jair.org/index.php/jair/article/view/12228,"predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. insights about the decision making are mostly opaque for humans. particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. the decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. this survey paper provides essential definitions, an overview of the different principles and methodologies of explainable supervised machine learning (sml). we conduct a state-of-the-art survey that reviews past and recent explainable sml approaches and classifies them according to the introduced definitions. finally, we illustrate principles by means of an explanatory case study and discuss important future directions."
2021,efficient multi-objective reinforcement learning via multiple-gradient descent with iteratively discovered weight-vector sets,https://www.jair.org/index.php/jair/article/view/12270,"solving multi-objective optimization problems is important in various applications where users are interested in obtaining optimal policies subject to multiple (yet often conflicting) objectives. a typical approach to obtain the optimal policies is to first construct a loss function based on the scalarization of individual objectives and then derive optimal policies that minimize the scalarized loss function. albeit simple and efficient, the typical approach provides no insights/mechanisms on the optimization of multiple objectives due to the lack of ability to quantify the inter-objective relationship. to address the issue, we propose to develop a new efficient gradient-based multi-objective reinforcement learning approach that seeks to iteratively uncover the quantitative inter-objective relationship via finding a minimum-norm point in the convex hull of the set of multiple policy gradients when the impact of one objective on others is unknown a priori. in particular, we first propose a new paols algorithm that integrates pruning and approximate optimistic linear support algorithm to efficiently discover the weight-vector sets of multiple gradients that quantify the inter-objective relationship. then we construct an actor and a multi-objective critic that can co-learn the policy and the multi-objective vector value function. finally, the weight discovery process and the policy and vector value function learning process can be iteratively executed to yield stable weight-vector sets and policies. to validate the effectiveness of the proposed approach, we present a quantitative evaluation of the approach based on three case studies."
2021,the computational complexity of understanding binary classifier decisions,https://www.jair.org/index.php/jair/article/view/12359,"for a d-ary boolean function ph: {0, 1}d - {0, 1} and an assignment to its variables x = (x1, x2, . . . , xd) we consider the problem of finding those subsets of the variables that are sufficient to determine the function value with a given probability d. this is motivated by the task of interpreting predictions of binary classifiers described as boolean circuits, which can be seen as special cases of neural networks. we show that the problem of deciding whether such subsets of relevant variables of limited size k <= d exist is complete for the complexity class nppp and thus, generally, unfeasible to solve. we then introduce a variant, in which it suffices to check whether a subset determines the function value with probability at least d or at most d - g for 0 < g < d. this promise of a probability gap reduces the complexity to the class npbpp. finally, we show that finding the minimal set of relevant variables cannot be reasonably approximated, i.e. with an approximation factor d1-a for a > 0, by a polynomial time algorithm unless p = np. this holds even with the promise of a probability gap."
2021,hybrid-order network consensus for distributed multi-agent systems,https://www.jair.org/index.php/jair/article/view/12061,"as an important field of distributed artificial intelligence (dai), multi-agent systems (mass) have attracted the attention of extensive research scholars. consensus as the most important issue in mas, much progress has been made in studying the consensus control of mas, but there are some problems remained largely unaddressed which cause the mas to lose some useful network structure information. first, multi-agent consensus protocol usually proceeds over the low-order structure by only considering the direct edges between agents, but ignores the higher-order structure of the whole topology network. second, the existing work assumes all the edges in a topology network have the same weight without exploring the potential diversity of the connections. in this way, multi-agent systems fail to enforce consensus, resulting in fragmentation into multiple clusters. to address the above issues, this paper proposes a motif-aware weighted multi-agent system (mwms) method for consensus control. we focus more on triangle motif in the network, but it can be extended to other kinds of motifs as well. first, a novel weighted network is used which is the combination of the edge-based lower-order structure and the motif-based higher-order structure, i.e., hybrid-order structure. subsequently, by simultaneously considering the quantity and the quality of the connections in the network, a novel consensus framework for mas is designed to update agents. then, two baseline consensus algorithms are used in mwms. in our experiments, we use ten topologies of different shapes, densities and ranges to comprehensively analyze the performance of our proposed algorithms. the simulation results show that the hybrid higher-order network can effectively enhance the consensus of the multi-agent system in different network topologies."
2021,benchmark and survey of automated machine learning frameworks,https://www.jair.org/index.php/jair/article/view/11854,"machine learning (ml) has become a vital part in many aspects of our daily life. however, building well performing machine learning applications requires highly specialized data scientists and domain experts. automated machine learning (automl) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. this paper is a combination of a survey on current automl methods and a benchmark of popular automl frameworks on real data sets. driven by the selected frameworks for evaluation, we summarize and review important automl techniques and methods concerning every step in building an ml pipeline. the selected automl frameworks are evaluated on 137 data sets from established automl benchmark suites."
2021,on super strong eth,https://www.jair.org/index.php/jair/article/view/11859,"multiple known algorithmic paradigms (backtracking, local search and the polynomial method) only yield a 2n(1-1/o(k)) time algorithm for k-sat in the worst case. for this reason, it has been hypothesized that the worst-case k-sat problem cannot be solved in 2n(1-f(k)/k) time for any unbounded function f. this hypothesis has been called the ""super-strong eth"", modelled after the eth and the strong eth.
it has also been hypothesized that k-sat is hard to solve for randomly chosen instances near the ""critical threshold"", where the clause-to-variable ratio is such that randomly chosen instances are satisfiable with probability 1/2. we give a randomized algorithm which refutes the super-strong eth for the case of random k-sat and planted k-sat for any clause-to-variable ratio. for example, given any random k-sat instance f with n variables and m clauses, our algorithm decides satisfiability for f in 2n(1-c*log(k)/k) time with high probability (over the choice of the formula and the randomness of the algorithm). it turns out that a well-known algorithm from the literature on sat algorithms does the job: the ppz algorithm of paturi, pudlak, and zane (1999).
the unique k-sat problem is the special case where there is at most one satisfying assignment. improving prior reductions, we show that the super-strong eths for unique k-sat and k-sat are equivalent. more precisely, we show the time complexities of unique k-sat and k-sat are very tightly correlated: if unique k-sat is in 2n(1-f(k)/k) time for an unbounded f, then k-sat is in 2n(1-f(k)/(2k)) time."
2021,general value function networks,https://www.jair.org/index.php/jair/article/view/12105,"state construction is important for learning in partially observable environments. a general purpose strategy for state construction is to learn the state update using a recurrent neural network (rnn), which updates the internal state using the current internal state and the most recent observation. this internal state provides a summary of the observed sequence, to facilitate accurate predictions and decision-making. at the same time, specifying and training rnns is notoriously tricky, particularly as the common strategy to approximate gradients back in time, called truncated back-prop through time (bptt), can be sensitive to the truncation window. further, domain-expertise--which can usually help constrain the function class and so improve trainability--can be difficult to incorporate into complex recurrent units used within rnns. in this work, we explore how to use multi-step predictions to constrain the rnn and incorporate prior knowledge. in particular, we revisit the idea of using predictions to construct state and ask: does constraining (parts of) the state to consist of predictions about the future improve rnn trainability? we formulate a novel rnn architecture, called a general value function network (gvfn), where each internal state component corresponds to a prediction about the future represented as a value function. we first provide an objective for optimizing gvfns, and derive several algorithms to optimize this objective. we then show that gvfns are more robust to the truncation level, in many cases only requiring one-step gradient updates."
2021,an external knowledge enhanced graph-based neural network for sentence ordering,https://www.jair.org/index.php/jair/article/view/12078,"as an important text coherence modeling task, sentence ordering aims to coherently organize a given set of unordered sentences. to achieve this goal, the most important step is to effectively capture and exploit global dependencies among these sentences. in this paper, we propose a novel and flexible external knowledge enhanced graph-based neural network for sentence ordering. specifically, we first represent the input sentences as a graph, where various kinds of relations (i.e., entity-entity, sentence-sentence and entity-sentence) are exploited to make the graph representation more expressive and less noisy. then, we introduce graph recurrent network to learn semantic representations of the sentences. to demonstrate the effectiveness of our model, we conduct experiments on several benchmark datasets. the experimental results and in-depth analysis show our model significantly outperforms the existing state-of-the-art models."
2021,on the distortion value of elections with abstention,https://www.jair.org/index.php/jair/article/view/12306,"in spatial voting theory, distortion is a measure of how good the winner is. it has been proved that no deterministic voting mechanism can guarantee a distortion better than 3, even for simple metrics such as a line. in this study, we wish to answer the following question: how does the distortion value change if we allow less motivated agents to abstain from the election?
we consider an election with two candidates and suggest an abstention model, which is a general form of the abstention model proposed by kirchgassner. our results characterize the distortion "" value and provide a rather complete picture of the model."
2021,generic constraint-based block modeling using constraint programming,https://www.jair.org/index.php/jair/article/view/12280,"block modeling has been used extensively in many domains including social science, spatial temporal data analysis and even medical imaging. original formulations of the problem modeled it as a mixed integer programming problem, but were not scalable. subsequent work relaxed the discrete optimization requirement, and showed that adding constraints is not straightforward in existing approaches. in this work, we present a new approach based on constraint programming, allowing discrete optimization of block modeling in a manner that is not only scalable, but also allows the easy incorporation of constraints. we introduce a new constraint filtering algorithm that outperforms earlier approaches, in both constrained and unconstrained settings, for an exhaustive search and for a type of local search called large neighborhood search. we show its use in the analysis of real datasets. finally, we show an application of the cp framework for model selection using the minimum description length principle."
2021,regarding goal bounding and jump point search,https://www.jair.org/index.php/jair/article/view/12255,"jump point search (jps) is a well known symmetry-breaking algorithm that can substantially improve performance for grid-based optimal pathfinding. when the input grid is static further speedups can be obtained by combining jps with goal bounding techniques such as geometric containers (instantiated as bounding boxes) and compressed path databases. two such methods, jps+bb and two-oracle path planning (topping), are currently among the fastest known approaches for computing shortest paths on grids. the principal drawback for these algorithms is the overhead costs: each one requires an all-pairs precomputation step, the running time and subsequent storage costs of which can be prohibitive. in this work we consider an alternative approach where we precompute and store goal bounding data only for grid cells which are also jump points. since the number of jump points is usually much smaller than the total number of grid cells, we can save up to orders of magnitude in preprocessing time and space.
considerable precomputation savings do not necessarily mean performance degradation. for a second contribution we show how canonical orderings, partial expansion strategies and enhanced intermediate pruning can be leveraged to improve online query performance despite a reduction in preprocessed data. the combination of faster preprocessing and stronger online reasoning leads to three new and highly performant algorithms: jps+bb+ and two-oracle pathfinding search (tops) based on search, and topping+ based on path extraction. we give a theoretical analysis showing that each method is complete and optimal. we also report convincing gains in a comprehensive empirical evaluation that includes almost all current and cutting-edge algorithms for grid-based pathfinding."
2021,classifier chains: a review and perspectives,https://www.jair.org/index.php/jair/article/view/12376,"the family of methods collectively known as classifier chains has become a popular approach to multi-label learning problems. this approach involves chaining together off-the-shelf binary classifiers in a directed structure, such that individual label predictions become features for other classifiers. such methods have proved flexible and effective and have obtained state-of-the-art empirical performance across many datasets and multi-label evaluation metrics. this performance led to further studies of the underlying mechanism and efficacy, and investigation into how it could be improved. in the recent decade, numerous studies have explored the theoretical underpinnings of classifier chains, and many improvements have been made to the training and inference procedures, such that this method remains among the best options for multi-label learning. given this past and ongoing interest, which covers a broad range of applications and research themes, the goal of this work is to provide a review of classifier chains, a survey of the techniques and extensions provided in the literature, as well as perspectives for this approach in the domain of multi-label classification in the future. we conclude positively, with a number of recommendations for researchers and practitioners, as well as outlining key issues for future research."
2021,two-facility location games with minimum distance requirement,https://www.jair.org/index.php/jair/article/view/12319,"we study the mechanism design problem of a social planner for locating two facilities on a line interval [0, 1], where a set of n strategic agents report their locations and a mechanism determines the locations of the two facilities. we consider the requirement of a minimum distance 0 <= d <= 1 between the two facilities. given the two facilities are heterogeneous, we model the cost/utility of an agent as the sum of his distances to both facilities. in the heterogeneous two-facility location game to minimize the social cost, we show that the optimal solution can be computed in polynomial time and prove that carefully choosing one optimal solution as output is strategyproof. we also design a strategyproof mechanism minimizing the maximum cost. given the two facilities are homogeneous, we model the cost/utility of an agent as his distance to the closer facility. in the homogeneous two-facility location game for minimizing the social cost, we show that any deterministic strategyproof mechanism has unbounded approximation ratio. moreover, in the obnoxious heterogeneous two-facility location game for maximizing the social utility, we propose new deterministic group strategyproof mechanisms with provable approximation ratios and establish a lower bound (7 - d)/6 for any deterministic strategyproof mechanism. we also design a strategyproof mechanism maximizing the minimum utility. in the obnoxious homogeneous two-facility location game for maximizing the social utility, we propose deterministic group strategyproof mechanisms with provable approximation ratios and establish a lower bound 4/3. besides, in the two-facility location game with triple-preference, where each facility may be favorable, obnoxious, indifferent for any agent, we further motivate agents to report both their locations and preferences towards the two facilities truthfully, and design a deterministic group strategyproof mechanism with an approximation ratio 4."
2021,efficient large-scale multi-drone delivery using transit networks,https://www.jair.org/index.php/jair/article/view/12450,"we consider the problem of routing a large fleet of drones to deliver packages simultaneously across broad urban areas. besides flying directly, drones can use public transit vehicles such as buses and trams as temporary modes of transportation to conserve energy. adding this capability to our formulation augments effective drone travel range and the space of possible deliveries but also increases problem input size due to the large transit networks. we present a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery and addresses the multifaceted computational challenges of our problem through a two-layer approach. first, the upper layer assigns drones to package delivery sequences with an approximately optimal polynomial time allocation algorithm. then, the lower layer executes the allocation by periodically routing the fleet over the transit network, using efficient, bounded suboptimal multi-agent pathfinding techniques tailored to our setting. we demonstrate the efficiency of our approach on simulations with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in san francisco and the washington dc metropolitan area. our framework computes solutions for most settings within a few seconds on commodity hardware and enables drones to extend their effective range by a factor of nearly four using transit."
2021,a sufficient statistic for influence in structured multiagent environments,https://www.jair.org/index.php/jair/article/view/12136,"making decisions in complex environments is a key challenge in artificial intelligence (ai). situations involving multiple decision makers are particularly complex, leading to computational intractability of principled solution methods. a body of work in ai has tried to mitigate this problem by trying to distill interaction to its essence: how does the policy of one agent influence another agent? if we can find more compact representations of such influence, this can help us deal with the complexity, for instance by searching the space of influences rather than the space of policies. however, so far these notions of influence have been restricted in their applicability to special cases of interaction. in this paper we formalize influence-based abstraction (iba), which facilitates the elimination of latent state factors without any loss in value, for a very general class of problems described as factored partially observable stochastic games (fposgs). on the one hand, this generalizes existing descriptions of influence, and thus can serve as the foundation for improvements in scalability and other insights in decision making in complex multiagent settings. on the other hand, since the presence of other agents can be seen as a generalization of single agent settings, our formulation of iba also provides a sufficient statistic for decision making under abstraction for a single agent. we also give a detailed discussion of the relations to such previous works, identifying new insights and interpretations of these approaches. in these ways, this paper deepens our understanding of abstraction in a wide range of sequential decision making settings, providing the basis for new approaches and algorithms for a large class of problems."
2021,taking principles seriously: a hybrid approach to value alignment in artificial intelligence,https://www.jair.org/index.php/jair/article/view/12481,"an important step in the development of value alignment (va) systems in artificial intelligence (ai) is understanding how va can reflect valid ethical principles. we propose that designers of va systems incorporate ethics by utilizing a hybrid approach in which both ethical reasoning and empirical observation play a role. this, we argue, avoids committing ""naturalistic fallacy,"" which is an attempt to derive ""ought"" from ""is,"" and it provides a more adequate form of ethical reasoning when the fallacy is not committed. using quantified modal logic, we precisely formulate principles derived from deontological ethics and show how they imply particular ""test propositions"" for any given action plan in an ai rule base. the action plan is ethical only if the test proposition is empirically true, a judgment that is made on the basis of empirical va. this permits empirical va to integrate seamlessly with independently justified ethical principles.
this article is part of the special track on ai and society."
2021,on the evolvability of monotone conjunctions with an evolutionary mutation mechanism,https://www.jair.org/index.php/jair/article/view/12050,"a bernoulli(p)n distribution bn,p over {0, 1}n is a product distribution where each variable is satisfied with the same constant probability p. diochnos (2016) showed that valiant's swapping algorithm for monotone conjunctions converges efficiently under bn,p distributions over {0, 1}n for any 0 < p < 1. we continue the study of monotone conjunctions in valiant's framework of evolvability. in particular, we prove that given a bn,p distribution characterized by some p (0, 1/3] {1/2}, then an evolutionary mechanism that relies on the basic mutation mechanism of a (1+1) evolutionary algorithm converges efficiently, with high probability, to an e-optimal hypothesis. furthermore, for 0 < a <= 3/13, a slight modification of the algorithm, with a uniform setup this time, evolves with high probability an e-optimal hypothesis, for every bn,p distribution such that p [a, 1/3 - 4a/9] {1/3} {1/2}."
2021,safe multi-agent pathfinding with time uncertainty,https://www.jair.org/index.php/jair/article/view/12397,"in many real-world scenarios, the time it takes for a mobile agent, e.g., a robot, to move from one location to another may vary due to exogenous events and be difficult to predict accurately. planning in such scenarios is challenging, especially in the context of multi-agent pathfinding (mapf), where the goal is to find paths to multiple agents and temporal coordination is necessary to avoid collisions. in this work, we consider a mapf problem with this form of time uncertainty, where we are only given upper and lower bounds on the time it takes each agent to move. the objective is to find a safe solution, which is a solution that can be executed by all agents and is guaranteed to avoid collisions. we propose two complete and optimal algorithms for finding safe solutions based on well-known mapf algorithms, namely, a* with operator decomposition (a* + od) and conflict-based search (cbs). experimentally, we observe that on several standard mapf grids the cbs-based algorithm performs better. we also explore the option of online replanning in this context, i.e., modifying the agents' plans during execution, to reduce the overall execution cost. we consider two online settings: (a) when an agent can sense the current time and its current location, and (b) when the agents can also communicate seamlessly during execution. for each setting, we propose a replanning algorithm and analyze its behavior theoretically and empirically. our experimental evaluation confirms that indeed online replanning in both settings can significantly reduce solution cost."
2021,constrained multiagent markov decision processes: a taxonomy of problems and algorithms,https://www.jair.org/index.php/jair/article/view/12233,"in domains such as electric vehicle charging, smart distribution grids and autonomous warehouses, multiple agents share the same resources. when planning the use of these resources, agents need to deal with the uncertainty in these domains. although several models and algorithms for such constrained multiagent planning problems under uncertainty have been proposed in the literature, it remains unclear when which algorithm can be applied. in this survey we conceptualize these domains and establish a generic problem class based on markov decision processes. we identify and compare the conditions under which algorithms from the planning literature for problems in this class can be applied: whether constraints are soft or hard, whether agents are continuously connected, whether the domain is fully observable, whether a constraint is momentarily (instantaneous) or on a budget, and whether the constraint is on a single resource or on multiple. further we discuss the advantages and disadvantages of these algorithms. we conclude by identifying open problems that are directly related to the conceptualized domains, as well as in adjacent research areas."
2021,the societal implications of deep reinforcement learning,https://www.jair.org/index.php/jair/article/view/12360,"deep reinforcement learning (drl) is an avenue of research in artificial intelligence (ai) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. drl is one of the most promising routes towards developing more autonomous ai systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct 'answer'. this could have substantial implications for people's lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. in this paper, we review recent progress in drl, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand drl's societal implications.
this article appears in the special track on ai and society."
2021,induction and exploitation of subgoal automata for reinforcement learning,https://www.jair.org/index.php/jair/article/view/12372,"in this paper we present isa, an approach for learning and exploiting subgoals in episodic reinforcement learning (rl) tasks. isa interleaves reinforcement learning with the induction of a subgoal automaton, an automaton whose edges are labeled by the task's subgoals expressed as propositional logic formulas over a set of high-level events. a subgoal automaton also consists of two special states: a state indicating the successful completion of the task, and a state indicating that the task has finished without succeeding. a state-of-the-art inductive logic programming system is used to learn a subgoal automaton that covers the traces of high-level events observed by the rl agent. when the currently exploited automaton does not correctly recognize a trace, the automaton learner induces a new automaton that covers that trace. the interleaving process guarantees the induction of automata with the minimum number of states, and applies a symmetry breaking mechanism to shrink the search space whilst remaining complete. we evaluate isa in several gridworld and continuous state space problems using different rl algorithms that leverage the automaton structures. we provide an in-depth empirical analysis of the automaton learning performance in terms of the traces, the symmetry breaking and specific restrictions imposed on the final learnable automaton. for each class of rl problem, we show that the learned automata can be successfully exploited to learn policies that reach the goal, achieving an average reward comparable to the case where automata are not learned but handcrafted and given beforehand."
2021,lilotane: a lifted sat-based approach to hierarchical planning,https://www.jair.org/index.php/jair/article/view/12520,"one of the oldest and most popular approaches to automated planning is to encode the problem at hand into a propositional formula and use a satisfiability (sat) solver to find a solution. in all established sat-based approaches for hierarchical task network (htn) planning, grounding the problem is necessary and oftentimes introduces a combinatorial blowup in terms of the number of actions and reductions to encode. our contribution named lilotane (lifted logic for task networks) eliminates this issue for totally ordered htn planning by directly encoding the lifted representation of the problem at hand. we lazily instantiate the problem hierarchy layer by layer and use a novel sat encoding which allows us to defer decisions regarding method arguments to the stage of sat solving. we show the correctness of our encoding and compare it to the best performing prior sat encoding in a worst-case analysis. empirical evaluations confirm that lilotane outperforms established sat-based approaches, often by orders of magnitude, produces much smaller formulae on average, and compares favorably to other state-of-the-art htn planners regarding robustness and plan quality. in the international planning competition (ipc) 2020, a preliminary version of lilotane scored the second place. we expect these considerable improvements to sat-based htn planning to open up new perspectives for sat-based approaches in related problem classes."
2021,computational complexity of computing symmetries in finite-domain planning,https://www.jair.org/index.php/jair/article/view/12283,"symmetry-based pruning is a powerful method for reducing the search effort in finitedomain planning. this method is based on exploiting an automorphism group connected to the ground description of the planning task { these automorphisms are known as structural symmetries. in particular, we are interested in the structsym problem where the generators of this group are to be computed. it has been observed in practice that the structsym problem is surprisingly easy to solve. we explain this phenomenon by showing that structsym is gi-complete, i.e., the graph isomorphism problem is polynomial-time equivalent to it and, consequently, solvable in quasi-polynomial time. this implies that it is solvable substantially faster than most computationally hard problems encountered in ai. we accompany this result by identifying natural restrictions of the planning task and its causal graph that ensure that structsym can be solved in polynomial time. given that the structsym problem is gi-complete and thus solvable quite efficiently, it is interesting to analyse if other symmetries (than those that are encompassed by the structsym problem) can be computed and/or analysed efficiently, too. to this end, we present a highly negative result: checking whether there exists an automorphism of the state transition graph that maps one state s into another state t is a pspace-hard problem and, consequently, at least as hard as the planning problem itself."
2021,liquid democracy: an algorithmic perspective,https://www.jair.org/index.php/jair/article/view/12261,"we study liquid democracy, a collective decision making paradigm that allows voters to transitively delegate their votes, through an algorithmic lens. in our model, there are two alternatives, one correct and one incorrect, and we are interested in the probability that the majority opinion is correct. our main question is whether there exist delegation mechanisms that are guaranteed to outperform direct voting, in the sense of being always at least as likely, and sometimes more likely, to make a correct decision. even though we assume that voters can only delegate their votes to better-informed voters, we show that local delegation mechanisms, which only take the local neighborhood of each voter as input (and, arguably, capture the spirit of liquid democracy), cannot provide the foregoing guarantee. by contrast, we design a non-local delegation mechanism that does provably outperform direct voting under mild assumptions about voters."
2021,strategyproof mechanisms for additively separable and fractional hedonic games,https://www.jair.org/index.php/jair/article/view/12107,"additively separable hedonic games and fractional hedonic games have received considerable attention in the literature. they are coalition formation games among selfish agents based on their mutual preferences. most of the work in the literature characterizes the existence and structure of stable outcomes (i.e., partitions into coalitions) assuming that preferences are given. however, there is little discussion of this assumption. in fact, agents receive different utilities if they belong to different coalitions, and thus it is natural for them to declare their preferences strategically in order to maximize their benefit. in this paper we consider strategyproof mechanisms for additively separable hedonic games and fractional hedonic games, that is, partitioning methods without payments such that utility maximizing agents have no incentive to lie about their true preferences. we focus on social welfare maximization and provide several lower and upper bounds on the performance achievable by strategyproof mechanisms for general and specific additive functions. in most of the cases we provide tight or asymptotically tight results. all our mechanisms are simple and can be run in polynomial time. moreover, all the lower bounds are unconditional, that is, they do not rely on any computational complexity assumptions."
2021,weighted first-order model counting in the two-variable fragment with counting quantifiers,https://www.jair.org/index.php/jair/article/view/12320,"it is known due to the work of van den broeck, meert and darwiche that weighted first-order model counting (wfomc) in the two-variable fragment of first-order logic can be solved in time polynomial in the number of domain elements. in this paper we extend this result to the two-variable fragment with counting quantifiers."
2021,the ai liability puzzle and a fund-based work-around,https://www.jair.org/index.php/jair/article/view/12580,"confidence in the regulatory environment is crucial to enable responsible ai innovation and foster the social acceptance of these powerful new technologies. one notable source of uncertainty is, however, that the existing legal liability system is unable to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of ai and/or the harms they may trigger are not foreseeable in the legal sense. the unpredictability of how courts would handle such cases makes the risks involved in the investment and use of ai difficult to calculate with confidence, creating an environment that is not conducive to innovation and may deprive society of some benefits ai could provide. to tackle this problem, we propose to draw insights from financial regulatory best practices and establish a system of ai guarantee schemes. we envisage the system to form part of the broader market-structuring regulatory frameworks, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. we propose it to be at least partially industry-funded. funding arrangements should depend on whether it would pursue other potential policy goals aimed more broadly at controlling the trajectory of ai innovation to increase economic and social welfare worldwide. because of the global relevance of the issue, rather than focusing on any particular legal system, we trace relevant developments across multiple jurisdictions and engage in a high-level, comparative conceptual debate around the suitability of the foreseeability concept to limit legal liability. the paper also refrains from confronting the intricacies of the case law of specific jurisdictions for now and--recognizing the importance of this task--leaves this to further research in support of the legal system's incremental adaptation to the novel challenges of present and future ai technologies.
this article appears in the special track on ai and society."
2021,instance-level update in dl-lite ontologies through first-order rewriting,https://www.jair.org/index.php/jair/article/view/12414,"in this paper we study instance-level update in dl-litea , a well-known description logic that influenced the owl 2 ql standard. instance-level update regards insertions and deletions in the abox of an ontology. in particular we focus on formula-based approaches to instance-level update. we show that dl-litea , which is well-known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for instance-level update. that is, every update can be reformulated into a set of insertion and deletion instructions computable through a non-recursive datalog program with negation. such a program is readily translatable into a first-order query over the abox considered as a database, and hence into sql. by exploiting this result, we implement an update component for dl-litea-based systems and perform some experiments showing that the approach works in practice."
2021,confident learning: estimating uncertainty in dataset labels,https://www.jair.org/index.php/jair/article/view/12125,"learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. confident learning (cl) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. this results in a generalized cl which is provably consistent and experimentally performant. we present sufficient conditions where cl exactly finds label errors, and show cl performance exceeding seven recent competitive approaches for learning with noisy labels on the cifar dataset. uniquely, the cl framework is not coupled to a specific data modality or model (e.g., we use cl to find several label errors in the presumed error-free mnist dataset and improve sentiment classification on text data in amazon reviews). we also employ cl on imagenet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for resnet) by cleaning data prior to training. these results are replicable using the open-source cleanlab release."
2021,"aggregation over metric spaces: proposing and voting in elections, budgeting, and legislation",https://www.jair.org/index.php/jair/article/view/12388,"we present a unifying framework encompassing a plethora of social choice settings. viewing each social choice setting as voting in a suitable metric space, we offer a general model of social choice over metric spaces, in which--similarly to the spatial model of elections--each voter specifies an ideal element of the metric space. the ideal element acts as a vote, where each voter prefers elements that are closer to her ideal element. but it also acts as a proposal, thus making all participants equal not only as voters but also as proposers. we consider condorcet aggregation and a continuum of solution concepts, ranging from minimizing the sum of distances to minimizing the maximum distance. we study applications of our abstract model to various social choice settings, including single-winner elections, committee elections, participatory budgeting, and participatory legislation. for each setting, we compare each solution concept to known voting rules and study various properties of the resulting voting rules. our framework provides expressive aggregation for a broad range of social choice settings while remaining simple for voters; and may enable a unified and integrated implementation for all these settings, as well as unified extensions such as sybil-resiliency, proxy voting, and deliberative decision making."
2021,efficient retrieval of matrix factorization-based top-k recommendations: a survey of recent approaches,https://www.jair.org/index.php/jair/article/view/12403,"top-k recommendation seeks to deliver a personalized list of k items to each individual user. an established methodology in the literature based on matrix factorization (mf), which usually represents users and items as vectors in low-dimensional space, is an effective approach to recommender systems, thanks to its superior performance in terms of recommendation quality and scalability. a typical matrix factorization recommender system has two main phases: preference elicitation and recommendation retrieval. the former analyzes user-generated data to learn user preferences and item characteristics in the form of latent feature vectors, whereas the latter ranks the candidate items based on the learnt vectors and returns the top-k items from the ranked list. for preference elicitation, there have been numerous works to build accurate mf-based recommendation algorithms that can learn from large datasets. however, for the recommendation retrieval phase, naively scanning a large number of items to identify the few most relevant ones may inhibit truly real-time applications. in this work, we survey recent advances and state-of-the-art approaches in the literature that enable fast and accurate retrieval for mf-based personalized recommendations. also, we include analytical discussions of approaches along different dimensions to provide the readers with a more comprehensive understanding of the surveyed works."
2021,"loss functions, axioms, and peer review",https://www.jair.org/index.php/jair/article/view/12554,"it is common to see a handful of reviewers reject a highly novel paper, because they view, say, extensive experiments as far more important than novelty, whereas the community as a whole would have embraced the paper. more generally, the disparate mapping of criteria scores to final recommendations by different reviewers is a major source of inconsistency in peer review. in this paper we present a framework inspired by empirical risk minimization (erm) for learning the community's aggregate mapping. the key challenge that arises is the specification of a loss function for erm. we consider the class of l(p,q) loss functions, which is a matrix-extension of the standard class of lp losses on vectors; here the choice of the loss function amounts to choosing the hyperparameters p and q. to deal with the absence of ground truth in our problem, we instead draw on computational social choice to identify desirable values of the hyperparameters p and q. specifically, we characterize p=q=1 as the only choice of these hyperparameters that satisfies three natural axiomatic properties. finally, we implement and apply our approach to reviews from ijcai 2017."
2021,madras : multi agent driving simulator,https://www.jair.org/index.php/jair/article/view/12531,"autonomous driving has emerged as one of the most active areas of research as it has the promise of making transportation safer and more efficient than ever before. most real-world autonomous driving pipelines perform perception, motion planning and action in a loop. in this work we present madras, an open-source multi-agent driving simulator for use in the design and evaluation of motion planning algorithms for autonomous driving. given a start and a goal state, the task of motion planning is to solve for a sequence of position, orientation and speed values in order to navigate between the states while adhering to safety constraints. these constraints often involve the behaviors of other agents in the environment. madras provides a platform for constructing a wide variety of highway and track driving scenarios where multiple driving agents can be trained for motion planning tasks using reinforcement learning and other machine learning algorithms. madras is built on torcs, an open-source car-racing simulator. torcs offers a variety of cars with different dynamic properties and driving tracks with different geometries and surface. madras inherits these functionalities from torcs and introduces support for multi-agent training, inter-vehicular communication, noisy observations, stochastic actions, and custom traffic cars whose behaviors can be programmed to simulate challenging traffic conditions encountered in the real world. madras can be used to create driving tasks whose complexities can be tuned along eight axes in well-defined steps. this makes it particularly suited for curriculum and continual learning. madras is lightweight and it provides a convenient openai gym interface for independent control of each car. apart from the primitive steering-acceleration-brake control mode of torcs, madras offers a hierarchical track-position - speed control mode that can potentially be used to achieve better generalization. madras uses a udp based client server model where the simulation engine is the server and each client is a driving agent. madras uses multiprocessing to run each agent as a parallel process for efficiency and integrates well with popular reinforcement learning libraries like rllib. we show experiments on single and multi-agent reinforcement learning with and without curriculum"
2021,labeled bipolar argumentation frameworks,https://www.jair.org/index.php/jair/article/view/12394,"an essential part of argumentation-based reasoning is to identify arguments in favor and against a statement or query, select the acceptable ones, and then determine whether or not the original statement should be accepted. we present here an abstract framework that considers two independent forms of argument interaction--support and conflict--and is able to represent distinctive information associated with these arguments. this information can enable additional actions such as: (i) a more in-depth analysis of the relations between the arguments; (ii) a representation of the user's posture to help in focusing the argumentative process, optimizing the values of attributes associated with certain arguments; and (iii) an enhancement of the semantics taking advantage of the availability of richer information about argument acceptability. thus, the classical semantic definitions are enhanced by analyzing a set of postulates they satisfy. finally, a polynomial-time algorithm to perform the labeling process is introduced, in which the argument interactions are considered."
2020,point at the triple: generation of text summaries from knowledge base triples,https://www.jair.org/index.php/jair/article/view/11694,"we investigate the problem of generating natural language summaries from knowledge base triples. our approach is based on a pointer-generator network, which, in addition to generating regular words from a fixed target vocabulary, is able to verbalise triples in several ways. we undertake an automatic and a human evaluation on single and open-domain summaries generation tasks. both show that our approach significantly outperforms other data-driven baselines."
2020,constraint and satisfiability reasoning for graph coloring,https://www.jair.org/index.php/jair/article/view/11313,"graph coloring is an important problem in combinatorial optimization and a major component of numerous allocation and scheduling problems. in this paper we introduce a hybrid cp/sat approach to graph coloring based on the addition-contraction recurrence of zykov. decisions correspond to either adding an edge between two non-adjacent vertices or contracting these two vertices, hence enforcing inequality or equality, respectively. this scheme yields a symmetry-free tree and makes learnt clauses stronger by not committing to a particular color. we introduce a new lower bound for this problem based on mycielskian graphs; a method to produce a clausal explanation of this bound for use in a cdcl algorithm; a branching heuristic emulating brelaz' heuristic on the zykov tree; and dedicated pruning techniques relying on marginal costs with respect to the bound and on reasoning about transitivity when unit propagating learnt clauses. the combination of these techniques in both a branch-and-bound and in a bottom-up search outperforms other sat-based approaches and dsatur on standard benchmarks both for finding upper bounds and for proving lower bounds."
2020,on sparse discretization for graphical games,https://www.jair.org/index.php/jair/article/view/12391,"graphical games are one of the earliest examples of the impact that the general field of graphical models have had in other areas, and in this particular case, in classical mathematical models in game theory. graphical multi-hypermatrix games, a concept formally introduced in this research note, generalize graphical games while allowing the possibility of further space savings in model representation to that of standard graphical games. the main focus of this research note is discretization schemes for computing approximate nash equilibria, with emphasis on graphical games, but also briefly touching on normal-form and polymatrix games. the main technical contribution is a theorem that establishes sufficient conditions for a discretization of the players' space of mixed strategies to contain an approximate nash equilibrium. the result is actually stronger because every exact nash equilibrium has a nearby approximate nash equilibrium on the grid induced by the discretization. the sufficient conditions are weaker than those of previous results. in particular, a uniform discretization of size linear in the inverse of the approximation error and in the natural game-representation parameters suffices. the theorem holds for a generalization of graphical games, introduced here. the result has already been useful in the design and analysis of tractable algorithms for graphical games with parametric payoff functions and certain game-graph structures. for standard graphical games, under natural conditions, the discretization is logarithmic in the game-representation size, a substantial improvement over the linear dependency previously required. combining the improved discretization result with old results on constraint networks in ai simplifies the derivation and analysis of algorithms for computing approximate nash equilibria in graphical games."
2020,incompatibilities between iterated and relevance-sensitive belief revision,https://www.jair.org/index.php/jair/article/view/11871,"the agm paradigm for belief change, as originally introduced by alchourron, gardenfors and makinson, lacks any guidelines for the process of iterated revision. one of the most influential work addressing this problem is darwiche and pearl's approach (dp approach, for short), which, despite its well-documented shortcomings, remains to this date the most dominant. in this article, we make further observations on the dp approach. in particular, we prove that the dp postulates are, in a strong sense, inconsistent with parikh's relevance-sensitive axiom (p), extending previous initial conflicts. immediate consequences of this result are that an entire class of intuitive revision operators, which includes dalal's operator, violates the dp postulates, as well as that the independence postulate and spohn's conditionalization are inconsistent with axiom (p). the whole study, essentially, indicates that two fundamental aspects of the revision process, namely, iteration and relevance, are in deep conflict, and opens the discussion for a potential reconciliation towards a comprehensive formal framework for knowledge dynamics."
2020,contiguous cake cutting: hardness results and approximation algorithms,https://www.jair.org/index.php/jair/article/view/12222,"we study the fair allocation of a cake, which serves as a metaphor for a divisible resource, under the requirement that each agent should receive a contiguous piece of the cake. while it is known that no finite envy-free algorithm exists in this setting, we exhibit efficient algorithms that produce allocations with low envy among the agents. we then establish np-hardness results for various decision problems on the existence of envy-free allocations, such as when we fix the ordering of the agents or constrain the positions of certain cuts. in addition, we consider a discretized setting where indivisible items lie on a line and show a number of hardness results extending and strengthening those from prior work. finally, we investigate connections between approximate and exact envy-freeness, as well as between continuous and discrete cake cutting."
2020,annotator rationales for labeling tasks in crowdsourcing,https://www.jair.org/index.php/jair/article/view/12012,"when collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. to address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. we evaluate this approach on an information retrieval task in which human judges rate the relevance of web pages for different search topics. cost-benefit analysis over 10,000 judgments collected on amazon's mechanical turk suggests a win-win. firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. consequently, we can realize the above benefits with minimal additional cost."
2020,the parameterized complexity of motion planning for snake-like robots,https://www.jair.org/index.php/jair/article/view/11864,"we study the parameterized complexity of a variant of the classic video game snake that models real-world problems of motion planning. given a snake-like robot with an initial position and a final position in an environment (modeled by a graph), our objective is to determine whether the robot can reach the final position from the initial position without intersecting itself. naturally, this problem models a wide-variety of scenarios, ranging from the transportation of linked wagons towed by a locomotor at an airport or a supermarket to the movement of a group of agents that travel in an ""ant-like"" fashion and the construction of trains in amusement parks. unfortunately, already on grid graphs, this problem is pspace-complete. nevertheless, we prove that even on general graphs, the problem is solvable in fpt time with respect to the size of the snake. in particular, this shows that the problem is fixed-parameter tractable (fpt). towards this, we show how to employ color-coding to sparsify the configuration graph of the problem to reduce its size significantly. we believe that our approach will find other applications in motion planning. additionally, we show that the problem is unlikely to admit a polynomial kernel even on grid graphs, but it admits a treewidth-reduction procedure. to the best of our knowledge, the study of the parameterized complexity of motion planning problems (where the intermediate configurations of the motion are of importance) has so far been largely overlooked. thus, our work is pioneering in this regard."
2020,improved high dimensional discrete bayesian network inference using triplet region construction,https://www.jair.org/index.php/jair/article/view/12198,"performing efficient inference on high dimensional discrete bayesian networks (bns) is challenging. when using exact inference methods the space complexity can grow exponentially with the tree-width, thus making computation intractable. this paper presents a general purpose approximate inference algorithm, based on a new region belief approximation method, called triplet region construction (trc). trc reduces the cluster space complexity for factorized models from worst-case exponential to polynomial by performing graph factorization and producing clusters of limited size. unlike previous generations of region-based algorithms, trc is guaranteed to converge and effectively addresses the region choice problem that bedevils other region-based algorithms used for bn inference. our experiments demonstrate that it also achieves significantly more accurate results than competing algorithms."
2020,"planning high-level paths in hostile, dynamic, and uncertain environments",https://www.jair.org/index.php/jair/article/view/12077,"this paper introduces and studies a graph-based variant of the path planning problem arising in hostile environments. we consider a setting where an agent (e.g. a robot) must reach a given destination while avoiding being intercepted by probabilistic entities which exist in the graph with a given probability and move according to a probabilistic motion pattern known a priori. given a goal vertex and a deadline to reach it, the agent must compute the path to the goal that maximizes its chances of survival. we study the computational complexity of the problem, and present two algorithms for computing high quality solutions in the general case: an exact algorithm based on mixed-integer nonlinear programming, working well in instances of moderate size, and a pseudo-polynomial time heuristic algorithm allowing to solve large scale problems in reasonable time. we also consider the two limit cases where the agent can survive with probability 0 or 1, and provide specialized algorithms to detect these kinds of situations more efficiently."
2020,neural machine translation: a review,https://www.jair.org/index.php/jair/article/view/12007,"the field of machine translation (mt), the automatic translation of written text from one natural language into another, has experienced a major paradigm shift in recent years. statistical mt, which mainly relies on various count-based models and which used to dominate mt research for decades, has largely been superseded by neural machine translation (nmt), which tackles translation with a single neural network. in this work we will trace back the origins of modern nmt architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family. we will conclude with a short survey of more recent trends in the field."
2020,amp chain graphs: minimal separators and structure learning algorithms,https://www.jair.org/index.php/jair/article/view/12101,"this paper deals with chain graphs (cgs) under the andersson-madigan-perlman (amp) interpretation. we address the problem of finding a minimal separator in an amp cg, namely, finding a set z of nodes that separates a given non-adjacent pair of nodes such that no proper subset of z separates that pair. we analyze several versions of this problem and offer polynomial time algorithms for each. these include finding a minimal separator from a restricted set of nodes, finding a minimal separator for two given disjoint sets, and testing whether a given separator is minimal. to address the problem of learning the structure of amp cgs from data, we show that the pc-like algorithm is order dependent, in the sense that the output can depend on the order in which the variables are given. we propose several modifications of the pc-like algorithm that remove part or all of this order-dependence. we also extend the decomposition-based approach for learning bayesian networks (bns) to learn amp cgs, which include bns as a special case, under the faithfulness assumption. we prove the correctness of our extension using the minimal separator results. using standard benchmarks and synthetically generated models and data in our experiments demonstrate the competitive performance of our decomposition-based method, called lcd-amp, in comparison with the (modified versions of) pc-like algorithm. the lcd-amp algorithm usually outperforms the pc-like algorithm, and our modifications of the pc-like algorithm learn structures that are more similar to the underlying ground truth graphs than the original pc-like algorithm, especially in high-dimensional settings. in particular, we empirically show that the results of both algorithms are more accurate and stabler when the sample size is reasonably large and the underlying graph is sparse"
2020,the petlon algorithm to plan efficiently for task-level-optimal navigation,https://www.jair.org/index.php/jair/article/view/12181,"intelligent mobile robots have recently become able to operate autonomously in large-scale indoor environments for extended periods of time. in this process, mobile robots need the capabilities of both task and motion planning. task planning in such environments involves sequencing the robot's high-level goals and subgoals, and typically requires reasoning about the locations of people, rooms, and objects in the environment, and their interactions to achieve a goal. one of the prerequisites for optimal task planning that is often overlooked is having an accurate estimate of the actual distance (or time) a robot needs to navigate from one location to another. state-of-the-art motion planning algorithms, though often computationally complex, are designed exactly for this purpose of finding routes through constrained spaces.
in this article, we focus on integrating task and motion planning (tmp) to achieve task-level-optimal planning for robot navigation while maintaining manageable computational efficiency. to this end, we introduce tmp algorithm petlon (planning efficiently for task-level-optimal navigation), including two configurations with different trade-offs over computational expenses between task and motion planning, for everyday service tasks using a mobile robot. experiments have been conducted both in simulation and on a mobile robot using object delivery tasks in an indoor office environment. the key observation from the results is that petlon is more efficient than a baseline approach that pre-computes motion costs of all possible navigation actions, while still producing plans that are optimal at the task level. we provide results with two different task planning paradigms in the implementation of petlon, and offer tmp practitioners guidelines for the selection of task planners from an engineering perspective."
2020,properties of switch-list representations of boolean functions,https://www.jair.org/index.php/jair/article/view/12199,"in this paper, we focus on a less usual way to represent boolean functions, namely on representations by switch-lists, which are closely related to interval representations. given a truth table representation of a boolean function f the switch-list representation of f is a list of boolean vectors from the truth table which have a different function value than the preceding boolean vector in the truth table. the main aim of this paper is to include this type of representation in the knowledge compilation map by darwiche and marquis and to argue that switch-lists may in certain situations constitute a reasonable choice for a target language in knowledge compilation. first, we compare switch-list representations with a number of standard representations (such as cnf, dnf, and obdd) with respect to their relative succinctness. as a by-product of this analysis, we also give a short proof of a longstanding open question proposed by darwiche and marquis, namely the incomparability of mods (models) and pi (prime implicates) representations. next, using the succinctness result between switch-lists and obdds, we develop a polynomial time compilation algorithm from switch-lists to obdds. finally, we analyze which standard transformations and queries (those considered by darwiche and marquis) can be performed in polynomial time with respect to the size of the input if the input knowledge is represented by a switch-list. we show that this collection is very broad and the combination of polynomial time transformations and queries is quite unique. some of the queries can be answered directly using the switch-list input, others require a compilation of the input to obdd representations which are then used to answer the queries."
2020,computing bayes-nash equilibria in combinatorial auctions with verification,https://www.jair.org/index.php/jair/article/view/11525,"we present a new algorithm for computing pure-strategy e-bayes-nash equilibria (e-bnes) in combinatorial auctions. the main innovation of our algorithm is to separate the algorithm's search phase (for finding the e-bne) from the verification phase (for computing the e). using this approach, we obtain an algorithm that is both very fast and provides theoretical guarantees on the e it finds. our main contribution is a verification method which, surprisingly, allows us to upper bound the e across the whole continuous value space without making assumptions about the mechanism. using our algorithm, we can now compute e-bnes in multi-minded domains that are significantly more complex than what was previously possible to solve. we release our code under an open-source license to enable researchers to perform algorithmic analyses of auctions, to enable bidders to analyze different strategies, and many other applications."
2020,the bottleneck simulator: a model-based deep reinforcement learning approach,https://www.jair.org/index.php/jair/article/view/12463,"deep reinforcement learning has recently shown many impressive successes. however, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. to this end, we propose the bottleneck simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. the learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. we provide a mathematical analysis of the bottleneck simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. finally, we evaluate the bottleneck simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. on both tasks, the bottleneck simulator yields excellent performance beating competing approaches."
2020,maximin share allocations on cycles,https://www.jair.org/index.php/jair/article/view/11702,"the problem of fair division of indivisible goods is a fundamental problem of resource allocation in multi-agent systems, also studied extensively in social choice. recently, the problem was generalized to the case when goods form a graph and the goal is to allocate goods to agents so that each agent's bundle forms a connected subgraph. for the maximin share fairness criterion, researchers proved that if goods form a tree, an allocation offering each agent a bundle of at least her maximin share value always exists. moreover, it can be found in polynomial time. in this paper we consider the problem of maximin share allocations of goods on a cycle. despite the simplicity of the graph, the problem turns out to be significantly harder than its tree version. we present cases when maximin share allocations of goods on cycles exist and provide in this case results on allocations guaranteeing each agent a certain fraction of her maximin share. we also study algorithms for computing maximin share allocations of goods on cycles."
2020,"belief change and 3-valued logics: characterization of 19,683 belief change operators",https://www.jair.org/index.php/jair/article/view/12091,"in this work we introduce a 3-valued logic with modalities, with the aim of having a clear and precise representation of epistemic states, thus the formulas of this logic will be our epistemic states. indeed, these formulas are identified with ranking functions of 3 values, a generalization of total preorders of three levels. in this framework we analyze some types of changes of these epistemic structures and give syntactical characterizations of them in the introduced logic. in particular, we introduce and study carefully a new operator called cautious improvement operator. we also characterize all operators that are definable in this framework."
2020,the complexity landscape of outcome determination in judgment aggregation,https://www.jair.org/index.php/jair/article/view/11970,"we provide a comprehensive analysis of the computational complexity of the outcome determination problem for the most important aggregation rules proposed in the literature on logic-based judgment aggregation. judgment aggregation is a powerful and flexible framework for studying problems of collective decision making that has attracted interest in a range of disciplines, including legal theory, philosophy, economics, political science, and artificial intelligence. the problem of computing the outcome for a given list of individual judgments to be aggregated into a single collective judgment is the most fundamental algorithmic challenge arising in this context. our analysis applies to several different variants of the basic framework of judgment aggregation that have been discussed in the literature, as well as to a new framework that encompasses all existing such frameworks in terms of expressive power and representational succinctness."
2020,structure from randomness in halfspace learning with the zero-one loss,https://www.jair.org/index.php/jair/article/view/11506,"we prove risk bounds for halfspace learning when the data dimensionality is allowed to be larger than the sample size, using a notion of compressibility by random projection. in particular, we give upper bounds for the empirical risk minimizer learned efficiently from randomly projected data, as well as uniform upper bounds in the full high-dimensional space. our main findings are the following: i) in both settings, the obtained bounds are able to discover and take advantage of benign geometric structure, which turns out to depend on the cosine similarities between the classifier and points of the input space, and provide a new interpretation of margin distribution type arguments. ii) furthermore our bounds allow us to draw new connections between several existing successful classification algorithms, and we also demonstrate that our theory is predictive of empirically observed performance in numerical simulations and experiments. iii) taken together, these results suggest that the study of compressive learning can improve our understanding of which benign structural traits - if they are possessed by the data generator - make it easier to learn an effective classifier from a sample."
2020,using machine learning for decreasing state uncertainty in planning,https://www.jair.org/index.php/jair/article/view/11567,"we present a novel approach for decreasing state uncertainty in planning prior to solving the planning problem. this is done by making predictions about the state based on currently known information, using machine learning techniques. for domains where uncertainty is high, we define an active learning process for identifying which information, once sensed, will best improve the accuracy of predictions.
we demonstrate that an agent is able to solve problems with uncertainties in the state with less planning effort compared to standard planning techniques. moreover, agents can solve problems for which they could not find valid plans without using predictions. experimental results also demonstrate that using our active learning process for identifying information to be sensed leads to gathering information that improves the prediction process."
2020,mapping the landscape of artificial intelligence applications against covid-19,https://www.jair.org/index.php/jair/article/view/12162,"covid-19, the disease caused by the sars-cov-2 virus, has been declared a pandemic by the world health organization, which has reported over 18 million confirmed cases as of august 5, 2020. in this review, we present an overview of recent studies using machine learning and, more broadly, artificial intelligence, to tackle many aspects of the covid19 crisis. we have identified applications that address challenges posed by covid-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. we also review datasets, tools, and resources needed to facilitate artificial intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. we highlight the need for international cooperation to maximize the potential of ai in this and future pandemics."
2020,contrasting the spread of misinformation in online social networks,https://www.jair.org/index.php/jair/article/view/11509,"online social networks are nowadays one of the most effective and widespread tools used to share information. in addition to being employed by individuals for communicating with friends and acquaintances, and by brands for marketing and customer service purposes, they constitute a primary source of daily news for a significant number of users. unfortunately, besides legit news, social networks also allow to effectively spread inaccurate or even entirely fabricated ones. also due to sensationalist claims, misinformation can spread from the original sources to a large number of users in a very short time, with negative consequences that, in extreme cases, can even put at risk public safety or health.
in this work we discuss and propose methods to limit the spread of misinformation over online social networks. the issue is split in two separate sub-problems. we first aim to identify the most probable sources of the misinformation among the subset of users that have been reached by it. in the second step, assuming to know the misinformation sources, we want to locate a minimum number of monitors (that is, entities able to identify and block false information) in the network in order to prevent that the misinformation campaign reaches some ""critical"" nodes while maintaining low the number of nodes exposed to the infection.
for each of the two issues, we provide both heuristics and mixed integer programming formulations. to verify the quality and efficiency of our suggested solutions, we conduct experiments on several real-world networks. the results of this extensive experimental phase validate our heuristics as effective tools to contrast the spread of misinformation in online social networks.
regarding the source identification step, our approach showed success rates above 80% in most of the considered settings, and above 60% in almost all of them.
with respect to the second issue, our heuristic proved to be able to obtain solutions that exceeded (in terms of number of required monitors) the ones obtained through our milp-based approach of more than 20% in only few test scenarios. our heuristics for both problems also proved to outperform significantly some previously proposed algorithms."
2020,to regulate or not: a social dynamics analysis of an idealised ai race,https://www.jair.org/index.php/jair/article/view/12225,"rapid technological advancements in artificial intelligence (ai), as well as the growing deployment of intelligent technologies in new application domains, have generated serious anxiety and a fear of missing out among different stake-holders, fostering a racing narrative. whether real or not, the belief in such a race for domain supremacy through ai, can make it real simply from its consequences, as put forward by the thomas theorem. these consequences may be negative, as racing for technological supremacy creates a complex ecology of choices that could push stake-holders to underestimate or even ignore ethical and safety procedures. as a consequence, different actors are urging to consider both the normative and social impact of these technological advancements, contemplating the use of the precautionary principle in ai innovation and research. yet, given the breadth and depth of ai and its advances, it is difficult to assess which technology needs regulation and when. as there is no easy access to data describing this alleged ai race, theoretical models are necessary to understand its potential dynamics, allowing for the identification of when procedures need to be put in place to favour outcomes beneficial for all. we show that, next to the risks of setbacks and being reprimanded for unsafe behaviour, the time-scale in which domain supremacy can be achieved plays a crucial role. when this can be achieved in a short term, those who completely ignore the safety precautions are bound to win the race but at a cost to society, apparently requiring regulatory actions. our analysis reveals that imposing regulations for all risk and timing conditions may not have the anticipated effect as only for specific conditions a dilemma arises between what is individually preferred and globally beneficial. similar observations can be made for the long-term development case. yet different from the short-term situation, conditions can be identified that require the promotion of risk-taking as opposed to compliance with safety regulations in order to improve social welfare. these results remain robust both when two or several actors are involved in the race and when collective rather than individual setbacks are produced by risk-taking behaviour. when defining codes of conduct and regulatory policies for applications of ai, a clear understanding of the time-scale of the race is thus required, as this may induce important non-trivial effects.
this article is part of the special track on ai and society."
2020,qualitative numeric planning: reductions and complexity,https://www.jair.org/index.php/jair/article/view/11865,"qualitative numerical planning is classical planning extended with non-negative real variables that can be increased or decreased ""qualitatively"", i.e., by positive indeterminate amounts. while deterministic planning with numerical variables is undecidable in general, qualitative numerical planning is decidable and provides a convenient abstract model for generalized planning. the solutions to qualitative numerical problems (qnps) were shown to correspond to the strong cyclic solutions of an associated fully observable non-deterministic (fond) problem that terminate. this leads to a generate-and-test algorithm for solving qnps where solutions to a fond problem are generated one by one and tested for termination. the computational shortcomings of this approach for solving qnps, however, are that it is not simple to amend fond planners to generate all solutions, and that the number of solutions to check can be doubly exponential in the number of variables. in this work we address these limitations while providing additional insights on qnps. more precisely, we introduce two polynomial-time reductions, one from qnps to fond problems and the other from fond problems to qnps both of which do not involve termination tests. a result of these reductions is that qnps are shown to have the same expressive power and the same complexity as fond problems."
2020,modular structures and atomic decomposition in ontologies,https://www.jair.org/index.php/jair/article/view/12151,"with the growth of ontologies used in diverse application areas, the need for module extraction and modularisation techniques has risen. the notion of the modular structure of an ontology, which comprises a suitable set of base modules together with their logical dependencies, has the potential to help users and developers in comprehending, sharing, and maintaining an ontology. we have developed a new modular structure, called atomic decomposition (ad), which is based on modules that provide strong logical properties, such as locality-based modules. in this article, we present the theoretical foundations of ad, review its logical and computational properties, discuss its suitability as a modular structure, and report on an experimental evaluation of ad. in addition, we discuss the concept of a modular structure in ontology engineering and provide a survey of existing decomposition approaches."
2020,credibility-limited base revision: new classes and their characterizations,https://www.jair.org/index.php/jair/article/view/12298,"in this paper we study a kind of operator --known as credibility-limited base revisions-- which addresses two of the main issues that have been pointed out to the agm model of belief change. indeed, on the one hand, these operators are defined on belief bases (rather than belief sets) and, on the other hand, they are constructed with the underlying idea that not all new information is accepted. we propose twenty different classes of credibility-limited base revision operators and obtain axiomatic characterizations for each of them. additionally we thoroughly investigate the interrelations (in the sense of inclusion) among all those classes. more precisely, we analyse whether each one of those classes is or is not (strictly) contained in each of the remaining ones."
2020,representing fitness landscapes by valued constraints to understand the complexity of local search,https://www.jair.org/index.php/jair/article/view/12156,"local search is widely used to solve combinatorial optimisation problems and to model biological evolution, but the performance of local search algorithms on different kinds of fitness landscapes is poorly understood. here we consider how fitness landscapes can be represented using valued constraints, and investigate what the structure of such representations reveals about the complexity of local search.
first, we show that for fitness landscapes representable by binary boolean valued constraints there is a minimal necessary constraint graph that can be easily computed. second, we consider landscapes as equivalent if they allow the same (improving) local search moves; we show that a minimal constraint graph still exists, but is np-hard to compute.
we then develop several techniques to bound the length of any sequence of local search moves. we show that such a bound can be obtained from the numerical values of the constraints in the representation, and show how this bound may be tightened by considering equivalent representations. in the binary boolean case, we prove that a degree 2 or treestructured constraint graph gives a quadratic bound on the number of improving moves made by any local search; hence, any landscape that can be represented by such a model will be tractable for any form of local search.
finally, we build two families of examples to show that the conditions in our tractability results are essential. with domain size three, even just a path of binary constraints can model a landscape with an exponentially long sequence of improving moves. with a treewidth-two constraint graph, even with a maximum degree of three, binary boolean constraints can model a landscape with an exponentially long sequence of improving moves."
2020,epistemic argumentation framework: theory and computation,https://www.jair.org/index.php/jair/article/view/12121,"the paper introduces the notion of an epistemic argumentation framework (eaf) as a means to integrate the beliefs of a reasoner with argumentation. intuitively, an eaf encodes the beliefs of an agent who reasons about arguments. formally, an eaf is a pair of an argumentation framework and an epistemic constraint. the semantics of the eaf is defined by the notion of an o-epistemic labelling set, where o is complete, stable, grounded, or preferred, which is a set of o-labellings that collectively satisfies the epistemic constraint of the eaf. the paper shows how eaf can represent different views of reasoners on the same argumentation framework. it also includes representing preferences in eaf and multi-agent argumentation. finally, the paper discusses complexity issues and computation using epistemic logic programming."
2020,a differential privacy mechanism that accounts for network effects for crowdsourcing systems,https://www.jair.org/index.php/jair/article/view/12158,"in crowdsourcing systems, it is important for the crowdsource campaign initiator to incentivize users to share their data to produce results of the desired computational accuracy. this problem becomes especially challenging when users are concerned about the privacy of their data. to overcome this challenge, existing work often aims to provide users with differential privacy guarantees to incentivize privacy-sensitive users to share their data. however, this work neglects the network effect that a user enjoys greater privacy protection when he aligns his participation behaviour with that of other users. to explore this network effect, we formulate the interaction among users regarding their participation decisions as a population game, because a user's welfare from the interaction depends not only on his own participation decision but also the distribution of others' decisions. we show that the nash equilibrium of this game consists of a threshold strategy, where all users whose privacy sensitivity is below a certain threshold will participate and the remaining users will not. we characterize the existence and uniqueness of this equilibrium, which depends on the privacy guarantee, the reward provided by the initiator and the population size. based on this equilibria analysis, we design the pine (privacy incentivization with network effects) mechanism and prove that it maximizes the initiator's payoff while providing participating users with a guaranteed degree of privacy protection. numerical simulations, on both real and synthetic data, show that (i) pine improves the initiator's expected payoff by up to 75%, compared to state of the art mechanisms that do not consider this effect; (ii) the performance gain by exploiting the network effect is particularly good when the majority of users are flexible over their privacy attitudes and when there are a large number of low quality task performers."
2020,adaptive stress testing: finding likely failure events with reinforcement learning,https://www.jair.org/index.php/jair/article/view/12190,"finding the most likely path to a set of failure states is important to the analysis of safety-critical systems that operate over a sequence of time steps, such as aircraft collision avoidance systems and autonomous cars. in many applications such as autonomous driving, failures cannot be completely eliminated due to the complex stochastic environment in which the system operates. as a result, safety validation is not only concerned about whether a failure can occur, but also discovering which failures are most likely to occur. this article presents adaptive stress testing (ast), a framework for finding the most likely path to a failure event in simulation. we consider a general black box setting for partially observable and continuous-valued systems operating in an environment with stochastic disturbances. we formulate the problem as a markov decision process and use reinforcement learning to optimize it. the approach is simulation-based and does not require internal knowledge of the system, making it suitable for black-box testing of large systems. we present different formulations depending on whether the state is fully observable or partially observable. in the latter case, we present a modified monte carlo tree search algorithm that only requires access to the pseudorandom number generator of the simulator to overcome partial observability. we also present an extension of the framework, called differential adaptive stress testing (dast), that can find failures that occur in one system but not in another. this type of differential analysis is useful in applications such as regression testing, where we are concerned with finding areas of relative weakness compared to a baseline. we demonstrate the effectiveness of the approach on an aircraft collision avoidance application, where a prototype aircraft collision avoidance system is stress tested to find the most likely scenarios of near mid-air collision."
2020,lifted bayesian filtering in multiset rewriting systems,https://www.jair.org/index.php/jair/article/view/12066,"we present a model for bayesian filtering (bf) in discrete dynamic systems where multiple entities (inter)-act, i.e. where the system dynamics is naturally described by a multiset rewriting system (mrs). typically, bf in such situations is computationally expensive due to the high number of discrete states that need to be maintained explicitly.
we devise a lifted state representation, based on a suitable decomposition of multiset states, such that some factors of the distribution are exchangeable and thus afford an efficient representation. intuitively, this representation groups together similar entities whose properties follow an exchangeable joint distribution. subsequently, we introduce a bf algorithm that works directly on lifted states, without resorting to the original, much larger ground representation.
this algorithm directly lends itself to approximate versions by limiting the number of explicitly represented lifted states in the posterior. we show empirically that the lifted representation can lead to a factorial reduction in the representational complexity of the distribution, and in the approximate cases can lead to a lower variance of the estimate and a lower estimation error compared to the original, ground representation."
2020,"reviewing autoencoders for missing data imputation: technical trends, applications and outcomes",https://www.jair.org/index.php/jair/article/view/12312,"missing data is a problem often found in real-world datasets and it can degrade the performance of most machine learning models. several deep learning techniques have been used to address this issue, and one of them is the autoencoder and its denoising and variational variants. these models are able to learn a representation of the data with missing values and generate plausible new ones to replace them. this study surveys the use of autoencoders for the imputation of tabular data and considers 26 works published between 2014 and 2020. the analysis is mainly focused on discussing patterns and recommendations for the architecture, hyperparameters and training settings of the network, while providing a detailed discussion of the results obtained by autoencoders when compared to other state-of-the-art methods, and of the data contexts where they have been applied. the conclusions include a set of recommendations for the technical settings of the network, and show that denoising autoencoders outperform their competitors, particularly the often used statistical methods."
2020,adapting behavior via intrinsic reward: a survey and empirical study,https://www.jair.org/index.php/jair/article/view/12087,"learning about many things can provide numerous benefits to a reinforcement learning system. for example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. the question we tackle in this paper is how to sculpt the stream of experience--how to adapt the learning system's behavior--to optimize the learning of a collection of value functions. a simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. it remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. in this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. we discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. we provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective."
2020,on the complexity of learning a class ratio from unlabeled data,https://www.jair.org/index.php/jair/article/view/12013,"in the problem of learning a class ratio from unlabeled data, which we call cr learning, the training data is unlabeled, and only the ratios, or proportions, of examples receiving each label are given. the goal is to learn a hypothesis that predicts the proportions of labels on the distribution underlying the sample. this model of learning is applicable to a wide variety of settings, including predicting the number of votes for candidates in political elections from polls.
in this paper, we formally define this class and resolve foundational questions regarding the computational complexity of cr learning and characterize its relationship to pac learning. among our results, we show, perhaps surprisingly, that for finite vc classes what can be efficiently cr learned is a strict subset of what can be learned efficiently in pac, under standard complexity assumptions. we also show that there exist classes of functions whose cr learnability is independent of zfc, the standard set theoretic axioms. this implies that cr learning cannot be easily characterized (like pac by vc dimension)."
2020,an evaluation of communication protocol languages for engineering multiagent systems,https://www.jair.org/index.php/jair/article/view/12212,"communication protocols are central to engineering decentralized multiagent systems. modern protocol languages are typically formal and address aspects of decentralization, such as asynchrony. however, modern languages differ in important ways in their basic abstractions and operational assumptions. this diversity makes a comparative evaluation of protocol languages a challenging task.
we contribute a rich evaluation of diverse and modern protocol languages. among the selected languages, scribble is based on session types; trace-c and trace-f on trace expressions; hapn on hierarchical state machines, and bspl on information causality. our contribution is four-fold. one, we contribute important criteria for evaluating protocol languages. two, for each criterion, we compare the languages on the basis of whether they are able to specify elementary protocols that go to the heart of the criterion. three, for each language, we map our findings to a canonical architecture style for multiagent systems, highlighting where the languages depart from the architecture. four, we identify design principles for protocol languages as guidance for future research."
2020,bounds on the size of pc and urc formulas,https://www.jair.org/index.php/jair/article/view/12006,"in this paper, we investigate cnf encodings, for which unit propagation is strong enough to derive a contradiction if the encoding is not consistent with a partial assignment of the variables (unit refutation complete or urc encoding) or additionally to derive all implied literals if the encoding is consistent with the partial assignment (propagation complete or pc encoding). we prove an exponential separation between the sizes of pc and urc encodings without auxiliary variables and strengthen the known results on their relationship to the pc and urc encodings that can use auxiliary variables. besides of this, we prove that the sizes of any two irredundant pc formulas representing the same function differ at most by a factor polynomial in the number of the variables and present an example of a function demonstrating that a similar statement is not true for urc formulas. one of the separations above implies that a q-horn formula may require an exponential number of additional clauses to become a urc formula. on the other hand, for every q-horn formula, we present a polynomial size urc encoding of the same function using auxiliary variables. this encoding is not q-horn in general."
2020,deep reinforcement learning: a state-of-the-art walkthrough,https://www.jair.org/index.php/jair/article/view/12412,"deep reinforcement learning is a topic that has gained a lot of attention recently, due to the unprecedented achievements and remarkable performance of such algorithms in various benchmark tests and environmental setups. the power of such methods comes from the combination of an already established and strong field of deep learning, with the unique nature of reinforcement learning methods. it is, however, deemed necessary to provide a compact, accurate and comparable view of these methods and their results for the means of gaining valuable technical and practical insights. in this work we gather the essential methods related to deep reinforcement learning, extracting common property structures for three complementary core categories: a) model-free, b) model-based and c) modular algorithms. for each category, we present, analyze and compare state-of-the-art deep reinforcement learning algorithms that achieve high performance in various environments and tackle challenging problems in complex and demanding tasks. in order to give a compact and practical overview of their differences, we present comprehensive comparison figures and tables, produced by reported performances of the algorithms under two popular simulation platforms: the atari learning environment and the mujoco physics simulation platform. we discuss the key differences of the various kinds of algorithms, indicate their potential and limitations, as well as provide insights to researchers regarding future directions of the field."
2020,diagnosis of deep discrete-event systems,https://www.jair.org/index.php/jair/article/view/12171,"an abduction-based diagnosis technique for a class of discrete-event systems (dess), called deep dess (ddess), is presented. a ddes has a tree structure, where each node is a network of communicating automata, called an active unit (au). the interaction of components within an au gives rise to emergent events. an emergent event occurs when specific components collectively perform a sequence of transitions matching a given regular language. any event emerging in an au triggers the transition of a component in its parent au. we say that the ddes has a deep behavior, in the sense that the behavior of an au is governed not only by the events exchanged by the components within the au but also by the events emerging from child aus. deep behavior characterizes not only living beings, including humans, but also artifacts, such as robots that operate in contexts at varying abstraction levels. surprisingly, experimental results indicate that the hierarchical complexity of the system translates into a decreased computational complexity of the diagnosis task. hence, the diagnosis technique is shown to be (formally) correct as well as (empirically) efficient."
2020,asnets: deep learning for generalised planning,https://www.jair.org/index.php/jair/article/view/11633,"in this paper, we discuss the learning of generalised policies for probabilistic and classical planning problems using action schema networks (asnets). the asnet is a neural network architecture that exploits the relational structure of (p)pddl planning problems to learn a common set of weights that can be applied to any problem in a domain. by mimicking the actions chosen by a traditional, non-learning planner on a handful of small problems in a domain, asnets are able to learn a generalised reactive policy that can quickly solve much larger instances from the domain. this work extends the asnet architecture to make it more expressive, while still remaining invariant to a range of symmetries that exist in ppddl problems. we also present a thorough experimental evaluation of asnets, including a comparison with heuristic search planners on seven probabilistic and deterministic domains, an extended evaluation on over 18,000 blocksworld instances, and an ablation study. finally, we show that sparsity-inducing regularisation can produce asnets that are compact enough for humans to understand, yielding insights into how the structure of asnets allows them to generalise across a domain."
2020,vocabulary alignment in openly specified interactions,https://www.jair.org/index.php/jair/article/view/11497,"the problem of achieving common understanding between agents that use different vocabularies has been mainly addressed by techniques that assume the existence of shared external elements, such as a meta-language or a physical environment. in this article, we consider agents that use different vocabularies and only share knowledge of how to perform a task, given by the specification of an interaction protocol. we present a framework that lets agents learn a vocabulary alignment from the experience of interacting. unlike previous work in this direction, we use open protocols that constrain possible actions instead of defining procedures, making our approach more general. we present two techniques that can be used either to learn an alignment from scratch or to repair an existent one, and we evaluate their performance experimentally."
2020,variational bayes in private settings (vips),https://www.jair.org/index.php/jair/article/view/11763,"many applications of bayesian data analysis involve sensitive information such as personal documents or medical records, motivating methods which ensure that privacy is protected. we introduce a general privacy-preserving framework for variational bayes (vb), a widely used optimization-based bayesian inference method. our framework respects differential privacy, the gold-standard privacy criterion, and encompasses a large class of probabilistic models, called the conjugate exponential (ce) family. we observe that we can straightforwardly privatise vb's approximate posterior distributions for models in the ce family, by perturbing the expected sufficient statistics of the complete-data likelihood. for a broadly-used class of non-ce models, those with binomial likelihoods, we show how to bring such models into the ce family, such that inferences in the modified model resemble the private variational bayes algorithm as closely as possible, using the polya-gamma data augmentation scheme. the iterative nature of variational bayes presents a further challenge since iterations increase the amount of noise needed. we overcome this by combining: (1) an improved composition method for differential privacy, called the moments accountant, which provides a tight bound on the privacy cost of multiple vb iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect of subsampling mini-batches from large-scale data in stochastic learning. we empirically demonstrate the effectiveness of our method in ce and non-ce models including latent dirichlet allocation, bayesian logistic regression, and sigmoid belief networks, evaluated on real-world datasets."
2020,towards knowledgeable supervised lifelong learning systems,https://www.jair.org/index.php/jair/article/view/11432,"learning a sequence of tasks is a long-standing challenge in machine learning. this setting applies to learning systems that observe examples of a range of tasks at different points in time. a learning system should become more knowledgeable as more related tasks are learned. although the problem of learning sequentially was acknowledged for the first time decades ago, the research in this area has been rather limited. research in transfer learning, multitask learning, metalearning and deep learning has studied some challenges of these kinds of systems. recent research in lifelong machine learning and continual learning has revived interest in this problem. we propose proficiente, a full framework for long-term learning systems. proficiente relies on knowledge transferred between hypotheses learned with support vector machines. the first component of the framework is focused on transferring forward selectively from a set of existing hypotheses or functions representing knowledge acquired during previous tasks to a new target task. a second component of proficiente is focused on transferring backward, a novel ability of long-term learning systems that aim to exploit knowledge derived from recent tasks to encourage refinement of existing knowledge. we propose a method that transfers selectively from a task learned recently to existing hypotheses representing previous tasks. the method encourages retention of existing knowledge whilst refining. we analyse the theoretical properties of the proposed framework. proficiente is accompanied by an agnostic metric that can be used to determine if a long-term learning system is becoming more knowledgeable. we evaluate proficiente in both synthetic and real-world datasets, and demonstrate scenarios where knowledgeable supervised learning systems can be achieved by means of transfer."
2020,improving nash social welfare approximations,https://www.jair.org/index.php/jair/article/view/11618,"we consider the problem of fairly allocating a set of indivisible goods among n agents. various fairness notions have been proposed within the rapidly growing field of fair division, but the nash social welfare (nsw) serves as a focal point. in part, this follows from the 'unreasonable' fairness guarantees provided, in the sense that a max nsw allocation meets multiple other fairness metrics simultaneously, all while satisfying a standard economic concept of efficiency, pareto optimality. however, existing approximation algorithms fail to satisfy all of the remarkable fairness guarantees offered by a max nsw allocation, instead targeting only the specific nsw objective. we address this issue by presenting a 2 max nsw, prop-1, 1/(2n) mms, and pareto optimal allocation in strongly polynomial time. our techniques are based on a market interpretation of a fractional max nsw allocation. we present novel definitions of fairness concepts in terms of market prices, and design a new scheme to round a market equilibrium into an integral allocation in a way that provides most of the fairness properties of an integral max nsw allocation."
2020,"bridging the gap between probabilistic model checking and probabilistic planning: survey, compilations, and empirical comparison",https://www.jair.org/index.php/jair/article/view/11595,"markov decision processes are of major interest in the planning community as well as in the model checking community. but in spite of the similarity in the considered formal models, the development of new techniques and methods happened largely independently in both communities. this work is intended as a beginning to unite the two research branches. we consider goal-reachability analysis as a common basis between both communities. the core of this paper is the translation from jani, an overarching input language for quantitative model checkers, into the probabilistic planning domain definition language (ppddl), and vice versa from ppddl into jani. these translations allow the creation of an overarching benchmark collection, including existing case studies from the model checking community, as well as benchmarks from the international probabilistic planning competitions (ippc). we use this benchmark set as a basis for an extensive empirical comparison of various approaches from the model checking community, variants of value iteration, and mdp heuristic search algorithms developed by the ai planning community. on a per benchmark domain basis, techniques from one community can achieve state-ofthe-art performance in benchmarks of the other community. across all benchmark domains of one community, the performance comparison is however in favor of the solvers and algorithms of that particular community. reasons are the design of the benchmarks, as well as tool-related limitations. our translation methods and benchmark collection foster crossfertilization between both communities, pointing out specific opportunities for widening the scope of solvers to different kinds of models, as well as for exchanging and adopting algorithms across communities."
2020,sliding-window thompson sampling for non-stationary settings,https://www.jair.org/index.php/jair/article/view/11407,"multi-armed bandit (mab) techniques have been successfully applied to many classes of sequential decision problems in the past decades. however, non-stationary settings -- very common in real-world applications -- received little attention so far, and theoretical guarantees on the regret are known only for some frequentist algorithms. in this paper, we propose an algorithm, namely sliding-window thompson sampling (sw-ts), for nonstationary stochastic mab settings. our algorithm is based on thompson sampling and exploits a sliding-window approach to tackle, in a unified fashion, two different forms of non-stationarity studied separately so far: abruptly changing and smoothly changing. in the former, the reward distributions are constant during sequences of rounds, and their change may be arbitrary and happen at unknown rounds, while, in the latter, the reward distributions smoothly evolve over rounds according to unknown dynamics. under mild assumptions, we provide regret upper bounds on the dynamic pseudo-regret of sw-ts for the abruptly changing environment, for the smoothly changing one, and for the setting in which both the non-stationarity forms are present. furthermore, we empirically show that sw-ts dramatically outperforms state-of-the-art algorithms even when the forms of non-stationarity are taken separately, as previously studied in the literature."
2020,conservative extensions in horn description logics with inverse roles,https://www.jair.org/index.php/jair/article/view/12182,"we investigate the decidability and computational complexity of conservative extensions and the related notions of inseparability and entailment in horn description logics (dls) with inverse roles. we consider both query conservative extensions, defined by requiring that the answers to all conjunctive queries are left unchanged, and deductive conservative extensions, which require that the entailed concept inclusions, role inclusions, and functionality assertions do not change. upper bounds for query conservative extensions are particularly challenging because characterizations in terms of unbounded homomorphisms between universal models, which are the foundation of the standard approach to establishing decidability, fail in the presence of inverse roles. we resort to a characterization that carefully mixes unbounded and bounded homomorphisms and enables a decision procedure that combines tree automata and a mosaic technique. our main results are that query conservative extensions are 2exptime-complete in all dls between eli and horn-alchif and between horn-alc and horn-alchif, and that deductive conservative extensions are 2exptime-complete in all dls between eli and elhif_bot. the same results hold for inseparability and entailment."
2020,predicting strategic behavior from free text,https://www.jair.org/index.php/jair/article/view/11849,"the connection between messaging and action is fundamental both to web applications, such as web search and sentiment analysis, and to economics. however, while prominent online applications exploit messaging in natural (human) language in order to predict non-strategic action selection, the economics literature focuses on the connection between structured stylized messaging to strategic decisions in games and multi-agent encounters. this paper aims to connect these two strands of research, which we consider highly timely and important due to the vast online textual communication on the web. particularly, we introduce the following question: can free text expressed in natural language serve for the prediction of action selection in an economic context, modeled as a game
in order to initiate the research on this question, we introduce the study of an individual's action prediction in a one-shot game based on free text he/she provides, while being unaware of the game to be played. we approach the problem by attributing commonsensical personality attributes via crowd-sourcing to free texts written by individuals, and employing transductive learning to predict actions taken by these individuals in one-shot games based on these attributes. our approach allows us to train a single classifier that can make predictions with respect to actions taken in multiple games. in experiments with three well-studied games, our algorithm compares favorably with strong alternative approaches. in ablation analysis, we demonstrate the importance of our modeling choices--the representation of the text with the commonsensical personality attributes and our classifier--to the predictive power of our model."
2020,automated conjecturing ii: chomp and reasoned game play,https://www.jair.org/index.php/jair/article/view/12188,"we demonstrate the use of a program that generates conjectures about positions of the combinatorial game chomp--explanations of why certain moves are bad. these could be used in the design of a chomp-playing program that gives reasons for its moves. we prove one of these chomp conjectures--demonstrating that our conjecturing program can produce genuine chomp knowledge.
the conjectures are generated by a general purpose conjecturing program that was previously and successfully used to generate mathematical conjectures. our program is initialized with chomp invariants and example game boards--the conjectures take the form of invariant-relation statements interpreted to be true for all board positions of a certain kind. the conjectures describe a theory of chomp positions.
the program uses limited, natural input and suggests how theories generated on-the-fly might be used in a variety of situations where decisions--based on reasons--are required."
2020,preferences single-peaked on a circle,https://www.jair.org/index.php/jair/article/view/11732,"we introduce the domain of preferences that are single-peaked on a circle, which is a generalization of the well-studied single-peaked domain. this preference restriction is useful, e.g., for scheduling decisions, certain facility location problems, and for one-dimensional decisions in the presence of extremist preferences. we give a fast recognition algorithm of this domain, provide a characterisation by finitely many forbidden subprofiles, and show that many popular single- and multi-winner voting rules are polynomial-time computable on this domain. in particular, we prove that proportional approval voting can be computed in polynomial time for profiles that are single-peaked on a circle. in contrast, kemeny's rule remains hard to evaluate, and several impossibility results from social choice theory can be proved using only profiles in this domain."
2020,ontology reasoning with deep neural networks,https://www.jair.org/index.php/jair/article/view/11661,"the ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. more recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. in this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. this is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. we present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view."
2020,simulating offender mobility: modeling activity nodes from large-scale human activity data,https://www.jair.org/index.php/jair/article/view/11831,"in recent years, simulation techniques have been applied to investigate the spatiotemporal dynamics of crime. researchers have instantiated mobile offenders in agent-based simulations for theory testing, experimenting with crime prevention strategies, and exploring crime prediction techniques, despite facing challenges due to the complex dynamics of crime and the lack of detailed information about offender mobility. this paper presents a simulation model to explore offender mobility, focusing on the interplay between the agent's awareness space and activity nodes. the simulation generates patterns of individual mobility aiming to cumulatively match crime patterns. to instantiate a realistic urban environment, we use open data to simulate the urban structure, location-based social networks data to represent activity nodes as a proxy for human activity, and taxi trip data as a proxy for human movement between regions of the city. we analyze and systematically compare 35 different mobility strategies and demonstrate the benefits of using large-scale human activity data to simulate offender mobility. the strategies combining taxi trip data or historic crime data with popular activity nodes perform best compared to other strategies, especially for robbery. our approach provides a basis for building agent-based crime simulations that infer offender mobility in urban areas from real-world data."
2020,scalable planning with deep neural network learned transition models,https://www.jair.org/index.php/jair/article/view/11829,"in many complex planning problems with factored, continuous state and action spaces such as reservoir control, heating ventilation and air conditioning (hvac), and navigation domains, it is difficult to obtain a model of the complex nonlinear dynamics that govern state evolution. however, the ubiquity of modern sensors allows us to collect large quantities of data from each of these complex systems and build accurate, nonlinear deep neural network models of their state transitions. but there remains one major problem for the task of control - how can we plan with deep network learned transition models without resorting to monte carlo tree search and other black-box transition model techniques that ignore model structure and do not easily extend to continuous domains? in this paper, we introduce two types of planning methods that can leverage deep neural network learned transition models: hybrid deep milp planner (hd-milp-plan) and tensorflow planner (tf-plan). in hd-milp-plan, we make the critical observation that the rectified linear unit (relu) transfer function for deep networks not only allows faster convergence of model learning, but also permits a direct compilation of the deep network transition model to a mixed-integer linear program (milp) encoding. further, we identify deep network specific optimizations for hd-milp-plan that improve performance over a base encoding and show that we can plan optimally with respect to the learned deep networks. in tf-plan, we take advantage of the efficiency of auto-differentiation tools and gpu-based computation where we encode a subclass of purely continuous planning problems as recurrent neural networks and directly optimize the actions through backpropagation. we compare both planners and show that tf-plan is able to approximate the optimal plans found by hd-milp-plan in less computation time. hence this article offers two novel planners for continuous state and action domains with learned deep neural net transition models: one optimal method (hd-milp-plan) and a scalable alternative for large-scale problems (tf-plan)."
2020,a general approach to multimodal document quality assessment,https://www.jair.org/index.php/jair/article/view/11647,"the perceived quality of a document is affected by various factors, including grammat- icality, readability, stylistics, and expertise depth, making the task of document quality assessment a complex one. in this paper, we explore this task in the context of assessing the quality of wikipedia articles and academic papers. observing that the visual rendering of a document can capture implicit quality indicators that are not present in the document text -- such as images, font choices, and visual layout -- we propose a joint model that combines the text content with a visual rendering of the document for document qual- ity assessment. our joint model achieves state-of-the-art results over five datasets in two domains (wikipedia and academic papers), which demonstrates the complementarity of textual and visual features, and the general applicability of our model. to examine what kinds of features our model has learned, we further train our model in a multi-task learning setting, where document quality assessment is the primary task and feature learning is an auxiliary task. experimental results show that visual embeddings are better at learning structural features while textual embeddings are better at learning readability scores, which further verifies the complementarity of visual and textual features."
2020,the effects of experience on deception in human-agent negotiation,https://www.jair.org/index.php/jair/article/view/11924,"negotiation is the complex social process by which multiple parties come to mutual agreement over a series of issues. as such, it has proven to be a key challenge problem for designing adequately social ais that can effectively navigate this space. artificial ai agents that are capable of negotiating must be capable of realizing policies and strategies that govern offer acceptances, offer generation, preference elicitation, and more. but the next generation of agents must also adapt to reflect their users' experiences.
the best human negotiators tend to have honed their craft through hours of practice and experience. but, not all negotiators agree on which strategic tactics to use, and endorsement of deceptive tactics in particular is a controversial topic for many negotiators. we examine the ways in which deceptive tactics are used and endorsed in non-repeated human negotiation and show that prior experience plays a key role in governing what tactics are seen as acceptable or useful in negotiation. previous work has indicated that people that negotiate through artificial agent representatives may be more inclined to fairness than those people that negotiate directly. we present a series of three user studies that challenge this initial assumption and expand on this picture by examining the role of past experience.
this work constructs a new scale for measuring endorsement of manipulative negotiation tactics and introduces its use to artificial intelligence research. it continues by presenting the results of a series of three studies that examine how negotiating experience can change what negotiation tactics and strategies human endorse. study #1 looks at human endorsement of deceptive techniques based on prior negotiating experience as well as representative effects. study #2 further characterizes the negativity of prior experience in relation to endorsement of deceptive techniques. finally, in study #3, we show that the lessons learned from the empirical observations in study #1 and #2 can in fact be induced--by designing agents that provide a specific type of negative experience, human endorsement of deception can be predictably manipulated."
2020,image captioning using facial expression and attention,https://www.jair.org/index.php/jair/article/view/12025,"benefiting from advances in machine vision and natural language processing techniques, current image captioning systems are able to generate detailed visual descriptions. for the most part, these descriptions represent an objective characterisation of the image, although some models do incorporate subjective aspects related to the observer's view of the image, such as sentiment; current models, however, usually do not consider the emotional content of images during the caption generation process. this paper addresses this issue by proposing novel image captioning models which use facial expression features to generate image captions. the models generate image captions using long short-term memory networks applying facial features in addition to other visual features at different time steps. we compare a comprehensive collection of image captioning models with and without facial features using all standard evaluation metrics. the evaluation metrics indicate that applying facial features with an attention mechanism achieves the best performance, showing more expressive and more correlated image captions, on an image caption dataset extracted from the standard flickr 30k dataset, consisting of around 11k images containing faces. an analysis of the generated captions finds that, perhaps unexpectedly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions."
2020,subgoaling techniques for satisficing and optimal numeric planning,https://www.jair.org/index.php/jair/article/view/11875,"this paper studies novel subgoaling relaxations for automated planning with propositional and numeric state variables. subgoaling relaxations address one source of complexity of the planning problem: the requirement to satisfy conditions simultaneously. the core idea is to relax this requirement by recursively decomposing conditions into atomic subgoals that are considered in isolation. such relaxations are typically used for pruning, or as the basis for computing admissible or inadmissible heuristic estimates to guide optimal or satisificing heuristic search planners. in the last decade or so, the subgoaling principle has underpinned the design of an abundance of relaxation-based heuristics whose formulations have greatly extended the reach of classical planning. this paper extends subgoaling relaxations to support numeric state variables and numeric conditions. we provide both theoretical and practical results, with the aim of reaching a good trade-off between accuracy and computation costs within a heuristic state-space search planner. our experimental results validate the theoretical assumptions, and indicate that subgoaling substantially improves on the state of the art in optimal and satisficing numeric planning via forward state-space search."
2020,how to do things with words: a bayesian approach,https://www.jair.org/index.php/jair/article/view/11951,"communication changes the beliefs of the listener and of the speaker. the value of a communicative act stems from the valuable belief states which result from this act. to model this we build on the interactive pomdp (ipomdp) framework, which extends pomdps to allow agents to model others in multi-agent settings, and we include communication that can take place between the agents to formulate communicative ipomdps (cipomdps). we treat communication as a type of action and therefore, decisions regarding communicative acts are based on decision-theoretic planning using the bellman optimality principle and value iteration, just as they are for all other rational actions. as in any form of planning, the results of actions need to be precisely specified. we use the bayes' theorem to derive how agents update their beliefs in cipomdps; updates are due to agents' actions, observations, messages they send to other agents, and messages they receive from others. the bayesian decision-theoretic approach frees us from the commonly made assumption of cooperative discourse - we consider agents which are free to be dishonest while communicating and are guided only by their selfish rationality. we use a simple tiger game to illustrate the belief update, and to show that the ability to rationally communicate allows agents to improve efficiency of their interactions."
2020,gradient-based learning methods extended to smooth manifolds applied to automated clustering,https://www.jair.org/index.php/jair/article/view/12192,"grassmann manifold based sparse spectral clustering is a classification technique that consists in learning a latent representation of data, formed by a subspace basis, which is sparse. in order to learn a latent representation, spectral clustering is formulated in terms of a loss minimization problem over a smooth manifold known as grassmannian. such minimization problem cannot be tackled by one of traditional gradient-based learning algorithms, which are only suitable to perform optimization in absence of constraints among parameters. it is, therefore, necessary to develop specific optimization/learning algorithms that are able to look for a local minimum of a loss function under smooth constraints in an efficient way. such need calls for manifold optimization methods. in this paper, we extend classical gradient-based learning algorithms on at parameter spaces (from classical gradient descent to adaptive momentum) to curved spaces (smooth manifolds) by means of tools from manifold calculus. we compare clustering performances of these methods and known methods from the scientific literature. the obtained results confirm that the proposed learning algorithms prove lighter in computational complexity than existing ones without detriment in clustering efficacy."
2020,towards partial order reductions for strategic ability,https://www.jair.org/index.php/jair/article/view/11936,"we propose a general semantics for strategic abilities of agents in asynchronous systems, with and without perfect information. based on the semantics, we show some general complexity results for verification of strategic abilities in asynchronous interaction. more importantly, we develop a methodology for partial order reduction in verification of agents with imperfect information. we show that the reduction preserves an important subset of strategic properties, with as well as without the fairness assumption. we also demonstrate the effectiveness of the reduction on a number of benchmarks. interestingly, the reduction does not work for strategic abilities under perfect information."
2020,"best-first enumeration based on bounding conflicts, and its application to large-scale hybrid estimation",https://www.jair.org/index.php/jair/article/view/11892,"there is an increasing desire for autonomous systems to have high levels of robustness and safety, attained through continuously planning and self-repairing online. underlying this is the need to accurately estimate the system state and diagnose subtle failures. estimation methods based on hybrid discrete and continuous state models have emerged as a method of precisely computing these estimates. however, existing methods have difficulty scaling to systems with more than a handful of components. discrete, consistency based state estimation capabilities can scale to this level by combining best-first enumeration and conflict-directed search. while best-first methods have been developed for hybrid estimation, conflict-directed methods have thus far been elusive as conflicts learn inconsistencies from constraint violation, but probabilistic hybrid estimation is relatively unconstrained. in this paper we present an approach to hybrid estimation that unifies best-first enumeration and conflict-directed search through the concept of ""bounding"" conflicts, an extension of conflicts that represent tighter bounds on the cost of regions of the search space. this paper presents a general best-first enumeration algorithm based on bounding conflicts (a*bc) and a hybrid estimation method using this enumeration algorithm. experiments show that an a*bc powered state estimator produces estimates up to an order of magnitude faster than the current state of the art, particularly on large systems."
2020,the impact of treewidth on grounding and solving of answer set programs,https://www.jair.org/index.php/jair/article/view/11515,"in this paper, we aim to study how the performance of modern answer set programming (asp) solvers is influenced by the treewidth of the input program and to investigate the consequences of this relationship. we first perform an experimental evaluation that shows that the solving performance is heavily influenced by treewidth, given ground input programs that are otherwise uniform, both in size and construction. this observation leads to an important question for asp, namely, how to design encodings such that the treewidth of the resulting ground program remains small. to this end, we study two classes of disjunctive programs, namely guarded and connection-guarded programs. in order to investigate these classes, we formalize the grounding process using mso transductions. our main results show that both classes guarantee that the treewidth of the program after grounding only depends on the treewidth (and the maximum degree, in case of connection-guarded programs) of the input instance. in terms of parameterized complexity, our findings yield corresponding fpt results for answer-set existence for bounded treewidth (and also degree, for connection-guarded programs) of the input instance. we further show that bounding treewidth alone leads to np-hardness in the data complexity for connection-guarded programs, which indicates that the two classes are fundamentally different. finally, we show that for both classes, the data complexity remains as hard as in the general case of asp."
2020,the 2^k neighborhoods for grid path planning,https://www.jair.org/index.php/jair/article/view/11383,"grid path planning is an important problem in ai. its understanding has been key for the development of autonomous navigation systems. an interesting and rather surprising fact about the vast literature on this problem is that only a few neighborhoods have been used when evaluating these algorithms. indeed, only the 4- and 8-neighborhoods are usually considered, and rarely the 16-neighborhood. this paper describes three contributions that enable the construction of effective grid path planners for extended 2k-neighborhoods; that is, neighborhoods that admit 2k neighbors per state, where k is a parameter. first, we provide a simple recursive definition of the 2k-neighborhood in terms of the 2k-1-neighborhood. second, we derive distance functions, for any k >= 2, which allow us to propose admissible heuristics that are perfect for obstacle-free grids, which generalize the well-known manhattan and octile distances. third, we define the notion of canonical path for the 2k-neighborhood; this allows us to incorporate our neighborhoods into two versions of a*, namely canonical a* and jump point search (jps), whose performance, we show, scales well when increasing k. our empirical evaluation shows that, when increasing k, the cost of the solution found improves substantially. used with the 2k-neighborhood, canonical a* and jps, in many configurations, are also superior to the any-angle path planner theta* both in terms of solution quality and runtime. our planner is competitive with one implementation of the any-angle path planner, anya in some configurations. our main practical conclusion is that standard, well-understood grid path planning technology may provide an effective approach to any-angle grid path planning."
2020,regret bounds for reinforcement learning via markov chain concentration,https://www.jair.org/index.php/jair/article/view/11316,"we give a simple optimistic algorithm for which it is easy to derive regret bounds of o(sqrt{t-mix sat}) steps in uniformly ergodic markov decision processes with s states, a actions, and mixing time parameter t-mix. these bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. they could only be improved by using an alternative mixing time parameter."
2020,saturated cost partitioning for optimal classical planning,https://www.jair.org/index.php/jair/article/view/11673,"cost partitioning is a method for admissibly combining a set of admissible heuristic estimators by distributing operator costs among the heuristics. computing an optimal cost partitioning, i.e., the operator cost distribution that maximizes the heuristic value, is often prohibitively expensive to compute. saturated cost partitioning is an alternative that is much faster to compute and has been shown to yield high-quality heuristics. however, its greedy nature makes it highly susceptible to the order in which the heuristics are considered. we propose a greedy algorithm to generate orders and show how to use hill-climbing search to optimize a given order. combining both techniques leads to significantly better heuristic estimates than using the best random order that is generated in the same time. since there is often no single order that gives good guidance on the whole state space, we use the maximum of multiple orders as a heuristic that is significantly better informed than any single-order heuristic, especially when we actively search for a set of diverse orders."
2020,the force awakens: artificial intelligence for consumer law,https://www.jair.org/index.php/jair/article/view/11519,"recent years have been tainted by market practices that continuously expose us, as consumers, to new risks and threats. we have become accustomed, and sometimes even resigned, to businesses monitoring our activities, examining our data, and even meddling with our choices. artificial intelligence (ai) is often depicted as a weapon in the hands of businesses and blamed for allowing this to happen. in this paper, we envision a paradigm shift, where ai technologies are brought to the side of consumers and their organizations, with the aim of building an efficient and effective counter-power. ai-powered tools can support a massive-scale automated analysis of textual and audiovisual data, as well as code, for the benefit of consumers and their organizations. this in turn can lead to a better oversight of business activities, help consumers exercise their rights, and enable the civil society to mitigate information overload. we discuss the societal, political, and technological challenges that stand before that vision.
this article is part of the special track on ai and society."
2020,blind spot detection for safe sim-to-real transfer,https://www.jair.org/index.php/jair/article/view/11436,"agents trained in simulation may make errors when performing actions in the real world due to mismatches between training and execution environments. these mistakes can be dangerous and difficult for the agent to discover because the agent is unable to predict them a priori. in this work, we propose the use of oracle feedback to learn a predictive model of these blind spots in order to reduce costly errors in real-world applications. we focus on blind spots in reinforcement learning (rl) that occur due to incomplete state representation: when the agent lacks necessary features to represent the true state of the world, and thus cannot distinguish between numerous states. we formalize the problem of discovering blind spots in rl as a noisy supervised learning problem with class imbalance. our system learns models for predicting blind spots within unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. these models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections.
we evaluate our approach across two domains and demonstrate that it achieves higher predictive performance than baseline methods, and also that the learned model can be used to selectively query an oracle at execution time to prevent errors. we also empirically analyze the biases of various feedback types and how these biases influence the discovery of blind spots. further, we include analyses of our approach that incorporate relaxed initial optimality assumptions. (interestingly, relaxing the assumptions of an optimal oracle and an optimal simulator policy helped our models to perform better.) we also propose extensions to our method that are intended to improve performance when using corrections and demonstrations data."
2020,planning for hybrid systems via satisfiability modulo theories,https://www.jair.org/index.php/jair/article/view/11751,"planning for hybrid systems is important for dealing with real-world applications, and pddl+ supports this representation of domains with mixed discrete and continuous dynamics. in this paper we present a new approach for planning for hybrid systems, based on encoding the planning problem as a satisfiability modulo theories (smt) formula. this is the first smt encoding that can handle the whole set of pddl+ features (including processes and events), and is implemented in the planner smtplan. smtplan not only covers the full semantics of pddl+, but can also deal with non-linear polynomial continuous change without discretization. this allows it to generate plans with non-linear dynamics that are correct-by-construction. the encoding is based on the notion of happenings, and can be applied on domains with nonlinear continuous change. we describe the encoding in detail and provide in-depth examples. we apply this encoding in an iterative deepening planning algorithm. experimental results show that the approach dramatically outperforms existing work in finding plans for pddl+ problems. we also present experiments which explore the performance of the proposed approach on temporal planning problems, showing that the scalability of the approach is limited by the size of the discrete search space. we further extend the encoding to include planning with control parameters. the extended encoding allows the definition of actions to include infinite domain parameters, called control parameters. we present experiments on a set of problems with control parameters to demonstrate the positive effect they provide to the approach of planning via smt."
2020,tensorlog: a probabilistic database implemented using deep-learning infrastructure,https://www.jair.org/index.php/jair/article/view/11944,"we present an implementation of a probabilistic first-order logic called tensorlog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as tensorflow or theano. this leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. the integration with these frameworks enables use of gpu-based parallel processors for inference and learning, making tensorlog the first highly parallellizable probabilistic logic. experimental results show that tensorlog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples."
2020,jointly improving parsing and perception for natural language commands through human-robot dialog,https://www.jair.org/index.php/jair/article/view/11485,"in this work, we present methods for using human-robot dialog to improve language understanding for a mobile robot agent. the agent parses natural language to underlying semantic meanings and uses robotic sensors to create multi-modal models of perceptual concepts like red and heavy. the agent can be used for showing navigation routes, delivering objects to people, and relocating objects from one location to another. we use dialog clari_cation questions both to understand commands and to generate additional parsing training data. the agent employs opportunistic active learning to select questions about how words relate to objects, improving its understanding of perceptual concepts. we evaluated this agent on amazon mechanical turk. after training on data induced from conversations, the agent reduced the number of dialog questions it asked while receiving higher usability ratings. additionally, we demonstrated the agent on a robotic platform, where it learned new perceptual concepts on the y while completing a real-world task."
2020,adversarial attacks on crowdsourcing quality control,https://www.jair.org/index.php/jair/article/view/11332,"crowdsourcing is a popular methodology to collect manual labels at scale. such labels are often used to train ai models and, thus, quality control is a key aspect in the process. one of the most popular quality assurance mechanisms in paid micro-task crowdsourcing is based on gold questions: the use of a small set of tasks of which the requester knows the correct answer and, thus, is able to directly assess crowd work quality. in this paper, we show that such mechanism is prone to an attack carried out by a group of colluding crowd workers that is easy to implement and deploy: the inherent size limit of the gold set can be exploited by building an inferential system to detect which parts of the job are more likely to be gold questions. the described attack is robust to various forms of randomisation and programmatic generation of gold questions. we present the architecture of the proposed system, composed of a browser plug-in and an external server used to share information, and briefly introduce its potential evolution to a decentralised implementation. we implement and experimentally validate the gold detection system, using real-world data from a popular crowdsourcing platform. our experimental results show that crowdworkers using the proposed system spend more time on signalled gold questions but do not neglect the others thus achieving an increased overall work quality. finally, we discuss the economic and sociological implications of this kind of attack."
2020,graph width measures for cnf-encodings with auxiliary variables,https://www.jair.org/index.php/jair/article/view/11750,"we consider bounded width cnf-formulas where the width is measured by popular graph width measures on graphs associated to cnf-formulas. such restricted graph classes, in particular those of bounded treewidth, have been extensively studied for their uses in the design of algorithms for various computational problems on cnf-formulas. here we consider the expressivity of these formulas in the model of clausal encodings with auxiliary variables. we first show that bounding the width for many of the measures from the literature leads to a dramatic loss of expressivity, restricting the formulas to such of low communication complexity. we then show that the width of optimal encodings with respect to different measures is strongly linked: there are two classes of width measures, one containing primal treewidth and the other incidence cliquewidth, such that in each class the width of optimal encodings only differs by constant factors. moreover, between the two classes the width differs at most by a factor logarithmic in the number of variables. both these results are in stark contrast to the setting without auxiliary variables where all width measures we consider here differ by more than constant factors and in many cases even by linear factors."
2020,catching cheats: detecting strategic manipulation in distributed optimisation of electric vehicle aggregators,https://www.jair.org/index.php/jair/article/view/11573,"given the rapid rise of electric vehicles (evs) worldwide, and the ambitious targets set for the near future, the management of large ev fleets must be seen as a priority. specifically, we study a scenario where ev charging is managed through self-interested ev aggregators who compete in the day-ahead market in order to purchase the electricity needed to meet their clients' requirements. with the aim of reducing electricity costs and lowering the impact on electricity markets, a centralised bidding coordination framework has been proposed in the literature employing a coordinator. in order to improve privacy and limit the need for the coordinator, we propose a reformulation of the coordination framework as a decentralised algorithm, employing the alternating direction method of multipliers (admm). however, given the self-interested nature of the aggregators, they can deviate from the algorithm in order to reduce their energy costs. hence, we study the strategic manipulation of the admm algorithm and, in doing so, describe and analyse different possible attack vectors and propose a mathematical framework to quantify and detect manipulation. importantly, this detection framework is not limited to the considered ev scenario and can be applied to general admm algorithms. finally, we test the proposed decentralised coordination and manipulation detection algorithms in realistic scenarios using real market and driver data from spain. our empirical results show that the decentralised algorithm's convergence to the optimal solution can be effectively disrupted by manipulative attacks achieving convergence to a different non-optimal solution which benefits the attacker. with respect to the detection algorithm, results indicate that it achieves very high accuracies and significantly outperforms a naive benchmark."
2020,fair allocation with diminishing differences,https://www.jair.org/index.php/jair/article/view/11994,"ranking alternatives is a natural way for humans to explain their preferences. it is used in many settings, such as school choice, course allocations and residency matches. without having any information on the underlying cardinal utilities, arguing about the fairness of allocations requires extending the ordinal item ranking to ordinal bundle ranking. the most commonly used such extension is stochastic dominance (sd), where a bundle x is preferred over a bundle y if its score is better according to all additive score functions. sd is a very conservative extension, by which few allocations are necessarily fair while many allocations are possibly fair. we propose to make a natural assumption on the underlying cardinal utilities of the players, namely that the difference between two items at the top is larger than the difference between two items at the bottom. this assumption implies a preference extension which we call diminishing differences (dd), where x is preferred over y if its score is better according to all additive score functions satisfying the dd assumption. we give a full characterization of allocations that are necessarily-proportional or possibly-proportional according to this assumption. based on this characterization, we present a polynomial-time algorithm for finding a necessarily-dd-proportional allocation whenever it exists. using simulations, we compare the various fairness criteria in terms of their probability of existence, and their probability of being fair by the underlying cardinal valuations. we find that necessary-dd-proportionality fares well in both measures. we also consider envy-freeness and pareto optimality under diminishing-differences, as well as chore allocation under the analogous condition --- increasing-differences."
2020,a global constraint for the exact cover problem: application to conceptual clustering,https://www.jair.org/index.php/jair/article/view/11870,"we introduce the exactcover global constraint dedicated to the exact cover problem, the goal of which is to select subsets such that each element of a given set belongs to exactly one selected subset. this np-complete problem occurs in many applications, and we more particularly focus on a conceptual clustering application.
we introduce three propagation algorithms for exactcover, called basic, dl, and dl+: basic ensures the same level of consistency as arc consistency on a classical decomposition of exactcover into binary constraints, without using any specific data structure; dl ensures the same level of consistency as basic but uses dancing links to efficiently maintain the relation between elements and subsets; and dl+ is a stronger propagator which exploits an extra property to filter more values than dl.
we also consider the case where the number of selected subsets is constrained to be equal to a given integer variable k, and we show that this may be achieved either by combining exactcover with existing constraints, or by designing a specific propagator that integrates algorithms designed for the nvalues constraint.
these different propagators are experimentally evaluated on conceptual clustering problems, and they are compared with state-of-the-art declarative approaches. in particular, we show that our global constraint is competitive with recent ilp and cp models for mono-criterion problems, and it has better scale-up properties for multi-criteria problems."
2020,robust multi-agent path finding and executing,https://www.jair.org/index.php/jair/article/view/11734,"multi-agent path-finding (mapf) is the problem of finding a plan for moving a set of agents from their initial locations to their goals without collisions. following this plan, however, may not be possible due to unexpected events that delay some of the agents. in this work, we propose a holistic solution for mapf that is robust to such unexpected delays. first, we introduce the notion of a k-robust mapf plan, which is a plan that can be executed even if a limited number (k) of delays occur. we propose sufficient and required conditions for finding a k-robust plan, and show how to convert several mapf solvers to find such plans. then, we propose several robust execution policies. an execution policy is a policy for agents executing a mapf plan. an execution policy is robust if following it guarantees that the agents reach their goals even if they encounter unexpected delays. several classes of such robust execution policies are proposed and evaluated experimentally. finally, we present robust execution policies for cases where communication between the agents may also be delayed. we performed an extensive experimental evaluation in which we compared different algorithms for finding robust mapf plans, compared different ro- bust execution policies, and studied the interplay between having a robust plan and the performance when using a robust execution policy."
2020,agreement on target-bidirectional recurrent neural networks for sequence-to-sequence learning,https://www.jair.org/index.php/jair/article/view/12008,"recurrent neural networks are extremely appealing for sequence-to-sequence learning tasks. despite their great success, they typically suffer from a shortcoming: they are prone to generate unbalanced targets with good prefixes but bad suffixes, and thus performance suffers when dealing with long sequences. we propose a simple yet effective approach to overcome this shortcoming. our approach relies on the agreement between a pair of target-directional rnns, which generates more balanced targets. in addition, we develop two efficient approximate search methods for agreement that are empirically shown to be almost optimal in terms of either sequence level or non-sequence level metrics. extensive experiments were performed on three standard sequence-to-sequence transduction tasks: machine transliteration, grapheme-to-phoneme transformation and machine translation. the results show that the proposed approach achieves consistent and substantial improvements, compared to many state-of-the-art systems."
2020,solving delete free planning with relaxed decision diagram based heuristics,https://www.jair.org/index.php/jair/article/view/11659,"we investigate the use of relaxed decision diagrams (dds) for computing admissible heuristics for the cost-optimal delete-free planning (dfp) problem. our main contributions are the introduction of two novel dd encodings for a dfp task: a multivalued decision diagram that includes the sequencing aspect of the problem and a binary decision diagram representation of its sequential relaxation. we present construction algorithms for each dd that leverage these different perspectives of the dfp task and provide theoretical and empirical analyses of the associated heuristics. we further show that relaxed dds can be used beyond heuristic computation to extract delete-free plans, find action landmarks, and identify redundant actions. our empirical analysis shows that while dd-based heuristics trail the state of the art, even small relaxed dds are competitive with the linear programming heuristic for the dfp task, thus, revealing novel ways of designing admissible heuristics."
2020,a set of recommendations for assessing human-machine parity in language translation,https://www.jair.org/index.php/jair/article/view/11371,"the quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a number of empirical investigations. we reassess hassan et al.'s 2018 investigation into chinese to english news translation, showing that the finding of human-machine parity was owed to weaknesses in the evaluation design--which is currently considered best practice in the field. we show that the professional human translations contained significantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference translations. our results call for revisiting current best practices to assess strong machine translation systems in general and human-machine parity in particular, for which we offer a set of recommendations based on our empirical findings."
2020,using task descriptions in lifelong machine learning for improved performance and zero-shot transfer,https://www.jair.org/index.php/jair/article/view/11304,"knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of inter-task relationships to identify the relevant knowledge to transfer. these inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. to reduce this burden, we develop a lifelong learning method based on coupled dictionary learning that utilizes high-level task descriptions to model inter-task relationships. we show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of learning problems. given only the descriptor for a new task, the lifelong learner is also able to accurately predict a model for the new task through zero-shot learning using the coupled dictionary, eliminating the need to gather training data before addressing the task."
2020,hedonic games with ordinal preferences and thresholds,https://www.jair.org/index.php/jair/article/view/11531,"we propose a new representation setting for hedonic games, where each agent partitions the set of other agents into friends, enemies, and neutral agents, with friends and enemies being ranked. under the assumption that preferences are monotonic (respectively, antimonotonic) with respect to the addition of friends (respectively, enemies), we propose a bipolar extension of the responsive extension principle, and use this principle to derive the (partial) preferences of agents over coalitions. then, for a number of solution concepts, we characterize partitions that necessarily or possibly satisfy them, and we study the related problems in terms of their complexity."
2020,compositionality decomposed: how do neural networks generalise?,https://www.jair.org/index.php/jair/article/view/11674,"despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. as a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. we collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. in particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. to demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub pcfg set and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. we provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement."
2020,incomplete preferences in single-peaked electorates,https://www.jair.org/index.php/jair/article/view/11577,"incomplete preferences are likely to arise in real-world preference aggregation scenarios. this paper deals with determining whether an incomplete preference profile is single-peaked. this is valuable information since many intractable voting problems become tractable given singlepeaked preferences. we prove that the problem of recognizing single-peakedness is np-complete for incomplete profiles consisting of partial orders. despite this intractability result, we find several polynomial-time algorithms for reasonably restricted settings. in particular, we give polynomial-time recognition algorithms for weak orders, which can be viewed as preferences with indifference."
2020,htn planning as heuristic progression search,https://www.jair.org/index.php/jair/article/view/11282,"the majority of search-based htn planning systems can be divided into those searching a space of partial plans (a plan space) and those performing progression search, i.e., that build the solution in a forward manner. so far, all htn planners that guide the search by using heuristic functions are based on plan space search. those systems represent the set of search nodes more effectively by maintaining a partial ordering between tasks, but they have only limited information about the current state during search. in this article, we propose the use of progression search as basis for heuristic htn planning systems. such systems can calculate their heuristics incorporating the current state, because it is tracked during search. our contribution is the following: we introduce two novel progression algorithms that avoid unnecessary branching when the problem at hand is partially ordered and show that both are sound and complete. we show that defining systematicity is problematic for search in htn planning, propose a definition, and show that it is fulfilled by one of our algorithms. then, we introduce a method to apply arbitrary classical planning heuristics to guide the search in htn planning. it relaxes the htn planning model to a classical model that is only used for calculating heuristics. it is updated during search and used to create heuristic values that are used to guide the htn search. we show that it can be used to create htn heuristics with interesting theoretical properties like safety, goal-awareness, and admissibility. our empirical evaluation shows that the resulting system outperforms the state of the art in search-based htn planning."
2020,learning the language of software errors,https://www.jair.org/index.php/jair/article/view/11798,"we propose to use algorithms for learning deterministic finite automata (dfa), such as angluin's l* algorithm, for learning a dfa that describes the possible scenarios under which a given program error occurs. the alphabet of this automaton is given by the user (for instance, a subset of the function call sites or branches), and hence the automaton describes a user-defined abstraction of those scenarios. more generally, the same technique can be used for visualising the behavior of a program or parts thereof. it can also be used for visually comparing different versions of a program (by presenting an automaton for the behavior in the symmetric difference between them), and for assisting in merging several development branches. we present experiments that demonstrate the power of an abstract visual representation of errors and of program segments, accessible via the project's web page. in addition, our experiments in this paper demonstrate that such automata can be learned efficiently over real-world programs. we also present lazy learning, which is a method for reducing the number of membership queries while using l*, and demonstrate its effectiveness on standard benchmarks."
